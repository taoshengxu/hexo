<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[DOS-Ubuntu命令集合]]></title>
    <url>%2Fhexo%2F2029%2F12%2F12%2F2029-12-12-UbuntuCommand%2F</url>
    <content type="text"><![CDATA[DOS cd / [root] cd ~ [home or root/user] cd . [Current] cd .. [Father] cd - [previous] pwd [current] d: +Enter [D:/] UbuntuBasic sudo su 进入root exit or logout or ctrl D 退出root 修改root密码 $ sudo passwd 修改用户密码 $ sudo passwd username 修改主机名： $ sudo gedit /etc/hostname 和 $ sudo gedit /etc/hosts &amp; 加在一个命令之后可以把这个命令放到后台运行 jobs -l 查看后台运行的命令 fg jobnumber(PID) 将后台命令调到前台 bg jobnumber(PID) 重启后台暂停的命令 kill %num 杀死 kill -9 PID 强制杀死进程 ps 查看进程号PID nohup 始终执行 ps -aux | grep xxx 查找后台进程 ps -ef | grep xxx 查找数后台进程数 wc -l 用于统计一个文件中的行数、字数、字节数或字符数 Ctrl+Z 可以把一个前台执行的命令放到后台 Ctrl+c 在命令行下起着终止当前执行程序的作用， Ctrl+d 相当于exit命令，退出当前shell Ctrl+s挂起当前shell（保护作用很明显哦） Ctrl+q 解冻挂起的shell再不行就重新连接打开一个终端，reboot linux 或 kill 相关进程。 目录操作 ls 列出所有文件 ls -lh 列出目录的文件的详细信息 ls -ld 文件目录 #查看文件12345678910ls -l#drwxr-xr-x 2 nsf users 1024 12-10 17:37 下载文件备份 #文件属性 连接数 文件拥有者 所属群组 文件大小 文件修改时间 文件名r 是可读 w 可写 x 可执行，其中文件属性分为四段，---- --- --- 10个位置 d rwx r-x r-x 第一段字符指定了文件类型。如果是d，表示是一个目录。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。第二段是文件拥有者的属性， 第三段是文件所属群组的属性， 第四段是对于其它用户的属性， ubuntu创建、删除文件及文件夹，强制清空回收站方法 mkdir 目录名 创建一个目录 rmdir 空目录名 删除一个空目录 rm 文件名 文件名 删除一个文件或多个文件 rm –rf 非空目录名 删除一个非空目录下的一切 touch 文件名 创建一个空文件 mv file1 file2 将文件 file1，更改文件名为 file2。 mv file1 dir1 将文件 file1，移到目录 dir1下，文件名仍为 file1。 mv dir1 dir2 若目录 dir2 存在，则将目录 dir1，及其所有文件和子目录，移到目录 dir2 下，新目录名称为 dir1。若目录 dir2 不存在，则将dir1，及其所有文件和子目录，更改为目录 dir2。 开关机重启 reboot shutdown -r now 立刻重启(root用户使用) shutdown -r 10 过10分钟自动重启(root用户使用) shutdown -r 20:35 在时间为20:35时候重启(root用户使用) 如果是通过shutdown命令设置重启的话，可以用shutdown -c命令取消重启 关机 halt 立刻关机 poweroff 立刻关机 shutdown -h now 立刻关机(root用户使用) shutdown -h 10 10分钟后自动关机 卸载软件 [dpkg —list]查找软件[sudo apt-get remove ] 卸载 查看CPU top命令 可以再按1 sudo apt-get install htophtop #分辨系统进程和用户进程及其作用, 非常管用 创建新用户和用户组1234567891011##创建新用户有两条指令（1） useradd -g new_user usergroup_name #新建用户同时增加工作组 这个命令新建的用户没有用户目录（2） adduser new_user 这个命令新建的用户具有完全的功能##创建新的用户组 groupadd usergroup_name ## 将已经存在的用户user_name 添加到用户组usermod -aG usergroup_name user_name ## 查看所属的用户组groups user_name 修改文件权限和所有者 修改文件权限 sudo chmod -[读,写,执行]×××（所有者）×××（组用户）×××（其他用户） sudo chmod -R 700 Document/变更权限(-R参数是递归) 1234567- sudo chmod 600 ××× （只有所有者有读和写的权限）- sudo chmod 644 ××× （所有者有读和写的权限，组用户只有读的权限）- sudo chmod 700 ××× （只有所有者有读和写以及执行的权限）- sudo chmod 666 ××× （每个人都有读和写的权限）- sudo chmod 777 ××× （每个人都有读和写以及执行的权限）- chmod +x test.py # 脚本文件添加可执行权限- chmod -x test.py # 脚本文件去除可执行权限 修改文件所有者 ​ 命令：chown 用户 目录或文件名​ 例如：chown qq /home/qq (把home目录下的qq目录的拥有者改为qq用户) ​ sudo chown -R username:root /data/abc ##将所有者为root用户的abc(文件或者目录)修改为username用户 修改文件或目录所属的组： 命令：chgrp 组 目录或文件名 例如：chgrp qq /home/qq (把home目录下的qq目录的所属组改为qq组) ​ sudo chmod 775 /srv/shiny-server/xxxx/ ​ sudo chown :shiny /srv/shiny-server/xxxx/ 修改该目录为 shiny用户组 磁盘管理 sudo mount -t ext4 /dev/sdb /data 挂载到data sudo umount /dev/sdb 卸载 mkdir -p 递归创建目录 cp 复制 rmdir 删除空目录 df -lh 查看磁盘空间 df -kh 查看磁盘挂载 df -m 以M为单位显示磁盘使用量和占用率 df -k 以KB为单位显示磁盘使用量和占用率 du-ck /dir 列出目录下所有文件或目录占用的大小，以KB作为计量单位 sudo fdisk -lu 显示硬盘及所属分区情况 sudo fdisk /dev/sdb 对sbd盘分区m(help)-n(增加一个新分区)-e(扩展分区)-1-4(分几个区)-…-p(显示分区表)-w(保存分区表) sudo blkid 查看所有分区的UUID sudo blkid /dev/sda5 查看指定盘的UUID (谨慎复制)sudo mkfs -t ext4 /dev/sdb 对sdb盘格式化为ext4格式 自动挂载 sudo gedit /etc/fstab 在这个文件夹下修改 1UUID=09d387b1-8438-41db-9b88-f3b145cc1674 /data2 ext4 defaults 0 0 ubuntu 加载新硬盘教程pdf ubuntu命令手册 ubuntu上查看cpu、内存、硬盘等参数 CPU： 型号：grep &quot;model name&quot; /proc/cpuinfo |awk -F &#39;:&#39; &#39;{print $NF}&#39; 数量：lscpu |grep &quot;CPU socket&quot; |awk &#39;{print $NF}&#39; 或 lscpu |grep &quot;Socket&quot; |awk &#39;{print $NF}&#39; 每个CPU的核数：lscpu |grep &quot;Core(s) per socket&quot; |awk &#39;{print $NF}&#39; 内存： 卡槽数量：sudo dmidecode -t memory |grep &quot;Number Of Devices&quot; |awk &#39;{print $NF}&#39;或sudo dmidecode -t memory |grep &quot;Associated Memory Slots&quot; |awk &#39;{print $NF}&#39; 内存数量：sudo dmidecode -t memory |grep -A16 &quot;Memory Device$&quot; |grep &#39;Size:.*MB&#39; |wc -l 内存支持类型：sudo dmidecode -t memory |grep -A16 &quot;Memory Device$&quot; |grep &quot;Type:&quot; 每个内存频率：sudo dmidecode -t memory |grep -A16 &quot;Memory Device$&quot; |grep &quot;Speed:&quot; 每个内存大小：sudo dmidecode -t memory |grep -A16 &quot;Memory Device$&quot; |grep &quot;Size:&quot; 释放缓冲区内存：echo 3 &gt; /proc/sys/vm/drop_caches 硬盘： 硬盘数量、大小：sudo fdisk -l |grep &quot;Disk /dev/sd&quot; 硬盘型号：sudo hdparm -i /dev/sda |grep &quot;Model&quot; 遇到根目录占满的情况：查看磁盘文件系统与目录的占用情况：df -h 查看最近30天内出现的占用大小超10000 blocks的文件：find / -xdev -type f -mtime -30 -size +10000（作者：信仰与初衷https://www.jianshu.com/p/a08ed6146352） 主板型号,主板支持最大内存,单条内存的参数sudo dmidecode -t 2 //查看主板信息sudo dmidecode -t 16 |grep Maximum //查看主板支持最大内存sudo dmidecode -t memory //查看单条内存的参数 硬盘品牌及其参数及其健康状态的检查sudo apt-get install sysstat 2秒刷新：sudo iostat -x 2sudo hdparm -i /dev/sda 服务器品牌,服务编号sudo dmidecode |grep ‘Product Name’ //查看服务器品牌，型号sudo dmidecode -s system-serial-number //查看服务编号 网卡的型号,所使用的驱动版本sudo lspci | grep Eth //查看网卡型号sudo modinfo pcnet32原文链接：https://blog.csdn.net/djd1234567/article/details/48372911 文本编辑—vi 操作 基本上vi可以分为三种状态 分别是命令模式（command mode）：移动光标,删除 插入模式（Insert mode）：用于输入字符 底行模式（last line mode）：设置编辑环境和保存 进入vi 初始为命令模式$vi file 进入插入模式： 在命令模式下输入[i] ESC 返回命令模式 [:]进入底行模式 [w fileName]以指定名字保存 [wq]存盘并退出 [q!]不存盘退出 更复杂用法 文本编辑 — nono 操作nono是一个很好的命令行文本编辑软件 123sudo nano /mnt/expect.sh。。。。编辑ctrl+x 退出询问是否保存,选择Y。 如何在 Ubuntu 系统中修复损坏的程序包方法1：使用 apt 或 apt-getapt 命令有几个特殊参数，可以用来修复在安装过程中被破坏或缺失依赖的项或软件包。常见的情况是：当您使用 .deb 安装某个软件时，发现它存在你不知道的依赖关系，而这些依赖项又不会被自行引入，dpkg 就会提示缺少依赖。此时，您就可以用到以下步骤： 1在「终端」中执行以下更新命令，确保没有所需软件包的更新版本： 1sudo apt update --fix-missing 2执行以下命令以强制 apt 查找并更正缺少的依赖项或损坏的包： 1sudo apt install -f 有关 apt 与 apt-get 的区别请参考Linux 中 apt 与 apt-get 命令的区别与解释 方法2：使用 dpkg程序包安装过程中，另一个容易出错的地方就是「配置过程」。而在幕后处理配置过程的是 dpkg 而非 apt。所以在当一个程序包在配置期间出现故障时，dpkg 是修复它的最佳工具。 1在「终端」中执行以下命令以强制 dpkg 重新配置软件包的损坏部分： 1sudo dpkg --configure -a 2如果不能解决问题，您可以通过如下命令列出 dpkg 标记为需要重新安装的所有软件包： 1sudo dpkg -l | grep ^..r grep 命令的更多用法请参考如何使用 grep 命令在 Linux 中搜索文件 3使用以下命令强制移除已损坏的程序包： 1sudo dpkg --remove --force-remove--reinstreq 4在 dpkg 的工作完成后，尝试使用 apt 进行清理： 12sudo apt cleansudo apt update 方法3：解除 dpkg 锁还有一种不太常见的情况——dpkg 锁。当你尝试使用 apt 或 dpkg 时，都会遇到错误提示，提示一个不存在的程序已经存在。此种情况通常是在程序包的安装过程发生错误后，一些被锁定的文件无法自动删除，在文件系统中扔被保留下来造成的。 如果遇到这种情况，就需要使用以下命令手动解除 dpkg 锁： 1sudo rm /var/lib/apt/lists/lock 还需要删除对缓存文件的锁定： 1sudo rm /var/cache/apt/archives/lock Bash 命令 | 表示管道命令，命令从前往后执行，前一个命令的输出可以作为后一个命令的输入 ls .fastq.gz|while read id;do echo ${id%_}_1.fastq.gz;done ${id%_*}表示把id中_后面的内容都去掉 ubuntu的一些命令及查看已安装软件包的命令 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// 系统# uname -a # 查看内核/操作系统/CPU信息# head -n 1 /etc/issue # 查看操作系统版本# cat /proc/cpuinfo # 查看CPU信息# hostname # 查看计算机名# lspci -tv # 列出所有PCI设备# lsusb -tv # 列出所有USB设备# lsmod # 列出加载的内核模块# env # 查看环境变量// 资源# free -m # 查看内存使用量和交换区使用量# df -h # 查看各分区使用情况# du -sh &lt;目录名&gt; # 查看指定目录的大小# grep MemTotal /proc/meminfo # 查看内存总量# grep MemFree /proc/meminfo # 查看空闲内存量# uptime # 查看系统运行时间、用户数、负载# cat /proc/loadavg # 查看系统负载// 磁盘和分区# mount | column -t # 查看挂接的分区状态# fdisk -l # 查看所有分区# swapon -s # 查看所有交换分区# hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备)# dmesg | grep IDE # 查看启动时IDE设备检测状况// 网络# ifconfig # 查看所有网络接口的属性# iptables -L # 查看防火墙设置# route -n # 查看路由表# netstat -lntp # 查看所有监听端口# netstat -antp # 查看所有已经建立的连接# netstat -s # 查看网络统计信息// 进程# ps -ef # 查看所有进程# top # 实时显示进程状态// 用户# w # 查看活动用户# id &lt;用户名&gt; # 查看指定用户信息# last # 查看用户登录日志# cut -d: -f1 /etc/passwd # 查看系统所有用户# cut -d: -f1 /etc/group # 查看系统所有组# crontab -l # 查看当前用户的计划任务// 服务# chkconfig --list # 列出所有系统服务# chkconfig --list | grep on # 列出所有启动的系统服务// 程序apt-get update——在修改/etc/apt/sources.list或者/etc/apt/preferences之后运行该命令。此外您需要定期运行这一命令以确保您的软件包列表是最新的。 apt-get install packagename——安装一个新软件包（参见下文的aptitude） apt-get remove packagename——卸载一个已安装的软件包（保留配置文件） apt-get --purge remove packagename——卸载一个已安装的软件包（删除配置文件） dpkg --force-all --purge packagename 有些软件很难卸载，而且还阻止了别的软件的应用，就可以用这个，不过有点冒险。 apt-get autoclean apt会把已装或已卸的软件都备份在硬盘上，所以如果需要空间的话，可以让这个命令来删除你已经删掉的软件 apt-get clean 这个命令会把安装的软件的备份也删除，不过这样不会影响软件的使用的。 apt-get upgrade——更新所有已安装的软件包 apt-get dist-upgrade——将系统升级到新版本 apt-cache search string——在软件包列表中搜索字符串 dpkg -l package-name-pattern——列出所有与模式相匹配的软件包。如果您不知道软件包的全名，您可以使用“*package-name-pattern*”。 aptitude——详细查看已安装或可用的软件包。与apt-get类似，aptitude可以通过命令行方式调用，但仅限于某些命令——最常见的有安装和卸载命令。由于aptitude比apt-get了解更多信息，可以说它更适合用来进行安装和卸载。 apt-cache showpkg pkgs——显示软件包信息。 apt-cache dumpavail——打印可用软件包列表。 apt-cache show pkgs——显示软件包记录，类似于dpkg –print-avail。 apt-cache pkgnames——打印软件包列表中所有软件包的名称。 dpkg -S file——这个文件属于哪个已安装软件包。 dpkg -L package——列出软件包中的所有文件。 apt-file search filename——查找包含特定文件的软件包（不一定是已安装的），这些文件的文件名中含有指定的字符串。apt-file是一个独立的软件包。您必须 先使用apt-get install来安装它，然后运行apt-file update。如果apt-file search filename输出的内容太多，您可以尝试使用apt-file search filename | grep -w filename（只显示指定字符串作为完整的单词出现在其中的那些文件名）或者类似方法，例如：apt-file search filename | grep /bin/（只显示位于诸如/bin或/usr/bin这些文件夹中的文件，如果您要查找的是某个特定的执行文件的话，这样做是有帮助的）。--------------------- 原文：https://blog.csdn.net/helaisun/article/details/80712287 Ubuntu进入单用户模式—系统修复偶尔会遇到Ubuntu无法正常启动的情况，这时候需修改某些文件让系统正常启动，如果直接进入 recovery 模式，默认是文件权限只读，无法修改文件。这是我们需要进入recovery 的单用户模式，获得修改文件的权限。 重启ubuntu，随即长按shirft进入grub菜单，或等待grub菜单的出现. 选择recovery mode，接着用方向键将光标移至recovery mode，按”e”键进入编辑页面. 将 ro recovery nomodeset 改为 rw single init=/bin/bash 按 ctrl+x或者F10 进入单用户模式，当前用户即为root。可以进行操作，如：用vi命令修改/etc/fatab文件，按：wq保存退出。 修改完成安装ctrl+dele+alt 重启 Ubuntu 查看用户名并重置新密码 长按shift进入grub菜单 选择recovery mode 选择root 进入命令行 ls /home 查看系统用户名mount -rw -o remount(有空格)/passwd + 系统用户名输入新密码 Ubuntu的apt-get源为国内镜像源的方法123456789101112131415cp /etc/apt/sources.list /etc/apt/sources.list.bak sudo vim /etc/apt/sources.list# 将原来的列表删除，添加如下内容（中科大镜像源）deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiversedeb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiversedeb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiversedeb http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiversedeb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiversedeb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiversedeb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiversesudo apt-get update 解压12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152- .tar （注：tar是打包，不是压缩！） - 解包：tar xvf FileName.tar - 打包：tar cvf FileName.tar DirName- .gz - 解压1：gunzip FileName.gz - 解压2：gzip -d FileName.gz - 压缩：gzip FileName- .tar.gz 和 .tgz - 解压：tar zxvf FileName.tar.gz - 压缩：tar zcvf FileName.tar.gz DirName- .bz2 - 解压1：bzip2 -d FileName.bz2 - 解压2：bunzip2 FileName.bz2 - 压缩： bzip2 -z FileName- .tar.bz2 - 解压：tar jxvf FileName.tar.bz2 - 压缩：tar jcvf FileName.tar.bz2 DirName- .bz - 解压1：bzip2 -d FileName.bz - 解压2：bunzip2 FileName.bz- .tar.bz - 解压：tar jxvf FileName.tar.bz - 压缩：未知- .Z - 解压：uncompress FileName.Z - 压缩：compress FileName- .tar.Z - 解压：tar Zxvf FileName.tar.Z - 压缩：tar Zcvf FileName.tar.Z DirName- .zip - 解压：unzip FileName.zip - 压缩：zip FileName.zip DirName- .rar - 解压：rar x FileName.rar - 压缩：rar a FileName.rar DirName- .lha - 解压：lha -e FileName.lha - 压缩：lha -a FileName.lha FileName- .rpm - 解包：rpm2cpio FileName.rpm | cpio -div- .deb - 解包：ar p FileName.deb data.tar.gz | tar zxf -- .tar .tgz .tar.gz .tar.Z .tar.bz .tar.bz2 .zip .cpio .rpm .deb .slp .arj .rar .ace .lha .lzh .lzx .lzs .arc .sda .sfx .lnx .zoo .cab .kar .cpt .pit .sit .sea - 解压：sEx x FileName.* - 压缩：sEx a FileName.* FileName sEx只是调用相关程序，本身并无压缩、解压功能，请注意！- gzip [选项] 压缩（解压缩） 减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。gzip 是在 Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。 - c 将输出写到标准输出上，并保留原有文件。 - d 将压缩文件解压。 - l 对每个压缩文件，显示下列字段：压缩文件的大小；未压缩文件的大小；压缩比；未压缩文件的名字-r 递归式地查找指定目录并压缩其中的所有文件或者是解压缩。-t 测试，检查压缩文件是否完整。-v 对每一个压缩和解压的文件，显示文件名和压缩比。-num 用指定的数字 num 调整压缩的速度，-1 或 --fast 表示最快压缩方法（低压缩比），-9 或--best表示最慢压缩方法（高压缩比）。系统缺省值为 6。指令实例：gzip *% 把当前目录下的每个文件压缩成 .gz 文件。gzip -dv *% 把当前目录下每个压缩的文件解压，并列出详细的信息。gzip -l *% 详细显示例1中每个压缩的文件的信息，并不解压。gzip usr.tar% 压缩 tar 备份文件 usr.tar，此时压缩文件的扩展名为.tar.gz。]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCGA数据下载 UCSC Xena]]></title>
    <url>%2Fhexo%2F2019%2F12%2F05%2F2019-12-05_TCGA_UCSC_xena%2F</url>
    <content type="text"><![CDATA[数据下载专题 | UCSC Xena UCSC Xena是由加州大学圣克鲁兹分校(University Of Cingifornia Sisha Cruz，UCSC)维护的数据库，前身是癌症基因组浏览器Cancer Browser ,Cancer Browser目前已经不再更新. 1 UCSC 官方网址：UCSC XenaXena收录了TCGA、GDC、ICGC、TARGET等多个数据库的公共数据，主页如下： 红框1- Analysis 列出了Xena支持的数据分析功能 数据下载方法一红框2- 点击Launch Xene 即可进入数据下载界面 箭头指向的是右边所有勾选的data hub中的存储的数据，有122组，1534个数据集，可通过勾选自由选择想要查看的数据平台的数据。以 COAD 为例： TCGA 的 COAD 数据) 包含：copy number (gene-level)、copy number segments、DNA methylation、exon expression RNAseq、gene expression array、gene expression RNAseq等多个层面的数据， 比如 gene expression RNAseq，下面对应四种数据集，区别在于使用的实验平台或是对数据的处理方法，具体介绍可点击方框里的链接查看 进去 IlluminaHiSeq, 界面如下： 8 绿色方框 对应着.json的格式文件，可以自行打开查看 蓝色方框 对应着下载的数据的 基因名 和 样本名 ，如下 黄色方框 的链接，进入 NCI(National Cancer Institue) ，which is part of the National Institutes of Health(NIH). 下载的数据形式 1、从上图链接下载对应的gene mapping数据打开如下： 13 2、COAD 的gene expression下载完成后，打开如下： 14 其他​ 帮助文档中给出了其他的下载方法数据下载 数据的可视化分析“VISUALIZATION” 对应着UCSC Xena 的可视化分析， 例子 可以发现通过移动鼠标，可以观察每个样本和对应探针的属性，按住鼠标拖动（横或竖），可以进行缩放。点击clear zoom 取消全部缩放， zoom out 撤回上一步缩放。当鼠标放在图片上，可以发现右下角有个三角，可以拖动进行放大。 ​ MGMT介绍: 化疗是治疗癌症的重要步骤之一，肿瘤对化疗的耐受是影响化疗结果的重要因素之一。DNA修复酶—O6-甲基鸟嘌呤DNA甲基转移酶（O6-methylguanine-DNA methytransferase，MGMT)在胶质瘤组织中的表达与肿瘤的耐药性相关，且MGMT基因启动子的甲基化状态是决定MGMT表达量的主要因素，并可能影响病人的化疗效果及其预后。 当我们自己想对某一数据集进行可视化分析：https://xenabrowser.net/heatmap/ 注 当我们添加数据时至少添加两列才可以进行分析，因为Xena的设计，需要有超过2种数据类型才能找到变化趋势。 添加自己的 data 到 Xena如果以后需要将自己的数据添加到Xena中进行分析，可通过Xena提供的帮助文档，进行操作 https://ucsc-xena.gitbook.io/project/local-xena-hub/getting-started 学习视频Xena 中提供了视频教学：https://ucsc-xena.gitbook.io/project/tutorials]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>Bioinformatics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCGA数据下载总结]]></title>
    <url>%2Fhexo%2F2019%2F12%2F05%2F2019-12-05_TCGA_Datadownload%2F</url>
    <content type="text"><![CDATA[1. Clinical data用 TCGAbiolinks下载临床数据， 优点： 临床数据量不大，容易下载。 Parse XML clinical data 会获取非常高质量的follow-up数据，对生存分析作用非常大。 2.甲基化数据甲基化数据量非常大，从firehose获取神经连接用迅雷下载，其他工具下载慢，容易中断。然后用TCGA Assembler 2进行数据处理。 3.CNV数据CNV是相对参考基因组而言的概念。 123Merge_snp__genome_wide_snp_6__broad_mit_edu__Level_3__segmented_scna_hg19__seg Merge_snp__genome_wide_snp_6__broad_mit_edu__Level_3__segmented_scna_minus_germline_cnv_hg19__seg 其中minus germline CNV的就是我们想要的癌症相关的somatic CNV。 在TCGA Assembler 2中分别用 na_cnv.hg18, cna_cnv.hg19, cna_nocnv.hg18, and cna_nocnv.hg19 来区别表示 4. RNASeq数据TCGA RNAseq数据中FPKM与TPM转换介绍TCGA TCGA下载 TPM FPKM 在新版TCGA（GDC Portral）的RNAseq数据主要提供了三种数据下载，HTSeq-Counts，HTSeq-FPKM，HTSeq-FPKM-UQ. TCGA官方文档 Counts是数据后台没有处理的原始表达量，而FPKM和FPKM-UQ是两种数据处理方法，也就是说，如果下载Counts数据，是表达量数据，如果下载FPKM数据，那么要注意这些数据是经过处理的。正常情况下，我们下载Counts数据就可以了，特殊情况选择FPKM数据也是可以的。 下载数据后，在数据分析时，用的方法也是不同的，Counts数据一般使用edgeR包或DESeq包，对数据做分析；如果下载FPKM数据，就不能使用edgeR包，只能只用DESeq包进行处理。在使用edgeR包做Counts数据处理时，是需要对数据进行normalize的，所以我们在下载数据时，下载counts是比较常用的。 如果要用edgR等筛选差异的话会下载使用Counts数据，但是笔者在过去的数据分析中发现TCGA数据使用edgR等软件筛选差… 但是笔者在过去的数据分析中发现TCGA数据使用edgR等软件筛选差异基因并不理想，细思主要有两方面原因： 一、肿瘤数据本身异质性很高 二、正常样本严重偏少 基于此笔者几乎很少使用edgR等软件来筛选差异了，那么就很少下载Counts数据了，所以大多数情况下都是用RPKM，但是RPKM数据本身也是饱受诟病，像cbioportal使用的是RSEM软件做的定量即TPM，在老版TCGA中也可以直接下载到这些数据，那么新版如何得到这样的数据呢，简单的办法是从FPKM转TPM. FPKM: Fragments Per Kilobase of exon model per Million mapped fragments 即每千个碱基的转录每百万映射读取的fragmentsTPM：TranscriptsPerKilobase of exonmodel per Million mapped reads 即每千个碱基的转录每百万映射读取的Transcripts他们的计算公式如下： 123456FPKM= total exon reads/ (mapped reads (Millions) * exon length(KB)) total exon reads：某个样本mapping到特定基因的外显子上的所有的reads mapped reads (Millions) :某个样本的所有reads总和 exon length(KB)：某个基因的长度（外显子的长度的总和，以KB为单位）TPMi=(Ni/Li)*1000000/sum(N0/L0+……..+ Nm/Lm) Ni：mapping到基因i上的read数； Li：基因i的外显子长度的总和;m：为所有基因的总数 从以上公式我们可以进一步推导如下： 从以上公式我们可以进一步推导如下： 1234561、FPKMi*(mapped reads (Millions)= total exon reads/ (mapped reads (Millions) * exon length(KB))2、total exon reads/ (mapped reads (Millions) * exon length(KB))=(Ni/Li)3、FPKMi*(mapped reads (Millions)=(Ni/Li)4、TPMi=FPKMi*(mapped reads (Millions)*1000000/(N0/L0+……..+ Nm/Lm)5、TPMi=FPKMi*(mapped reads (Millions)*1000000/(FPKM0*(mapped reads (Millions)+……..+ FPKMm*(mapped reads (Millions))6、TPMi=FPKMi*1000000/(FPKM0+……..+ FPKMm) 最终我们得到了TPM和FPKM的转换公公式，从公式里可以看到TPM就是等于该基因的FPKM占所有基因的FPKM的总和的比例乘以一百万，那么值得注意的是每个样本所有基因的TPM加和就等于一百万了，这很类似样本间标准化最后你下载的TCGA FPKM数据转换TPM就不会懵逼了吧参考文献：https://academic.oup.com/bioinformatics/article/26/4/493/243395/RNA-Seq-gene-expression-estimation-with-read 5. 其他数据可以直接用TCGA assembler 2 下载。]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCGA样本命名详解]]></title>
    <url>%2Fhexo%2F2019%2F12%2F05%2F2019-12-05_TCGAsampleName%2F</url>
    <content type="text"><![CDATA[在TCGA中，一个患者可能会对应多个样本，如TCGA-A6-6650可以得到3个样本数据： TCGA-A6-6650-01A-11R-1774-07, TCGA-A6-6650-01A-11R-A278-07, TCGA-A6-6650-01B-02R-A277-07 大家知道一般在做TCGA数据分析的时候样本名实际上只保留到前四个元素（以”-“分割），例如TCGA-A6-6650-01。所以实际上上示3个样本一般只保留一个. TCGA：Project, 所有TCGA样本名均以这个开头，标志 A6：Tissue source site，组织来源编码，如A6就表示来源于Christiana Healthcare中心的结肠癌组织。更多编码所代表的意义详见： https://gdc.cancer.gov/resources-tcga-users/tcga-code-tables/tissue-source-site-codes 6650：Participant, 参与者编号 01：Sample, 这两个数字可以说是最关键、最被大家注意的，其中编号01~09表示肿瘤，10~19表示正常对照，如下： https://gdc.cancer.gov/resources-tcga-users/tcga-code-tables/sample-type-codes 所以在TCGA样本名中，这个位置最常见的就是01和11，当然偶尔也会有其他的数字 A：Vial, 在一系列患者组织中的顺序，绝大多数样本该位置编码都是A; 很少数的是B，表示福尔马林固定石蜡包埋组织，已被证明用于测序分析的效果不佳，所以不建议使用-01B的样本数据. 11：Portion, 同属于一个患者组织的不同部分的顺序编号，同一组织会分割为100-120mg的部分，分别使用R：Analyte, 分析的分子类型，对应关系如下所示：https://gdc.cancer.gov/resources-tcga-users/tcga-code-tables/portion-analyte-codes 1774：Plate, 在一系列96孔板中的顺序，值大表示制板越晚 07：Center, 测序或鉴定中心编码，更多编码详见：https://tcga-data.nci.nih.gov/datareports/codeTablesReport.htm?codeTable=center 一个借鉴的图片： 更多内容详见：https://wiki.nci.nih.gov/display/TCGA/TCGA+barcodehttp://docs.cavatica.org/docs/tcga-grch38-metadata 所以现在看这三个样本其区别就在于，前两个使用的是患者的冰冻组织做的测序，而第三个用的是福尔马林固定石蜡包埋组织；而前两个样本的区别在于同一组织后续使用了不同的96孔板。 理解了命名规则及三者命名上的主要区别后，现在可以重点解决如何从一个患者的多个样本中挑选样本的问题了，首先排除TCGA-A6-6650-01B-02R-A277-07，因为是01B，福尔马林固定石蜡包埋组织！剩下的两个： TCGA-A6-6650-01A-11R-1774-07TCGA-A6-6650-01A-11R-A278-07先看看GDAC firehose遇到这种情况怎么解决，总结起来就是： 1、对RNA数据来说，Analyte为R的优先级最该，其次是R和T，而对于DNA层面的分析来说，D的优先级最高。2、如果Analyte相同，那就选择Portion和/或Plate值更大的。所以按照GDAC firehose的方法，最终保留TCGA-A6-6650-01A-11R-A278-07，因为其相对于TCGA-A6-6650-01A-11R-1774-07的板号(Plate)更晚：https://github.com/BioinformaticsFMRP/TCGAbiolinks/issues/163虽然看起来可能这么选比较准确，但是稍微有些麻烦~ 然后是cBioPortal中的处理方式： 随机选择了一个，理由很简单啊，来源于同一个患者的癌组织样本差别不大，小编随机测试了两个样本，表达相关性值是大于0.8的。所以如果遇到需要选择的时候，就仁者见仁了，建议天秤座的小伙伴们也不要太纠结到底哪个最好，当然如果你有不同的意见和看法，欢迎交流讨论！ ————————————————版权声明：本文为CSDN博主「Mr番茄蛋」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qq_35203425/article/details/80851862]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCGA miRNA 表达谱矩阵的Name转换]]></title>
    <url>%2Fhexo%2F2019%2F12%2F05%2F2019-12-05_TCGA_miRNAName%2F</url>
    <content type="text"><![CDATA[TCGA数据库中miRNA名字都是小写表示，如何和基因以及成熟miRNA进行对应是个一直以来困惑的问题。 参见这篇文章]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>Data mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[判定基因名coding RNA OR noncoding RNA]]></title>
    <url>%2Fhexo%2F2019%2F12%2F05%2F2019-12-05_trancript_type%2F</url>
    <content type="text"><![CDATA[目前考虑两种方法 方法一 利用HNGC注释数据人类基因命名委员会（HGNC）（http://www.genenames.org) 包含了人类基因所有的命名，别名和ID，可以说是天下命名，无出其右．直接在下载页面 （http://www.genenames.org/cgi-bin/statistics）底部下载所有的命名关系的一个大表。 12345678910# bash axel ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt# 数据一致更新hgnc_geneset=fread(file = "hgnc_complete_set.txt",header = TRUE)xx=hgnc_geneset[1:10,]xx=hgnc_geneset[,c("symbol", "locus_group","status","entrez_id" )]yy=merge(gene_names,xx,by=c("symbol"),all.x = TRUE,sort=FALSE)index=match(gene_names1, yy$symbol)gene_names=yy[index,]save(sc_data_HMLE,gene_names, file="sc_data_HMLE.rda" )sum(na.omit(gene_names$locus_group)=="protein-coding gene") 方法二 利用Genecodehttps://www.gencodegenes.org/human/https://www.jianshu.com/p/3c290fee634f1234# 下载GTF文件处理即可 https://www.gencodegenes.org/human/# bash axel ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.chr_patch_hapl_scaff.annotation.gtf.gzfread(*.GTF)]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyGene.info R Client]]></title>
    <url>%2Fhexo%2F2019%2F11%2F26%2F2019-11-26_mygeneinfo%2F</url>
    <content type="text"><![CDATA[这是一个基因名字注释的数据库，可以查到很多信息，涉及基因组，转录组，蛋白组的很多名字，需要来熟悉这些名字。我对这些名字都不熟悉，如第一个例子. pdf]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Github机器学习100天]]></title>
    <url>%2Fhexo%2F2019%2F11%2F26%2F2019-11-26_githubDataMining%2F</url>
    <content type="text"><![CDATA[GitHub热门教程：100天搞定机器学习（中文版）https://github.com/Avik-Jain/100-Days-of-ML-Code-Chinese-Version https://github.com/Avik-Jain/100-Days-Of-ML-Code]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ggplot多图拼接]]></title>
    <url>%2Fhexo%2F2019%2F04%2F26%2F2019-04-26_ggplot%2F</url>
    <content type="text"><![CDATA[ggplot多图拼接1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# Multiple plot function## ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)# - cols: Number of columns in layout# - layout: A matrix specifying the layout. If present, 'cols' is ignored.## If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),# then plot 1 will go in the upper left, 2 will go in the upper right, and# 3 will go all the way across the bottom.#multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) &#123; library(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use 'cols' to determine layout if (is.null(layout)) &#123; # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) &#125; if (numPlots==1) &#123; print(plots[[1]]) &#125; else &#123; # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) &#123; # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) &#125; &#125;&#125;library(ggplot2)# This example uses the ChickWeight dataset, which comes with ggplot2# First plotp1 &lt;- ggplot(ChickWeight, aes(x=Time, y=weight, colour=Diet, group=Chick)) + geom_line() + ggtitle("Growth curve for individual chicks")# Second plotp2 &lt;- ggplot(ChickWeight, aes(x=Time, y=weight, colour=Diet)) + geom_point(alpha=.3) + geom_smooth(alpha=.2, size=1) + ggtitle("Fitted growth curve per diet")# Third plotp3 &lt;- ggplot(subset(ChickWeight, Time==21), aes(x=weight, colour=Diet)) + geom_density() + ggtitle("Final weight, by diet")# Fourth plotp4 &lt;- ggplot(subset(ChickWeight, Time==21), aes(x=weight, fill=Diet)) + geom_histogram(colour="black", binwidth=50) + facet_grid(Diet ~ .) + ggtitle("Final weight, by diet") + theme(legend.position="none") # No legend (redundant in this graph)#layout &lt;- matrix(c(1, 1, 1, 2, 2, rep(3, 5)), nrow = 2, byrow = TRUE)#multiplot(plotlist = list(p1, p2, p3,p4), layout = layout)multiplot(p1, p2, p3, p4, cols=2)]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>Data mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[下载参考基因组序列以及一些重要注释数据库汇总]]></title>
    <url>%2Fhexo%2F2019%2F04%2F03%2F2019-04-03_genomic_reference%2F</url>
    <content type="text"><![CDATA[参考基因组及注释文件下载 原文 一些关于基因组名字的含义 [ ] NC表示人类基因组DNA的RefSeq。 [ ] NM表示mRNA的RefSeq。 [ ] NP表示蛋白质的RefSeq。 1. 参考基因组hg19(UCSC)，GRCH37(NCBI)和Ensembl75(ENSEMBL)是三种国际生物信息学数据库资源收集存储单各自发布的基因组信息。 hg系列，hg18/19/38是目前使用频率最高的基因组。hg38是目前的最新版本。 NCBI Ensembl UCSC GRCh36 release_52 hg18 GRCh37 release_59/61/64/68/69/75 hg19 GRCh38 release_76/77/78/80/81/82 hg38 UCSC下载：12345678910111213141516171819202122232425##hg19：wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz ##hg19 tar -zxvf chromFa.tar.gzcat *.fa &gt;hg19.fa##hg38：wget http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.chromFa.tar.gz ##hg38tar -zxvf hg38.chromFa.tar.gzcat *.fa &gt;hg38.fa#http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/chromFa.tar.gz#http://hgdownload.cse.ucsc.edu/goldenPath/mm9/bigZips/chromFa.tar.gz#http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz#http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/chromFa.tar.gz# 或者用shell脚本指定下载的染色体号for i in $(seq 1 22) X Y M;do echo $i; wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/chr$&#123;i&#125;.fa.gz;done gunzip *.gz for i in $(seq 1 22) X Y M; do cat chr$&#123;i&#125;.fa &gt;&gt; hg19.fasta;donerm -fr chr*.fasta​ 但是注释文件下载稍微有点麻烦，需要设置一系列参数来生成： http://genome.ucsc.edu/cgi-bin/hgTablesSelect the following options:clade: Mammalgenome: Humanassembly: Feb. 2009 (GRCh37/hg19)group: Genes and Gene Predictionstrack: UCSC Genestable: knownGeneregion: Select “genome” for the entire genome.output format: GTF - gene transfer formatoutput file: enter a file name to save your results to a file, or leave blank to display results in the browserClick ‘get output’.123456789101112131415161718192021222324252627282930313233343536#### **NCBI下载****建议用迅雷或ncbi官网提供的下载器下载速度更快**```bash##GRCH37-- hg19：for i in $(seq 1 22) X Ydo nohup axel ftp://ftp.ncbi.nlm.nih.gov/genomes/Homo_sapiens/ARCHIVE/BUILD.37.3/Assembled_chromosomes/seq/hs_ref_GRCh37.p5_chr$&#123;i&#125;.fa.gz &amp;donefor i in $(seq 1 22) X Ydo gunzip -c ./download/hs_ref_GRCh37.p5_chr$&#123;i&#125;.fa.gz &gt; ./hs_ref_GRCh37.p5_chr$&#123;i&#125;.facat hs_ref_GRCh37.p5_chr$&#123;i&#125;.fa &gt;&gt;hg19.fasleep 10s;done##GRCH38-- hg38：同理，只要找到下载地址，改动一下即可。for i in $(seq 1 22) X Ydo nohup wget -c ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/ARCHIVE/ANNOTATION_RELEASE.109/Assembled_chromosomes/seq/hs_ref_GRCh38.p12_chr$&#123;i&#125;.fa.gz done#ftp://ftp.ncbi.nlm.nih.gov/genomes/Homo_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38.p7_chr$&#123;i&#125;.fa.gz for i in $(seq 1 22) X Ydo gunzip -c ./download/hs_ref_GRCh38.p12_chr$&#123;i&#125;.fa.gz &gt; ./hs_ref_GRCh38.p12_chr$&#123;i&#125;.facat hs_ref_GRCh38.p12_chr$&#123;i&#125;.fa &gt;&gt;hg38.fasleep 10s;done##注释文件gtfftp://ftp.ncbi.nlm.nih.gov/genomes/Homo_sapiens/ARCHIVE/ Ensembl下载12345#更改release后的数字下载相应的版本，包括dna、cdna、cds等序列信息，release-75是目前最新的hg19版本。ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/#注释文件下载（默认gtf，大部分比对软件输入格式）：ftp://ftp.ensembl.org/pub/release75/gtf/homo_sapiens/Homo_sapiens.GRCh37.75.gtf.gz 2. 重要基因名注释数据库genecode1axel ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.chr_patch_hapl_scaff.annotation.gtf.gz HNGC注释数据1axel ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt GTF注释文件NCBI：最新版（hg38） ● ftp://ftp.ncbi.nih.gov/genomes/H_sapiens/GFF/ 其它版本 ● ftp://ftp.ncbi.nlm.nih.gov/genomes/Homo_sapiens/ARCHIVE/Ensembl ● ftp://ftp.ensembl.org/pub/release-75/gtf/homosapiens/Homosapiens.GRCh37.75.gtf.gz变化上面链接中的release就可以拿到所有版本信息 ● ftp://ftp.ensembl.org/pub/ 3.UCSC Table 下载基因的特定区域http://genome.ucsc.edu/cgi-bin/hgTables 说明文档 http://blog.genesino.com/2013/05/ucsc-usages/ 4.根据gtf格式的基因注释文件得到人所有基因的染色体坐标原文 ：http://www.biotrainee.com/thread-472-1-1.html 1234567891011121314151617181920mkdir -p ~/reference/gtf/gencodecd ~/reference/gtf/gencode## https://www.gencodegenes.org/releases/current.htmlwget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.2wayconspseudos.gtf.gzwget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.long_noncoding_RNAs.gtf.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.polyAs.gtf.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.annotation.gtf.gz ## https://www.gencodegenes.org/releases/25lift37.html wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.annotation.gtf.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.metadata.HGNC.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.metadata.EntrezGene.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.metadata.RefSeq.gz zcat gencode.v25.long_noncoding_RNAs.gtf.gz |perl -alne '&#123;next unless $F[2] eq "gene" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;lncRNA.hg38.positionzcat gencode.v25.2wayconspseudos.gtf.gz |perl -alne '&#123;next unless $F[2] eq "transcript" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;pseudos.hg38.positionzcat gencode.v25.annotation.gtf.gz| grep protein_coding |perl -alne '&#123;next unless $F[2] eq "gene" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;protein_coding.hg38.positionzcat gencode.v25.annotation.gtf.gz|perl -alne '&#123;next unless $F[2] eq "gene" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;allGene.hg38.positionzcat gencode.v25lift37.annotation.gtf.gz | grep protein_coding |perl -alne '&#123;next unless $F[2] eq "gene" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;protein_coding.hg19.positionzcat gencode.v25lift37.annotation.gtf.gz | perl -alne '&#123;next unless $F[2] eq "gene" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;allGene.hg19.position 5. 在R语言中读取GTF文件的最好方法链接：https://www.jianshu.com/p/7f96adec2b0d 下载的TCGA数据是没有注释的，需要从ensemble上面下载GTF文件，现在需要把GTF文件读入R 第一种方法rtracklayer::import12345source("https://bioconductor.org/biocLite.R")biocLite("rtracklayer")biocLite("SummarizedExperiment")gtf1 &lt;- rtracklayer::import('Homo_sapiens.GRCh38.90.chr.gtf')gtf_df &lt;- as.data.frame(gtf1) 最终读入27个变量，2612129个观测，测试一下显示的不错 12test &lt;- gtf_df[1:5,]View(test) 取出我需要的gene_id,gene_biotype,gene_name，成为新的数据框 1geneid_df &lt;- dplyr::select(gtf_df,c(gene_name,gene_id,gene_biotype)) 第二种方法read.table1gtf2 &lt;- read.table('Homo_sapiens.GRCh38.90.chr.gtf', header = FALSE, sep = '\t') 最后发现，速度奇慢无比,取消参考了Y叔的帖子增加参数，读入成功，需要1分钟,读入9个变量 根据GTF画基因的多个转录本结构 12gtf2 &lt;- read.table('Homo_sapiens.GRCh38.90.chr.gtf', stringsAsFactors = F, header = FALSE, sep = '\t',comment = "#")test2 &lt;- gtf2[1:5,] 第三种方法readr包里面的read_table2速度很快，有进度条，读入了18个变量，但是最gene_biotype显示不对 1234gtf3 &lt;- readr::read_table2("Homo_sapiens.GRCh38.90.chr.gtf",comment = "#")gtf_df3 &lt;- as.data.frame(gtf3) #转成data.frametest3 &lt;- gtf_df3[1:5,]View(test3) 第四种方法read.table，fread读入数据跟read.table一样， 1234library(data.table)genes &lt;- fread("Homo_sapiens.GRCh38.90.chr.gtf")test4&lt;- genes[1:5,]View(test4) 上一步用时12秒，速度极快,只是读入后没有名字，需要自己添加 读入9个变量,跟read.table一样 1setnames(genes, names(genes), c("chr","source","type","start","end","score","strand","phase","attributes") ) 可选步骤，type那一项有转录本和gene，选取gene，其他有很多转录本 1genes &lt;- genes[type == "gene"] 我要的信息粗存在attributes中，构建函数提取 12345678extract_attributes &lt;- function(gtf_attributes, att_of_interest)&#123; att &lt;- strsplit(gtf_attributes,"; ") att &lt;- gsub("\"","",unlist(att)) if(!is.null(unlist(strsplit(att[grep(att_of_interest, att)], " "))))&#123; return( unlist(strsplit(att[grep(att_of_interest, att)], " "))[2]) &#125;else&#123; return(NA)&#125;&#125; 举例子解释,加入我们需要gene_id 12gtf_attributes &lt;- genes[1,9]att &lt;- strsplit(gtf_attributes,"; ") 这一步提示non-character argument，后面我查询才知道, 对于第九列的第一行的的获取两种方式返回的格式不一样 12genes[1,9] #data.framegenes$attributes[1] #character 而strsplit的对象是character,重来 123gtf_attributes &lt;- genes$attributes[1]att &lt;- strsplit(gtf_attributes,"; ")att &lt;- gsub("\"","",unlist(att)) grep获取位置，获取到内容后分隔,得到的第二个元素是我们需要的 1unlist(strsplit(att[grep("gene_id", att)], " "))[2] 利用函数获得基因列表 1genes$gene_id &lt;- unlist(lapply(genes$attributes, extract_attributes, "gene_id")) 如果我们选择使用第一种方法rtracklayer::import，这些事情都是不需要做的！！！ 总结：rtracklayer::import最简单，fread最快，read.table可选，read_table没戏！]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>Data mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[conda命令]]></title>
    <url>%2Fhexo%2F2019%2F03%2F21%2F2019-03-21_conda%2F</url>
    <content type="text"><![CDATA[Ubuntu里默认python为2.7，如何修改为python3版本呢，可以利用alternatives机制更改py3为默认。12sudo update-alternatives --install /usr/bin/python python /usr/bin/python2 100 sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 150 conda 命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364##创建环境conda create -n py3 python=3 ##安装python3最新版本conda create -n py2 python=2 ##安装python2最新版本conda create -n py_33 python=3.3 ##删除环境conda env remove -n py_33##列出所有环境conda env list ##进入离开环境conda activate py3conda deactivate## 搜索，安装，删除包conda search numpyconda install package_name ##conda install numpy=1.10conda remove package_namefor example： which masc2 在系统中查找masc2这个软件## 查看安装哪些包conda listconda upgrade --all：更新所有包## 导出环境conda activate py3conda env export &gt; py3.yaml## 导入环境conda env create -f py3.yaml## 共享环境pip freeze## 添加源####清华源#conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/#conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge #conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes # 设置搜索时显示通道地址#去掉~/.condarc里面的 defaults 能加快速度 改https为http也有助于连接#附加库conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud//pytorch/conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/###中科大源conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/conda config --set show_channel_urls yes##换回默认源：conda config --remove-key channels##取消base环境自动激活conda config --show | grep auto_activate_baseconda config --set auto_activate_base False## conda 清理conda clean -p //删除没有用的包conda clean -t //tar打包conda clean -y -all //删除所有的安装包及cache ======================================================================== 使用Bioconda管理Linux系统中的生物信息软件 原文 生物信息操作中必不可少的就是Linux系统中各种生物信息学软件的安装。不同软件有不同的安装方法，对系统环境的依赖不同也不同，对于新手来说，经常是一个软件的安装和配置就要折腾很长一段时间时间，大大增加了学习成本。 我自己有两个方法来尽量减少安装软件所消耗的时间：一是直接安装Bio-linux系统，这个系统已经内置了大部分生物信息分析所需要的软件，非常适合新手直接学习分析技术，绕过软件安装和环境配置的麻烦问题。二是使用Bioconda安装和管理各种软件。Bio-linux系统和常用的服务器系统还是有差别的，如果想在学习生物信息分析的同时掌握一些Linux系统的操作甚至维护的技术，配置一台CentOS系统的计算机就很有必要了。这个时候Bioconda就非常有用了。 本文参考知乎专栏以及基因课相关课程 (http://genek.tv/dirlist/index/id/65) 对Bioconda的安装和使用做简单介绍。 Bioconda介绍Bioconda是conda上一个分发生物信息的频道。而conda是最初为管理python包而建立的。以下是相关介绍： “Conda is a portable package manager primarily for python and precompiled binaries. Miniconda is the base system of conda. It includes a standard python and a few required dependencies such as readline and sqlite. In conda, a channel contains a set of software typically managed by the same group.Bioconda is a channel of conda focusing on bioinformatics software. ” Bioconda主页：Using bioconda - Bioconda documentation anaconda、miniconda和conda的区别：FAQs - Bioconda documentation 简单说来：“conda is a package manager, Miniconda is the conda installer, and Anaconda is a scientific Python distribution that also includes conda.” Bioconda的优点是安装简单，各个软件依赖的环境一同打包且相互隔离，非常适合在服务器中建立自己的生物信息分析环境。 Bioconda的下载与安装1.下载和安装miniconda bioconda的使用首先需要安装miniconda(http://conda.pydata.org/miniconda.html) 。选择linux的64位的python2.7版本（共提供win、Mac、linux三种系统，同时支持python3和python2），直接点击下载。或者复制链接后，用wget下载。下载完成后，在终端键入bash命令进行安装. 之后按照提示点击回车，输入要安装的位置，或者输入yes. 1234wget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.shbash Miniconda2-latest-Linux-x86_64.sh##输入yes后，还没有完成最后安装，还需要source一下source ~/.bashrc 这时miniconda就安装好了，输入“conda”会显示相应的信息： 2.添加channels 输入“conda list”来查看已经安装的软件： 1234conda config --add channels conda-forgeconda config --add channels defaultsconda config --add channels rconda config --add channels bioconda 查看已经添加的channels： conda config —get channels 3.更新miniconda conda update conda 4.卸载miniconda 删除miniconda的整个文件夹： rm -rf ~/miniconda 从环境变量中去掉miniconda：打开~/.bash_profile文件，删掉其中miniconda的路径，关闭并保存 删除隐藏的.condarc 、.conda以及.continuum文件 利用Bioconda安装生物信息软件要通过conda安装软件，首先从这里Available packages查找该软件是否被conda支持。如果支持，只需输入以下命令即可安装： 1234567conda install fastqc（软件名）conda install -c bioconda samtools=1.5conda install -c bioconda htseq=0.7.2conda install -c bioconda hisat2=2.1.0conda install -c bioconda fastqc=0.11.5conda install -c jfear sratoolkit=2.8.1 安装完成后，可以用“which 软件名”来查看该软件安装的位置： conda默认安装软件的最新版本，如果想安装指定版本的某个软件，可以先用“conda search 软件名”搜索软件版本 星号标记的表示是已经安装的版本。要安装其他版本，输入： conda install 软件名=版本号这时conda会先卸载已安装版本，然后重新安装指定版本。 查看已安装软件：conda list 更新指定软件：conda update 软件名 卸载指定软件：conda remove 软件名]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>Data mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markov Affinity-based Graph Imputation of Cells (MAGIC)]]></title>
    <url>%2Fhexo%2F2019%2F03%2F18%2F2019-03-18_magic%2F</url>
    <content type="text"><![CDATA[MAGIC (Markov Affinity-based Graph Imputation of Cells), is a method for imputing missing values restoring structure of large biological datasets. David van Dijk, et al. Recovering Gene Interactions from Single-Cell Data Using Data Diffusion. 2018. Cell.30724-4) https://github.com/KrishnaswamyLab/MAGIC 数据 https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114397 https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP145597]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[t-SNE：最好的降维方法之一]]></title>
    <url>%2Fhexo%2F2019%2F03%2F18%2F2019-03-18_t-NSE%2F</url>
    <content type="text"><![CDATA[教程]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>Data mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单样本得分 Single sample scoring-EMT score]]></title>
    <url>%2Fhexo%2F2019%2F03%2F17%2F2019-03-17_Single_sample_scoring%2F</url>
    <content type="text"><![CDATA[1.文献 Foroutan, M., et al. (2018). “Single sample scoring of molecular phenotypes.” BMC Bioinformatics 19(1): 404. （这篇文章需要好好学习一下） Hanzelmann, S., et al. (2013). “GSVA: gene set variation analysis for microarray and RNA-seq data.” BMC Bioinformatics 14: 7. 2. 计算方法Foroutan, M 文章中有详细总结，包括自己提出的共5种。前4种集成在GSVA package中。singscore有自己单独的package。 Several approaches have been developed to score individual samples against molecular signatures (or gene sets), including: ssGSEA (single sample gene set enrichment analysis) [1], GSVA (gene set variation analysis)[2], PLAGE (pathway level analysis of gene expression)[3] and combining z-scores [4]. Hänzelmann et al. (2013) implemented all four of these methods within the R/Bioconductor package GSVA and performed a detailed comparison [2, 5]. ssGSEA (single sample gene set enrichment analysis) [1] GSVA (gene set variation analysis)[2], PLAGE (pathway level analysis of gene expression)[3] combining z-scores [4]. singscore 3. 其他问题学习这个是因为需要计算EMT score. 所以从道理上这些方法都可以用来计算EMT score. 但是EMT score计算还有一个经典文章。文章中关于EMT score 采用A sample using a two-sample Kolmogorov–Smirnov test (2KS). Tan, T. Z., et al. (2014). &quot;Epithelial-mesenchymal transition spectrum quantification and its efficacy in deciphering survival and drug responses of cancer patients.&quot; EMBO Mol Med 6(10): 1279-1293.]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>EMT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激光捕获显微切割]]></title>
    <url>%2Fhexo%2F2019%2F03%2F15%2F2019-03-15_laser_captuer%2F</url>
    <content type="text"><![CDATA[激光捕获显微切割（Laser capture microdissection，LCM）在不破坏组织结构，保存要捕获的细胞和其周围组织形态完整的前提下，直接从冰冻或石蜡包埋组织切片中获取目标细胞 ，通常用于从组织中精确地分离一个单一的细胞。 机体组织包含有上百种不同的细胞，这些细胞各自与周围的细胞、基质、血管、腺体、炎症细胞或免疫细胞相互粘附。在正常或发育中的组织器官内，细胞内信号、相邻细胞的信号以及体液刺激作用于特定的细胞，使这些细胞表达不同的基因并且发生复杂的分子变化。在病理状态下，如果同一类型的细胞发生了相同的分子改变，则这种分子改变对于疾病的发生可能起着关键性的作用。然而，发生相同分子改变的细胞可能只占组织总体积的很小一部分；同时，研究的目标细胞往往被其它组织成分所环绕。为了对疾病发生过程中的组织损害进行分子水平分析，分离出纯净的目标细胞就显得非常必要。1996年，美国国立卫生院（NIH）国家肿瘤研究所的开发出激光捕获显微切割技术，次年，美国Arcturus Engineering公司成功研制激光捕获显微切割系统，并实现商品化销售。应用该技术可以在显微镜直视下快速、准确获取所需的单一细胞亚群，甚至单个细胞，从而成功解决了组织中细胞异质性问题。这项技术现已成为美国“肿瘤基因组解剖计划”的一项支撑技术 。]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Cell line 细胞系理解]]></title>
    <url>%2Fhexo%2F2019%2F03%2F15%2F2019-03-15_cell_line%2F</url>
    <content type="text"><![CDATA[一直以来对cell line 有错误认识。癌症细胞系是从基于癌细胞样本培养的细胞株系。 http://www.bioon.com/article/6725882.html 细胞系是癌症研究的支柱。这些细胞群体，通常是从患者的肿瘤样本中收集并在实验室中进行培养的，可在体外无限制地生长，从而能够用于从基础遗传研究到药物发现的一切应用中。不过，虽然科学家们认为即便单个细胞系持续地生长和分裂，它们仍然保持遗传上的一致性，但是在一项新的研究中，来自美国布罗德研究员、达纳-法伯癌症研究所、哈佛医学院、麻省总医院、布莱根妇女医院和霍华德-休斯医学研究所的研究人员发现它们实际上能够以大幅改变它们对药物作出反应的方式进行进化。细胞系中的细胞株持续进化—-可能是由它们生长的实验室条件触发的—-可能有助于解释为何使用相同细胞系的不同研究通常会产生相互矛盾的结果。]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Circos plot]]></title>
    <url>%2Fhexo%2F2019%2F03%2F15%2F2019-03-16_Circos_plot%2F</url>
    <content type="text"><![CDATA[由Martin Krzywinski等人开发的Circos，一出现便引起轰动，它打破了常规基因组学数据可视化的思路，通过对差异化交互数据的多维度展示，可以从不同层次全方位描述组学信息，让组学数据展示变成了艺术品。由于其实用性、美观性，Circos如今也广泛应用于社交网络、交通运输等领域。 circos]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TP53突变型和TP53野生型BRCA病人的差异分析结果（jimmy）]]></title>
    <url>%2Fhexo%2F2019%2F03%2F13%2F2019-03-13_TP53_BRCA%2F</url>
    <content type="text"><![CDATA[这个教程有很多学习练习之处，首先解决了国内安装Bioconductor的问题。 教程]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[乳腺癌专题]]></title>
    <url>%2Fhexo%2F2019%2F03%2F13%2F2019-03-13_breastcancer%2F</url>
    <content type="text"><![CDATA[今天2019年基金申请提交完毕。未来几年我想以乳腺癌为主开展研究，所以有必要对乳腺癌相关知识做一个梳理，以备不时之查。 乳腺癌免疫组化指标 ER： estrogen receptors（雌激素受体） PR： progesterone receptors（黄体酮，孕激素受体） HER2：（人类表皮生长因子2受体） 受体：这是内部和细胞表面发现的蛋白质。这些受体蛋白质是细胞的“眼睛”和“耳朵”，从血液中的物质接收信息，然后告诉细胞做什么。 乳腺癌临床亚型乳腺癌临床亚型 总结的很充分完全(必看）乳腺癌分子分型研究进展 大约2/3的女性患有激素受体阳性的乳腺癌。健康乳房细胞内部和表面的激素受体接受来自雌激素和黄体酮的信息。激素附着在受体上，并提供帮助细胞继续生长和功能良好的说明。大部分但不是全部的乳腺癌细胞也具有这些激素受体。 较小比例的乳腺癌（约20-30％）具有太多的HER2受体。在正常健康的乳腺细胞中，HER2受体接收刺激其生长的信号。然而，由于HER2受体过多，乳腺癌细胞生长和分裂过快。荷尔蒙疗法和HER2靶向疗法的作用是干扰荷尔蒙和HER2对乳腺癌的影响，这可以帮助减缓甚至阻止乳腺癌细胞的生长。 在实验室中约10-20％的乳腺癌对激素受体和HER2都呈阴性，这意味着它们是三重阴性的。包括他莫昔芬，Arimidex（化学名称：anastrozole），Aromasin（化学名称：依西美坦），Femara（化学名称：来曲唑）和Faslodex（化学名称：氟维司群）等荷尔蒙疗法不太可能对癌症产生反应。赫赛汀（化学名：曲妥珠单抗）或泰克（化学名：拉帕替尼），三阴性乳腺癌也不太可能响应靶向HER2的药物。 三阴乳腺癌更具侵略性(jimmy 2017-11-17)https://www.breastcancer.org/symptoms/diagnosis/trip_neg?what Triple Negative Breast Cancer，TNBC (Basal-like)：(ER-), (PR-), and (HER2-) 研究表明，三阴性乳腺癌更可能扩散到乳房外，更有可能在治疗后复发（回来）。这些风险在治疗后的头几年似乎是最大的。例如，2007年对加拿大1600多名女性进行的一项研究发现，患有三阴性乳腺癌的女性在乳房外复发的风险较高 - 但仅在前三年。其他研究也得出了类似的结论。随着岁月的流逝，三阴乳腺癌复发的风险与其他类型乳腺癌的风险水平相似。 三阴乳腺癌的五年生存率也往往较低。2007年对50,000多名乳腺癌各阶段妇女的研究发现，77％的三阴乳腺癌患者至少存活5年，而其他类型乳腺癌患者中有93％存活。对2007年出版的1,600多名女性进行的另一项研究发现，患有三重阴性乳腺癌的女性在诊断后的5年内死亡的风险较高，但在这段时间之后没有。这些研究和其他研究中的复发和生存率是所有三阴乳腺癌患者的平均值。诸如乳腺癌的分级和阶段等因素将影响个体女性的预后。倾向于比其他类型的乳腺癌更高的等级。等级越高，癌细胞就越不会像正常的，健康的乳房细胞的外观和生长模式。在1至3的范围内，三阴乳腺癌通常是3级。 三阴乳腺癌通常是称为“基底样”的细胞类型。文献： 基底细胞样癌：一种被新近认识的乳腺癌亚型 “基底样（basal-like）”意味着细胞类似乳房管道的基底细胞。这是研究人员使用基因分析技术确定的新型乳腺癌亚型。像其他类型的乳腺癌一样，基底样癌症可以与家族史相关联，也可以在没有任何明显的家族联系的情况下发生。基底样癌症往往是更积极，更高级别的癌症 - 就像三重阴性乳腺癌。据信大多数三阴性乳腺癌是基底样细胞类型。因为三阴性乳腺癌往往比其他类型的侵略性而且导致患者通常不是现有成熟疗法的候选人，如激素疗法和赫赛汀治疗。但是三阴性乳腺癌可以通过化疗和放疗来治疗，而新的治疗方法如PARP抑制剂则显示了前景。研究人员对三阴性乳腺癌给予了极大的关注，并努力寻找新的更好的治疗方法。这是乳腺癌领域研究的一个非常热门的领域，“医学肿瘤学家兼Breastcancer.org专业顾问委员会成员George Sledge博士说。 药物开发商，制药公司和乳腺癌实验室研究人员对这些患者的靶向治疗有着浓厚的兴趣。“ 阅读原文 整合近5万乳腺癌患者发现BRCA基因缺陷者倾向于患TNBC发表于 Front. Pharmacol., 21 August 2018，也算是新鲜出炉了，文章链接是 https://doi.org/10.3389/fphar.2018.00909 ，该研究整合了527篇文章里面的近5万乳腺癌患者信息，分为： 868 BRCA1 mutations (BRCA1 Mut) carriers 739 BRCA2 mutations (BRCA2 Mut) carriers 45,263 non-carriers. 但是文章的结论平淡无奇，就是携带BRCA1基因的先天性的病理性突变的比携带BRCA2基因的先天性的病理性突变的要显著的高发TNBC，当然高于那些没有这两个基因的其他人。有着BRCA1基因的先天性的病理性突变的肿瘤通常更恶性，突变数量也更多。]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[华山]]></title>
    <url>%2Fhexo%2F2019%2F01%2F27%2F2019-01-27_huashan%2F</url>
    <content type="text"><![CDATA[1月26日华山， 幼习知五岳，加之论剑于华山之武侠情使华山于五岳之中名气最盛。自古华山一条道，华山险峻是不曾涉足华山的的粗识。华山俊险盛名也让我心驰神往。 在过去，如果山太过于险，我也不确定有攀越之勇气。但是我性格中总是存在一种创新，打破常规，尝试新事物的执念。]]></content>
      <categories>
        <category>行者无疆</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2019]]></title>
    <url>%2Fhexo%2F2018%2F12%2F31%2F2018-12-31_newyear%2F</url>
    <content type="text"><![CDATA[岁月不居，时节如流。2018！ 少年听雨歌楼上。红烛昏罗帐。壮年听雨客舟中。江阔云低、断雁叫西风。而今听雨僧庐下。鬓已星星也。悲欢离合总无情。一任阶前、点滴到天明。 昨夜寒蛩不住鸣。惊回千里梦，已三更。起来独自绕阶行。人悄悄，帘外月胧明。白首为功名。旧山松竹老，阻归程。欲将心事付瑶琴。知音少，弦断有谁听。 当年万里觅封侯，匹马戍梁州。关河梦断何处？尘暗旧貂裘。胡未灭，鬓先秋，泪空流。此生谁料，心在天山，身老沧洲。]]></content>
      <categories>
        <category>闲情</category>
      </categories>
      <tags>
        <tag>感怀</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[英语-表达-写作]]></title>
    <url>%2Fhexo%2F2018%2F12%2F28%2F2025-01-01-English_expression%2F</url>
    <content type="text"><![CDATA[1. 短语1. 数据多huge amounts of high-dimensional dataaccumulate at an unprecedented rateheavily used 广泛应用an enormous collection of data setsin a speed unmatchable by the human’s capacity of data processionwe are experiencing a rapid growth in volume and diversity of biological data, presenting an increasing challenge for biologists.A plethora of studies have shown that 大量的研究表明 2. 肿瘤表达ten distinct malignancies 恶性肿瘤each single-tissue cancer type 单个癌症类型drives cancer initiation and progressiondynamics in cancer invasion and metastasis, progression, treatment response and survival.therapeutic regimes 治疗方案patient cohort 病人群体Inferred orthologous reactions 推断同源反应Complex genetic traits复杂的遗传性状DNA重排（rearrangement），基因扩增（amplification），点突变（point mutation）coordinated amplification in the DNA （ DNA 扩增） overexpression in the mRNA （基因的过度表达）epigenetic plasticity 表观遗传可塑性in a plethora of cancer-related events, including cancer invasion, metastasis, resistance to cell death, refractory responses to chemotherapy and immunotherapy, immunosuppression and the acquisition of stem cell-like properties morphological and biochemical characteristics typical of形态和生化特征Immunohistochemistry staining（免疫组化染色） heatmap of Oestrogen Receptor (ER), Progesterone Receptor (PR), and Epi (CDH1, ERBB2, CK19) as well as Mes (CK5, VIM, CDH2)stromal microenvironment 基质环境 a disease of significant morbidity and mortality 发病率和致死率 3. 短语the best finish ever for 有史以来最好的…readily intelligible to 容易理解ubiquitous in a variety of domains 普遍存在的presents challenges for effective and efficient data managementa critical issue is known as the curse of dimensionalityAlternatively authors 另外的作者are statistically significant 统计显著quite intuitively 非常直观的weak yet consistent 弱但是一致discriminative power 辨别力frequently used scoring criteria/metricsdifferent feature panelsphysically preserved 完整的保存vastly applied in biomedical researchsingle data view 单组数据视角substantial challenges 重大的挑战in terms of readability and interpretability 关于可读性性和可解释性( 依据/在xxx方面)pros and cons 正面和反面each single-tissue cancer type can be further divided into many molecular subtypes.Cancers are typically classified using pathologic criteria that rely heavily on the tissue site of origin（ 在原组织中） Yet, there is still no quantitative measure(定量) to assess the interplay（相互影响） between xxx and xxxwe derived a method for 我们推出一种方法extensively cross-referenced to other resources 广泛的交叉引用is of paramount importance in a plethora of 至关重要的 在大量的 still inadequately documented 没有被充分的研究This is so even after 在xxxx之后情况也是a panel of一组 true positivealso known as 2. 句子EMT scoring is thus a promising, versatile tool for the objective and systematic investigation of EMT roles Biological processes are intricate and the relationships among features (such as genes, metabolites andproteins) are complicated and evolve with dynamic physiological processes. Thus, analyzing data from the perspective of networks could provide more information to understand the associations among features and discover important markers. Elevated BCAR4 expression is correlated with higher metastatic potential and shorter survival time of breast cancer patients.In recently years, high-throughput experimental techniques such as microarray, RNA-Seq and mass spectrometry can detect cellular molecules at systems-level. These kinds of analyses generate huge quantitaties of data, which need to be given a biological interpretation. Cite for identifying predominant biological themes of a collection of genes. Through genomic profiling of 259 men with prostate cancer, scientists have identified five groups of prostate cancer with distinct DNA signatures. The discovery represents a major advance as researchers can now begin trying to tailor therapies to those subtypes. The approach has worked well in breast cancer and helped millions avoid the unnecessary cost, pain and time spent on treatments that are destined to fail. [Obama touts ‘lifesaving’ potential of personalized medicine] Such work is the backbone of President Obama’s $215 million precision medicine initiative announced in January, which aims to pioneer a new approach to how we treat disease by moving away from a “one-size-fits-all” approach to medicine to one that takes into account things like a person’s genetic makeup, or the genetic profile of a tumor. [A new way to study cancer and its treatments] The prostate cancer study, reported in EBioMedicine, used samples from 482 tumors from those men, who were part of studies in Cambridge, Britain, and Stockholm. The scientists identified 100 genes associated with prostate cancer, including 94 that had not been previously associated with the disease. Most importantly, they found that a small subset of the 100 genes predicted poor prognosis better than any other method that had been used in a clinic setting. [He had a 3.5-pound tumor and months to live. Here’s how he survived.] “For the first time in prostate cancer this study demonstrates the importance of integrated genomic analyses incorporating both benign and tumour tissue data in identifying molecular alterations leading to the generation of robust gene sets that are predictive of clinical outcome in independent patient cohorts,” the researchers wrote. They said this information could be “used for early detection of aggressive cases in a clinical setting, and inform treatment decisions.” One of the most challenging problems in biomedical research is to understand the underlying mechanisms of complex diseases. Great effort has been spent on finding the genes associated to diseases (Botstein and Risch, 2003; Kann, 2009). However, more and more evidences indicate that most human diseases cannot be attributed to a single gene but arise due to complex interactions among multiple genetic variants and environmental risk factors (Hirschhorn and Daly, 2005). Several databases have been developed storing associations between genes and diseases such as CTDTM (Davis,et al., 2014), OMIM® (Hamosh et al., 2005) and the NHGRI-EBI GWAS catalog (Welter et al., 2014). Each of these databases focuses on different aspects of the phenotype-genotype relationship, and due to the nature of the database curation process, they are not complete. Hence, integration of different databases with information extracted from the literature is needed to allow a comprehensive view of the state of the art knowledge within this research field. With this need in mind, we have created DisGeNET.cite 3.论文表达Visualization plays an important role in understanding and in knowledge advancement on base of empirical data. In initial stages of research of such type, visualization helps to explore the data interactively, to get an overview and to create meaningful hypotheses; in later stages, it helps to control and to steer partially automated analyses; and in final stages, where fully automated data analysis procedures are available, it provides summaries of the results that foster our understanding. Visualization is also instrumental in design, being it e.g. the design of pharmaceutically active compounds, the design of objects, or the planning of clinical therapies. Since full feature selection comparison is very hard to do without a full predictive model that combines both the feature selection and the classification stages we test feature selection methods in combination with one classification method – the linear support vector machine (SVM) (后面文章会用到) The availability of large cohorts and multiple different types of data at the DNA, RNA, and protein levels has made the Pan-Cancer project possible. Despite obvious uniqueness of each tumor type at the molecular level， researchers are finding common large-scale processes that might be involved in multiple different tumors.]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Granger因果检验]]></title>
    <url>%2Fhexo%2F2018%2F12%2F28%2F2018-12-29_GrangerCausality%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/baimafujinji/article/details/6496224 123456789101112131415161718192021222324252627GDP &lt;- c(3645.2, 4062.6, 4545.6, 4889.5, 5330.5, 5985.6, 7243.8, 9040.7, 10274.4, 12050.6, 15036.8, 17000.9, 18718.3, 21826.2, 26937.3, 35260.0, 48108.5, 59810.5, 70142.5, 78060.8, 83024.3, 88479.2, 98000.5, 108068.2, 119095.7)consumption &lt;- c(1759.1, 2014, 2336.9, 2627.5, 2867.1, 3220.9, 3689.5, 4627.4, 5293.5, 6047.6, 7532.1, 8778, 9435, 10544.5, 12312.2, 15696.2, 21446.1, 28072.9, 33660.3, 36626.3, 38821.8, 41914.9, 46987.8, 50708.8, 55076.4)stat_data &lt;- data.frame(GDP, consumption)ts.data &lt;-ts(stat_data,frequency=1,start=c(1978))ts.datalibrary(lmtest)grangertest(GDP ~ consumption, order = 2, data =ts.data)Granger causality testModel 1: GDP ~ Lags(GDP, 1:2) + Lags(consumption, 1:2)Model 2: GDP ~ Lags(GDP, 1:2) Res.Df Df F Pr(&gt;F)1 18 2 20 -2 1.6437 0.221grangertest(consumption ~ GDP, order = 2, data =ts.data)Granger causality testModel 1: consumption ~ Lags(consumption, 1:2) + Lags(GDP, 1:2)Model 2: consumption ~ Lags(consumption, 1:2) Res.Df Df F Pr(&gt;F) 1 18 2 20 -2 13.411 0.0002717 ***---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 显然，检验结果表示，当原假设为“consumption不是引起GDP变化的Granger原因”，P值为0.221&gt;0.05，我们无法拒绝原假设；而当原假设为“GDP不是引起consumption变化的Granger原因”时，P值为0.0002717&lt;0.05，我们可以拒绝原假设。因此，可以证明：GDP是consumption的Granger原因。之前计算结果中的P值也可以用下面代码算得（注意当p=2时，观测值的数量n=25-2=23）： pf(1.6437, 2, 18, lower.tail = FALSE)[1] 0.220978pf(13.411, 2, 18, lower.tail = FALSE)[1] 0.0002716636]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[☆基因名称转换☆]]></title>
    <url>%2Fhexo%2F2018%2F12%2F28%2F2018-12-28_geneIDtransform%2F</url>
    <content type="text"><![CDATA[1. clusterProfiler包bitr: 基因集ID转换认为是最便捷基因 名转换方式，封装的非常好。依赖于org.Hs.eg.db注释包。 123456789101112131415161718x &lt;- c("GPX3", "GLRX", "LBP", "CRYAB", "DEFB1", "HCLS1", "SOD2", "HSPA2", "ORM1", "IGFBP1", "PTHLH", "GPC3", "IGFBP3","TOB1", "MITF", "NDRG1", "NR1H4", "FGFR3", "PVR", "IL6", "PTPRM", "ERBB2", "NID2", "LAMB1", "COMP", "PLS3", "MCAM", "SPP1", "LAMC1", "COL4A2", "COL4A1", "MYOC", "ANXA4", "TFPI2", "CST6", "SLPI", "TIMP2", "CPM", "GGT1", "NNMT", "MAL", "EEF1A2", "HGD", "TCN2", "CDA", "PCCA", "CRYM", "PDXK", "STC1", "WARS", "HMOX1", "FXYD2", "RBP4", "SLC6A12", "KDELR3", "ITM2B")eg = bitr(x, fromType="SYMBOL", toType="ENTREZID", OrgDb="org.Hs.eg.db")head(eg)library(org.Hs.eg.db)keytypes(org.Hs.eg.db)## [1] "ACCNUM" "ALIAS" "ENSEMBL" "ENSEMBLPROT" ## [5] "ENSEMBLTRANS" "ENTREZID" "ENZYME" "EVIDENCE" ## [9] "EVIDENCEALL" "GENENAME" "GO" "GOALL" ## [13] "IPI" "MAP" "OMIM" "ONTOLOGY" ## [17] "ONTOLOGYALL" "PATH" "PFAM" "PMID" ## [21] "PROSITE" "REFSEQ" "SYMBOL" "UCSCKG" ## [25] "UNIGENE" "UNIPROT" 2. biomaRt软件包获得在线注释信息http://blog.qiuworld.com:8080/archives/3281 1234567891011121314151617library("biomaRt")#载入biomaRt包mart &lt;- useMart("ensembl","hsapiens_gene_ensembl")entrez &lt;- c("673","7157","837")getBM(attributes=c("entrezgene","hgnc_symbol","ensembl_gene_id","affy_hg_u133_plus_2"),filters ="entrezgene",values = entrez,mart = mart)#如何知道有哪些服务器，以及这些服务器上哪些数据库呢？用listMarts(),listDatasets()两个函数marts &lt;- listMarts() head(marts)#查看当前可用的数据源ensembl &lt;- useMart("ensembl")#使用ensembl数据源datasets &lt;- listDatasets(ensembl);datasets[1:10,]#查看ensembl中可用数据库#如何获取getBM中attributes，filters？用listFilters()以及listAttributes()mart &lt;- useMart("ensembl","hsapiens_gene_ensembl")filters &lt;- listFilters(mart)filters[grepl("entrez", filters[,1]),]listAttributes(mart)attributes[grepl("^ensembl|hgnc", attributes[,1]),] biomaRt还会有很多SNP, alternative splicing, exon, intron, 5’utr, 3’utr等等信息。 3. 原生org.Hs.eg.db12345678#AnnotationDb objects columns(x) keytypes(x) keys(x, keytype, ...) select(x, keys, columns, keytype, ...) mapIds(x, keys, column, keytype, ..., multiVals) saveDb(x, file) loadDb(file, packageName=NA) http://davetang.org/muse/2013/05/23/using-the-bioconductor-annotation-packages/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657library(org.Hs.eg.db)cols(org.Hs.eg.db)[1] "ENTREZID""PFAM""IPI""PROSITE""ACCNUM""ALIAS""CHR"[8] "CHRLOC""CHRLOCEND""ENZYME""MAP""PATH""PMID""REFSEQ"[15] "SYMBOL""UNIGENE""ENSEMBL""ENSEMBLPROT""ENSEMBLTRANS""GENENAME""UNIPROT"[22] "GO""EVIDENCE""ONTOLOGY""GOALL""EVIDENCEALL""ONTOLOGYALL""OMIM"[29] "UCSCKG"keytypes(org.Hs.eg.db)[1] "ENTREZID""PFAM""IPI""PROSITE""ACCNUM""ALIAS""CHR"[8] "CHRLOC""CHRLOCEND""ENZYME""MAP""PATH""PMID""REFSEQ"[15] "SYMBOL""UNIGENE""ENSEMBL""ENSEMBLPROT""ENSEMBLTRANS""GENENAME""UNIPROT"[22] "GO""EVIDENCE""ONTOLOGY""GOALL""EVIDENCEALL""ONTOLOGYALL""OMIM"[29] "UCSCKG"#which chromosome does the gene TP53 reside?select(org.Hs.eg.db, keys="TP53", cols=c("SYMBOL", "CHR"), keytype="SYMBOL")SYMBOL CHR1 TP53 17#find out the keys for the keytype CHRkeys(org.Hs.eg.db, "CHR")[1] "19""12""8""14""3""2""17""16""9""X""6""1""7""10""11""22""5""18""15""Y""20"[22] "21""4""13""MT""Un"#this question was asked in the lecture#How many genes are there on chromosome 22 are in this annotation database?#store all symbolssymbol &lt;- keys(org.Hs.eg.db, "SYMBOL")#how many gene symbolslength(symbol)[1] 43819#distribution of gene symbols along the chromosomessymbol_chr &lt;- select(org.Hs.eg.db, keys=symbol, cols=c("CHR","SYMBOL"), keytype="SYMBOL")Warning message:In .generateExtraRows(tab, keys, jointype) : 'select' resulted in 1:many mapping between keys and return rows#the above warning is for duplicated rows#double check how many gene symbols are in symbol_chrlength(symbol_chr$SYMBOL)[1] 43868#unique oneslength(unique(symbol_chr$SYMBOL))[1] 43819#distribution of symbols on chromosomestable(symbol_chr$CHR) 1 10 11 12 13 14 15 16 17 18 19 2 20 21 22 3 4 5 6 7 8 4173 1604 2524 1945 1124 1679 1475 1597 2079 674 2245 2812 1017 546 1029 2306 1702 1887 2432 2307 1586 9 MT Un X Y 1790 37 219 2087 535plot(table(symbol_chr$CHR), xlab="Chromosomes", ylab="Number of genes")#Find the GO IDs for the TP53 genetp53_go &lt;- select(org.Hs.eg.db, keys="TP53", cols=c("SYMBOL","GO"), keytype="SYMBOL")##转换ensids &lt;- c("ENSG00000130720", "ENSG00000103257", "ENSG00000156414","ENSG00000144644","ENSG00000159307", "ENSG00000144485")cols &lt;- c("SYMBOL", "GENENAME"，"ENTREZID")select(org.Hs.eg.db, keys=ensids, columns=cols, keytype="ENSEMBL")或者：select(org.Hs.eg.db, keys="BRCA1", columns= c("ENSEMBL"，"UNIGENE","ENTREZID","CHR","GO","GENENAME"), keytype="SYMBOL")]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>gene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EMT（Epithelial-mesenchymal transition）]]></title>
    <url>%2Fhexo%2F2018%2F12%2F28%2F2018-12-29_EMT%2F</url>
    <content type="text"><![CDATA[http://www.sohu.com/a/155204532_777125 上皮细胞-间充质转化 Epithelial-mesenchymal transitionEMT中文综述文章_张秀红2012 上皮细胞在一些因素的作用下,失去极性及细胞间紧密连接和黏附连接, 获得了浸润性和游走迁移能力, 变成具备间质细胞形态和特性的细胞的改变。这种行为是可逆的。 ​ 生理状态下，上皮细胞-间质细胞转换在胚胎发育过程中具有十分重要的地位。在胚胎发育过程中出现最早的EMT转换是在原肠胚形成期时发生的间质细胞形成和中胚层形成事件。正常组织在受损的情况下，也会在损伤部位的上皮细胞发生EMT的转化，从而完成损伤修复过程。在EMT的发生过程中,上皮细胞极性丧失,与周围细胞和基质细胞的接触减少,细胞间的相互作用减少,细胞迁移和运动能力增强,同时细胞表型发生改变,丧失上皮表型,如角蛋白丝,E- cadherin, E-cadherin水平下降可以导致细胞的粘附力降低,使细胞获得易于侵袭和转移的特性, E- cadherin表达的丢失已经被认为是EMT最显著的特征。同时细胞获得间质表型,如Vimentin, N-cadherin等表达升高。 目前EMT被看成是导致肿瘤进展的病理过程。在肿瘤的恶性演进过程中，EMT使得肿瘤细胞得以侵袭和转移，还可能使肿瘤细胞逃逸某些因素诱导的凋亡。因此，在肿瘤的研究中，EMT的发生预示肿瘤恶性进程。 EMT发生常见的标志分子 1.表达减少：E- cadherin，Cytokeratin，ZO-1。 2.表达增多：N- cadherin，Vinmentin，Snail1，Snail2，Twi st，MMP-2，MMP-3，MMP-9。 3.活性增加：ILK，Rho。 参与EMT过程控制的常见信号通路 TGF-β信号通路。 Wnt信号通路。 Notch信号通路。 SMAD信号通路。 PI3K-AKT-MTOR信号通路。 EMT markers (Thiery et al, 2009) Mes： TWIST1, SNAI1, SNAI2, VIM,CDH2, ZEB1Epi ：and CDH1, DDR1, ERBB2, ERBB3, KRT19 Nature：肿瘤有7种EMT状态，从而找出肿瘤转移的罪魁祸首！http://www.medsci.cn/article/show_article.asp?id=698113604632 肿瘤异质性使同一个肿瘤中不同细胞的存在差别。这些不同点对疾病诊断、预后及治疗有着主要的影响。各种不同的机制已经被用于解释这种肿瘤异质性，例如上皮细胞-间充质细胞转化（EMT）——上皮肿瘤细胞失去黏附能力获得间充质细胞迁移性能力以促进转移和耐药性的过程。EMT程度不同的细胞就会呈现出不同的转移性质，但是这种可能性还没有被探索过。 Ievgenia Pastushenko及其同事使用了先进的会自发发生EMT的皮肤癌和乳腺癌小鼠模型。通过筛查数百个单克隆抗体识别细胞表面分子以及进行单细胞RNA测序，研究人员发现皮肤癌和乳腺癌组织中至少存在7中EMT状态不同的癌细胞亚群：从完全上皮化（分化）到完全的间充质化（未分化）状态，中间是各种杂化状态。作者发现不是所有的癌细胞功能都相同、转移能力都相同，而处于杂化EMT状态的癌细胞导致了肺癌转移。“我们发现处于EMT早期的肿瘤细胞（而非完全EMT的肿瘤细胞）是最可能转移的群体，这与我们的认知相悖，这也是最令人兴奋的地方。”这项研究还找到了控制不同肿瘤转化状态的基因调节网络和肿瘤微环境。 Ievgenia Pastushenko， et al.Identification of the tumour transition states occurring during EMT.Nature.18 April 2018 http://www.csi.nus.edu.sg/bioinfo/index.php In general, significant p-value and a positive score means a sample is more mesenchymal, whereas significant p-value and a negative score means a sample is more epithelial. P-value that does notreach significant level is considered as intermediate. Please do not hesitate to contact me if you have any queries regarding the score or the computation of the score.]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>EMT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[英语-词汇]]></title>
    <url>%2Fhexo%2F2018%2F12%2F28%2F2025-01-01-English_word%2F</url>
    <content type="text"><![CDATA[curated 整理morphology 形态学plethora 过多过剩]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[滕王阁序]]></title>
    <url>%2Fhexo%2F2018%2F12%2F27%2F2018-12-27_tengwanggexu%2F</url>
    <content type="text"><![CDATA[豫章故郡，洪都新府。星分翼轸，地接衡庐。襟三江而带五湖，控蛮荆而引瓯越。物华天宝，龙光射牛斗之墟；人杰地灵，徐孺下陈蕃之榻。雄州雾列，俊采星驰。台隍枕夷夏之交，宾主尽东南之美。都督阎公之雅望，棨戟遥临；宇文新州之懿范，襜帷暂驻。十旬休假，胜友如云；千里逢迎，高朋满座。腾蛟起凤，孟学士之词宗；紫电青霜，王将军之武库。家君作宰，路出名区；童子何知，躬逢胜饯。 时维九月，序属三秋。潦水尽而寒潭清，烟光凝而暮山紫。俨骖騑于上路，访风景于崇阿；临帝子之长洲，得天人之旧馆。层峦耸翠，上出重霄；飞阁流丹，下临无地。鹤汀凫渚，穷岛屿之萦回；桂殿兰宫，即冈峦之体势。 披绣闼，俯雕甍，山原旷其盈视，川泽纡其骇瞩。闾阎扑地，钟鸣鼎食之家；舸舰弥津，青雀黄龙之舳。云销雨霁，彩彻区明。落霞与孤鹜齐飞，秋水共长天一色。渔舟唱晚，响穷彭蠡之滨；雁阵惊寒，声断衡阳之浦。 遥襟甫畅，逸兴遄飞。爽籁发而清风生，纤歌凝而白云遏。睢园绿竹，气凌彭泽之樽；邺水朱华，光照临川之笔。四美具，二难并。穷睇眄于中天，极娱游于暇日。天高地迥，觉宇宙之无穷；兴尽悲来，识盈虚之有数。望长安于日下，目吴会于云间。地势极而南溟深，天柱高而北辰远。关山难越，谁悲失路之人？萍水相逢，尽是他乡之客。怀帝阍而不见，奉宣室以何年？ 嗟乎！时运不齐，命途多舛。冯唐易老，李广难封。屈贾谊于长沙，非无圣主；窜梁鸿于海曲，岂乏明时？所赖君子见机，达人知命。老当益壮，宁移白首之心？穷且益坚，不坠青云之志。酌贪泉而觉爽，处涸辙以犹欢。北海虽赊，扶摇可接；东隅已逝，桑榆非晚。孟尝高洁，空余报国之情；阮籍猖狂，岂效穷途之哭！ 勃，三尺微命，一介书生。无路请缨，等终军之弱冠；有怀投笔，慕宗悫之长风。舍簪笏于百龄，奉晨昏于万里。非谢家之宝树，接孟氏之芳邻。他日趋庭，叨陪鲤对；今兹捧袂，喜托龙门。杨意不逢，抚凌云而自惜；钟期既遇，奏流水以何惭？ 呜乎！胜地不常，盛筵难再；兰亭已矣，梓泽丘墟。临别赠言，幸承恩于伟饯；登高作赋，是所望于群公。敢竭鄙怀，恭疏短引；一言均赋，四韵俱成。请洒潘江，各倾陆海云尔： 滕王高阁临江渚，佩玉鸣鸾罢歌舞。 画栋朝飞南浦云，珠帘暮卷西山雨。 闲云潭影日悠悠，物换星移几度秋。 阁中帝子今何在？槛外长江空自流。]]></content>
      <categories>
        <category>闲情</category>
      </categories>
      <tags>
        <tag>古散文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo]]></title>
    <url>%2Fhexo%2F2018%2F12%2F27%2F2018-12-27_Hexoblog%2F</url>
    <content type="text"><![CDATA[去年发现Hugo blog之时，喜甚。以Hugo+R blogdown+Yihui hugo-ivy主题搭建了Blog，所有内容采用markdown编辑。久用之，存稍许缺憾，一是公式显示问题，二是缺少搜索功能。 今年学习动态贝叶斯网络，偶尔翻到李环博士Blog, 非常喜爱该blog框架，满足我对Blog所有目前需求（1）文章列表显示，（2）搜索，（3）其他。细细探究一番，了解为Hexo，遂邮件索问，得其指引，再细加研究一晚，基本探索完毕。 去年用hugo-ivy之时，觉得自己可能会坚持到forever, 但是现在想重新更换主题框架。觉世事难料，也非人性善变，处不同阶段层次而已。我想有一点不太会变，blog作为学习记录会在很长一段时间维持。 在Hexo中使用mathjax渲染数学公式(成功)https://blog.csdn.net/u013282174/article/details/80666123 更换Hexo的markdown渲染引擎 12$ npm uninstall hexo-renderer-marked --save$ npm -g install hexo-renderer-kramed --save 解决语义冲突 1234567#修改node_modules\kramed\lib\rules\inline.js中的第11行：// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,#第20行：// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 安装mathjax插件 12npm install hexo-math --savehexo install math 在文章的front-matter中添加 12title: Titlemathjax: true 在主题_config.yml中添加支持mathjax(如果有) 1mathjax: true 在blog _config.yml中添加支持mathjax 12345mathjax: enable: true per_page: true #cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML 在hexo中使用mathjax的方法(不成功)https://blog.csdn.net/littlehaes/article/details/84370393 之前总是使用mathtype写公式然后再上传图片, 速度太慢了, 而且公式出错后不容易修改, 所以开始用mathjax, 但是原生hexo并不能直接渲染mathjax. 可以先换成淘宝的下载源,增加npm下载速度: 1npm config set registry https://registry.npm.taobao.org 一. 使用Kramed 代替 Marked渲染引擎kramed支持mathjax npm uninstall hexo-renderer-marked —savenpm install hexo-renderer-kramed —save 打开/node_modules/hexo-renderer-kramed/lib/renderer.js // Change inline math rulefunction formatText(text) { // Fit kramed’s rule: + \1 + return text.replace(/\$(.*?)\$/g, ‘$$$$$1$$$$’);} 更改为： // Change inline math rulefunction formatText(text) { // Fit kramed’s rule: + \1 +​ return text;} 二:使用hexo-renderer-mathjax 代替 hexo-math npm uninstall hexo-math —savenpm install hexo-renderer-mathjax —save 三:更新 Mathjax 的 CDN 链接打开node_modules/hexo-renderer-mathjax/mathjax.html将最下面&lt;script src=后的url改为 https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML 四. 更改默认转义规则打开:博客根目录/node_modules/kramed/lib/rules/inline.js 1234//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, 第11行，将其修改为escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 第20行，将其修改为em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 五:开启mathjax在主题的配置文件中,我用的是next主题,那么在其_config.yml中找到mathjax并设置为true 12345# MathJax Supportmathjax: enable: true per_page: true cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML 写文章时,加上mathjax: true123456---date: 2017/8/3 18:20:00tags: hexomathjax: truetitle: hexo博客MathJax公式渲染--- hexo 命令1234567891011#安装hexonpm install hexo-cli gnpm install hexo --save#安装hexo servernpm install hexo-server --save# 安装 hexo-deployer-gitnpm install hexo-deployer-git --save# 生成并部署hexo g -d#清理hexo clean 123456789101112#实时浏览cd hexo_project/npm install hexo-browsersync --savehexo s[BS] Access URLs: -------------------------------------- UI: http://localhost:3001 -------------------------------------- UI External: http://192.168.191.1:3001 --------------------------------------INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>杂项</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯网络(Bayesian Network)]]></title>
    <url>%2Fhexo%2F2018%2F12%2F26%2F2018-12-26_bayesnetwork%2F</url>
    <content type="text"><![CDATA[引用1： https://blog.csdn.net/gdp12315_gu/article/details/50002195引用2： https://longaspire.github.io/blog/动态贝叶斯网络/ 贝叶斯网络（Bayesian Networks）贝叶斯网络（Bayesian Networks）也被称为信念网络（Belif Networks）或者因果网络（Causal Networks），是描述数据变量之间依赖关系的一种图形模式，是一种用来进行推理的模型。 贝叶斯网络结构贝叶斯网的网络结构是一个有向无环图（Directed Acyclic Graph），其中每个结点代表一个属性或者数据变量，结点间的弧代表属性（数据变量） 间的概率依赖关系。一条弧由一个属性（数据变量）A指向另外一个属性（数据变量）B说明属性A的取值可以对属性B的取值产生影响，由于是有向无环图，A、B间不会出现有向回路。在贝叶斯网当中，直接的原因结点（弧尾）A叫做其结果结点（弧头）B的双亲结点（parents），B叫做A的孩子结点（children）。如果从一个结点X有一条有向通路指向Y，则称结点X为结点Y的祖先（ancestor），同时称结点Y为结点X的后代（descendent）。 在贝叶斯网中没有输入的结点被称作根结点（root），其他结点被统称为非根结点。 贝叶斯网络当中的弧表达了结点间的依赖关系，如果两个结点间有弧连接说明两者之间有因果联系，反之如果两者之间没有直接的弧连接或者是间接的有向联通路径，则说明两者之间没有依赖关系，即是相互独立的。结点间的相互独立关系是贝叶斯网络当中很重要的一个属性，可以大大减少建网过程当中的计算量。使用贝叶斯网络结构可以使人清晰的得出属性结点间的关系，进而也使得使用贝叶斯网进行推理和预测变得相对容易实现。 条件概率表一个原因结点的出现会导致某个结果的产生时，都是一个概率的表述，而不是必然的，这样就需要为每个结点添加一个条件概率。一个节点在其双亲节点（直接的原因接点）的不同取值组合条件下取不同属性值的概率，就构成了该结点的条件概率表。 贝叶斯网络中的条件概率表是结点的条件概率的集合。当使用贝叶斯网络进行推理时，实际上是使用条件概率表当中的先验概率和已知的证据结点来计算所查询的目标结点的后验概率的过程。 根据条件概率和贝叶斯网络结构，我们不仅可以由祖先接点推出后代的结果，还可以通过后代当中的证据结点来向前推出祖先取各种状态的概率。 贝叶斯网可以处理不完整和带有噪声的数据集，因此被日益广泛的应用于各种推理程序当中。同时由于可以方便的结合已有的先验知识，将已有的经验与数据集的潜在知识相结合，可以弥补相互的片面性与缺点，因此越来越受到研究者的喜欢。 贝叶斯网络分类 静态贝叶斯网络： 反映了一系列变量间的概率依存关系，没有考虑时间因素对变量的影响。 动态贝叶斯网络： 沿时间轴变化的贝叶斯网络。 以下部分全文引用（https://longaspire.github.io/blog/动态贝叶斯网络/） 动态贝叶斯网络（Dynamic Bayesian Network, DBN）动态贝叶斯网络是一种暂态模型（transient state model），能够学习变量间的概率依存关系及其随时间变化的规律，其主要用于时序数据建模。动态并不是说网络结构随着时间的变化而发生变化，而是样本数据，或者说观测数据，随着时间的变化而变化。隐马尔可夫模型（hidden markov model, HMM）是一种结构最简单的动态贝叶斯网络。线性状态空间模型（linear state-space models）如卡尔曼滤波也可以等价看做是动态贝叶斯网络的一种形式。 一般 的 DBN有两个特点。网络 的 拓扑结构在每个时间片(time slice,快照) 内是相同的，而片与片之间通过类似的弧进行连接。 为方便处理，假设动态贝叶斯网络满足2个条件： 网络拓扑结构不随时间发生改变，即除去初始时刻，其余时刻的变量及其概率依存关系相同； 满足一阶马尔可夫条件，即给定当前时刻的状态后，未来时刻的状态和先前时刻的状态无关。 满足上述条件后，动态贝叶斯网络可以看作是贝叶斯网络在时间序列上的展开，如下图所示。 上图中，贝叶斯网络的过程可以看成：新的数据集$M$在初始数据集$C$的基础上获得，使用贝叶斯公式结合初始数据集$C$得到估计值$O$。而动态贝叶斯网络的过程中，每个时刻的变量$X_{t} = \{ C_{t}, M_{t}, O_{t} \}$的概率依存关系随时间$t$变化。在任意时刻，变量$M_{t}$的状态由变量$C_{t}$决定，而$O_{t}$的状态由$C_{t}$和$M_{t}$共同决定，即变量集$X_{t}$的联合概率分布为： $P(X_{t}) = P(C_{t}, M_{t}, O_{t}) = P(C_{t})P(M_{t} \mid C_{t})P(O_{t} \mid C_{t}, M_{t})$ 考虑$O_{t}$和$C_{t}$间的条件概率分布： $P(O_{t} \mid C_{t}) = \frac{P(C_{t}, M_{t}, O_{t})}{P(C_{t})}$$= \frac{\sum_{m} P(C_{t}, O_{t}, M_{t} = m)}{P(C_{t})}$$= \frac{\sum_{m} P(C_{t})P(M_{t} = m \mid C_{t}) P(O_{t} \mid C_{t}, M_{t} = m)}{P(C_{t})}$$= \sum_{m} P(M_{t} = m \mid C_{t}) P(O_{t} \mid C_{t}, M_{t} = m)$ 在时刻$t-1$和$t$之间，变量集$C_{t}$的状态发生了转移，因此，变量集$X_{t}$的转移概率为$P(X_{t} \mid X_{t-1}) = P(C_{t} \mid C_{t-1})$。注意，$M_{t}$和$O_{t}$都是由$C_{t}$决定的。 可以看出，动态贝叶斯网络通过网络拓扑结构反映变量间的概率依存关系及随时间变化的情况，其不但能够对变量所对应的不同特征之间的依存关系进行概率建模，而且对特征之间的时序关系也能很好的加以反映。因此，适合对既具有特征相关性又具有时序相关性的复杂特征进行建模。 2. DBN模型结构每个时间片对应一个静态网络，时间片间通过时间关系进行互联 正如上图所示的典型结构，DBN的结构上具有一些显著的特点[4]： 每个时间片对应的静态模型是一定的，可以看做多个随机变量（状态）交互影响的结构 每个时刻的某一个状态可能依赖于上一个时刻的某几个状态和/或当前时刻的某几个状态 我们可以通过T个时刻的隐状态变量$X = \{ x_1, \ldots, x_T \}$和观测变量$Y = \{ y_1, \ldots, y_T \}$的概率分布函数来描述其对应的DBN，如下： $P(X, Y) = P(x_1)\prod\limits_{t=2}^{T}P(x_t \mid x_{t-1})\prod\limits_{t=1}^{T}P(y_t \mid x_t)$ 为了完整地对一个特定的DBN进行描述，我们需要确定以下参数： 状态转移的概率密度函数$P(X_t \mid X_{t-1})$，用于表述状态在时间上的依赖性 观测的概率密度函数$P(Y_t \mid X_t)$，用来描述某一个时间片内部，观测数据对于其他（未观测）结点的依赖性 初始状态的概率密度函数$P(X_1)$，用来描述过程开始之初的状态分布情况 以上的三个要素，在隐马尔科夫模型中可以一一完成对应，而动态贝叶斯网络则是采用了一种更为泛化，具有更通用数据和过程表达能力的模型。 对于上述前两个参数，需要在某个时间片上进行确定，我们通常简单地假定这些概率密度函数是不随时间改变（time-invariant）的。 根据随机变量的状态空间设定，DBN既可以是连续的、离散的或者二者皆有的。 3. DBN的任务及其问题解决DBN主要解决的问题可以列举如下： 推断（inference）：在给定初始分布和一些已知观测的情况下，对未知变量的分布进行求解计算； 解码（decoding）：在模型确定的情况下，根据已知观测结果对最佳（best-fitting probability value）的隐状态进行查找； 学习（learning）：给定一组观测序列，给结构已知模型的参数进行调整，以最好地支持观测到的数据； 剪枝（pruning）：找出当前DSN结构中哪些结点在语义层面上是重要的（semantically important），并将不重要的去除。 3.1 推断推断过程可以看做给定一组有限的$T$个连续的观测变量$Y_{0}^{T-1} = \{ y_0, \ldots, y_{T-1} \}$的情况下，对于连续隐变量序列$X_{0}^{T-1} = \{ x_0, \ldots, x_{T-1} \}$的条件概率分布$P(X_{0}^{T-1} \mid Y_{0}^{T-1})$进行计算。一个具体的例子如下图所示。 已知每个时刻的观测$y_i$，对对应的隐变量$x_i$的取值进行推断 有时候，由于计算$P(X_{0}^{T-1} \mid Y_{0}^{T-1})$过于复杂，可以考虑不对$X_{0}^{T-1}$的每一个排列（constellation）进行条件概率的求解，而转而对概率密度函数的充分统计量（sufficient statistics）进行估计。因此，可以静静选择某一个或几个状态，并在不同时刻对其取值进行估计，即$P(x_{t} \mid Y_{0}^{T-1})$。 推断过程可以通过前向传播（forward propagation）和后向传播（backward propagation）完成。 3.1.1 前向传播$t$时刻的前向概率分布（forward probability distribution）为： $\alpha_t(x_t) = P(Y_{0}^{t}, x_t)$ 根据网络结构的依赖关系，有： $\alpha_{t+1}(x_{t+1}) = P(y_{t+1} \mid x_{t+1}) \sum\limits_{x_{t}}P(x_{t+1} \mid x_t)\alpha_t(x_t)$同时，有：$\alpha_{0}(x_{0}) = P(x_{0})$ 3.1.2 后向传播$t$时刻的后向概率分布（backward probability distribution）为： $\beta_t(x_t) = P(Y_{t}^{T-1} \mid x_t)$ 根据网络结构的依赖关系，有： $\beta_{t-1}(x_{t-1}) = \sum\limits_{x_{t}}P(x_{t} \mid x_{t-1})\beta_t(x_t)$P(y_t \mid x_t)而且，有：$\beta_{T-1}(x_{T-1}) = 1$ 3.1.3 平滑根据当前的观测值，还可以对某一个时刻变量的取值进行推断计算，称之为平滑。平滑操作符（smoothing operator）可以定义如下： $\gamma_{t}(x_t) = P(x_t \mid Y_{0}^{T-1}) = \frac{\alpha_t(x_t)\beta_t(x_t)}{\sum_{x_t}\alpha_t(x_t)\beta_t(x_t)}$ 更高阶的平滑方程也可以在前向和后向概率分布的基础上定义。例如，定义一个一阶的平滑： $\xi_{t, t-1}(x_t, x_{t-1}) = P(x_t, x_{t-1} \mid Y_{0}^{T-1}) = \frac{\alpha_{t-1}(x_{t-1}) P(x_t \mid x_{t-1}) P(y_t \mid x_t) \beta_t(x_t)}{\sum_{x_t}\alpha_t(x_t)\beta_t(x_t)}$ 3.1.4 预测可形式化地描述为求解$P(y_{t+1} \mid Y_{0}^{t})$或者$P(x_{t+1} \mid Y_{0}^{t})$。 $P(x_{t+1} \mid Y_{0}^{t}) = P(x_{t+1}, Y_{0}^{t}) / P(Y_{0}^{t})$$= \sum_{x_t} P(x_{t+1} \mid x_t) \alpha_t(x_t) / \sum_{x_t} \alpha_t(x_t)$ 同时，也可以得到： $P(y_{t+1} \mid Y_{0}^{t}) = P(y_{t+1}, Y_{0}^{t}) / P(Y_{0}^{t})$$= \sum_{x_{t+1}} P(y_{t+1} \mid x_{t+1}) \sum\limits_{x_{t}}P(x_{t+1} \mid x_t)\alpha_t(x_t) / \sum_{x_t} \alpha_t(x_t)$$= \sum_{x_{t+1}} \alpha_{t+1}(x_{t+1}) / \sum_{x_t} \alpha_t(x_t)$ 预测问题可以表示为一个求最大似然（maximum likelihood）的问题： $x^{\star}_{t+1, t} = \arg\max_{x_{t+1}} P(x_{t+1} \mid Y_{0}^{t})$ $y^{\star}_{t+1, t} = \arg\max_{y_{t+1}} P(y_{t+1} \mid Y_{0}^{t})$ 3.2 解码解码问题可以表述如下： $\hat{X}_{0}^{T-1} = \arg\max\limits_{X_{0}^{T-1}} P(X_{0}^{T-1} \mid Y_{0}^{T-1})$ 该问题的求解可以使用经典的动态规划算法——维特比（Viterbi）算法进行求解。 首先考虑以下简单的形式： $\delta_{t+1}(x_{t+1}) = \max\limits_{X_{0}^{t}} P(X_{0}^{t+1} \mid Y_{0}^{t+1})$ 可以对其进行时序上的递推： $\delta_{t+1}(x_{t+1}) = P(y_{t+1} \mid x_{t+1}) \max\limits_{x_t}[P(x_{t+1}, x_t) \max\limits_{X_0^{T-1}} P(X_{0}^{t-1} \mid Y_{0}^{t})]$$= P(y_{t+1} \mid x_{t+1}) \max\limits_{x_t}[P(x_{t+1}, x_t) \delta_{t}(x_{t})]$ 则我们可以把以上简单形式嵌入到下式表达中： $\max\limits_{X_{0}^{T-1}} P(X_{0}^{T-1} \mid Y_{0}^{T-1}) = \max\limits_{x_{T-1}}\delta_{T-1}(x_{T-1})$ 其现实意义在于，为了找到$\hat{X}_{0}^{T-1}$，每一步都求解最大可能概率的隐变量$x_t$，其能够最大化$\delta_{t+1}(x_{t+1})$。 假定给定一个式子： $\psi_{t+1}(x_{t+1}) = \arg\max\limits_{x_t}[P(x_{t+1} \mid x_t) + \delta_{t}(x_t)]$ 则优化的目标为： $\hat{x}_t = \psi_{t+1}(\hat{x}_{t+1})$ 3.3 学习当DBN网络结构确定时，某些结点间的条件概率依赖无法确切计算，这个时候，就需要考虑对模型参数进行学习调整。EM算法或者GEM（general EM）算法可用来对DBN参数进行学习。 $\log P(X_{0}^{T-1}, Y_{0}^{T-1} \mid \theta) = \log [P(x_0) \prod_{1}^{T-1}P(x_i \mid x_{i-1}) \prod_{0}^{T-1}P(y_i \mid x_i)]$$= \log P(x_0) + \sum_{1}^{T-1}\log P(x_i \mid x_{i-1}) + \sum_{0}^{T-1} \log P(y_i \mid x_i)]$ 对上式进行梯度下降，优化的目标为参数向量$\theta$： $\frac{\partial \log P(x_0)}{\partial \theta} + \sum_{1}^{T-1} \frac{\partial \log P(x_i \mid x_{i-1})}{\partial \theta} + \sum_{0}^{T-1} \frac{\partial \log P(y_i \mid x_i)}{\partial \theta} = 0$ 3.4 剪枝对DBN进行剪枝的过程是非常复杂的。剪枝通常包括以下步骤： 删除某个特定结点的某一个状态 去除两个结点间的关联 去除一个结点 例如，对于$t$时刻的某个结点$V_{i}^{(t)}$，如果已经知道其概率$P(V_{i}^{(t)} = s_i) = 1$，即一定等于某个状态$s_i$，则其他的状态可以去除。同时，如果一个结点不存在前继结点同时其对某个状态的概率为0的情况下，该状态也可以被删除。 剪枝的决定是在推断执行的时间节省和做出错误的可能性间寻找平衡。 4. 构建DBN 结构 / 数据观测 方法 已知 / 完整（complete） simple statistics 已知 / 部分（incomplete）- 存在隐变量 EM or gradient ascent 未知 / 完整 search through model space 未知 / 部分 structural EM 4.1 已知结构和完整观测已知结构$G$的情况下，对于参数的确定可以认为是模型参数在观测数据下的最大似然估计。 给定$S$个独立的数据序列，数据$\boldsymbol{D}$表示为$\{ D_1, \ldots, D_S \}$。 我们首先根据链式法则，将所有结点的联合概率分布写为： $P(X_1, \ldots, X_{m}) = \prod_i P(X_i \mid X_1, \ldots, X_{i-1}) = \prod_i P(X_i \mid Parent(X_i))$ 正规化对数似然（normalized log likelihood），也即平均对数似然，可以写为： $LL = 1/N \cdot \log P(\boldsymbol{D} \mid G)$ $G$也即整个模型参数，$N$是采样的数量。 可以进一步写为： $LL = 1/N \cdot \sum_{i=1}^{m} \sum_{l=1}^{S} \log P(X_i \mid Parent(X_i), D_l)$ 按照上述公式，我们可以对每个DBN中每个结点对对数似然的贡献进行单独的计算。 4.2 已知结构和部分观测当结构已知，但是数据是部分观测的情况下，我们需要对模型中的部分未知变量进行估计，这个时候需要借助于一些迭代方法，例如EM或者梯度下降，来找到MLE或者MAP的局部最优解。其要点可参照3.1推断中的内容。 4.3 未知结构和完整观测当结构未知但是观测数据完整的情况下，可以考虑利用观测数据对模型结构进行学习。对于结构的学习，一般是通过初始化一个结构，并通过以下方式的修改，得到一个新的结构，并对其进行评估： 新增一条边； 改变图中的某一条边； 删除图的一条边 以上操作可以保障修改的静态图结构始终为一个DAG。 为了完成对模型结构的学习，我们按照如下方式对任务进行定义： 找到一个能够比较不同结构的度量； 找到查找不同结构的搜索算法； 搜索算法可以分为启发式算法和基于条件依赖（CI, conditional independence）的算法—即考量不同结点间的依赖关系强弱。启发式算法虽然在执行上较为快速，但其很难收敛到最优解。相比之下，通过CI测试的方法一般可以获得最优或者近似最优解。 4.3.1 基于条件依赖测试的方法在信息论汇总，两个结点$X_i$和$X_j$的互信息（mutual information）可以定义为： $I(X_i, X_j) = \sum_{x_i, x_j} P(x_i, x_j) \log \frac{P(x_i, x_j)}{P(x_i)P(x_j)}$ 条件互信息（conditional mutual information）则可以定义为： $I(X_i, X_j \mid Y) = \sum_{x_i, x_j, y} P(x_i, x_j, y) \log \frac{P(x_i, x_j \mid y)}{P(x_i \mid y)P(x_j \mid y)}$ 在基于CI测试的方法中，$Y$可以看做一组有依赖关系的结点集合，而当两个结点$X_i$和$X_j$的条件互信息$I(X_i, X_j \mid Y)$小于某个阈值的时候，我们可以用条件集（conditional set）$Y$来对上述两个结点进行d-分割，即两个结点在给定$Y$的情况下条件独立。 4.3.2 启发式搜索方法给定训练集$D$，启发式搜索需要找到一个模型结构$B = (G, \Theta)$来最好地拟合数据$D$。其中，$G$对应于网络中所有的随机变量$X (X_1, \ldots, X_N)$的DAG；而$\Theta$表示的是模型的参数。此时需要引入得分函数，并通过最大化评分函数来找到最优的网络结构和模型参数。参照静态贝叶斯网络。 总体而言，我们可将问题形式化为： $\arg\max\limits_{G}P(G \mid D) = \arg\max\limits_{G}\frac{P(D \mid G)P(G)}{P(D)}$ 两边取对数，可以写为： $\arg\min\limits_{G} \log P(G \mid D) = \arg\min\limits_{G} \log P(D \mid G) + \log P(G) + c$ 其中，在给定数据的情况下，可以将$\log P(D)$视为上式中的常数$c$。直接使用精确的贝叶斯推断方法通常很难解，这是由于求边缘分布$P(D) = \sum_{G} P(D,G)$通常会引入指数级的计算量。这种情况下，一般可以对后验概率$P(D \mid B)$进行近似，同时加上一定的惩罚。这个惩罚是针对模型的复杂程度进行的，特别是考虑到通常最大似然的网络结构往往是全连接的。 典型的Bayesian Information Criterion（BIC）如下： $\log P(G \mid D) \approx \log P(D \mid G, \hat{\Theta}_{G}) - \frac{\log N}{2} len(G)$ 上式中，$len(G)$表示模型的维度，$N$是所有采样值的个数，而$\hat{\Theta}_{G}$是模型$G$的最大似然的参数。 总而言之，也是最为重要的，对于DBN的结构学习方法基本与静态贝叶斯网络的结构学习方法相同。 4.4 未知结构和部分观测很多情况下，未知结构和部分观测都是真实世界系统中出现的情况。在此情况下，如果还考虑到DBN中时间维度上的变化，则情况变得更加复杂。考虑到部分观测的情况下时，一般很难保证随机过程的马尔科夫性。 一般情况下，可以考虑使用EM算法进行求解： E步：使用当前评估的参数补充完整观测数据； M步：在认为补充值为真实观测值的情况下，重新对最大似然的参数进行计算； 每一步EM都确保数据的似然提升，直到到达一个局部最优解。 EM算法在部分观测的情况下，也需要进行一定的调整，如结构化的EM（structural EM, SEM）算法。 E步：同EM算法，根据当前的结构和参数，计算变量的期望值来补充完整数据； M步：分为以下两个部分 同EM算法，重新对最大似然的参数进行估计； 根据当前的结构，利用期望值来评估其他的候选结构。候选结构的获取是通过完全搜索与当前结构类似的结构来获得的。 5. DBN和HMM的对比假定目前有$D$个对象，需要在一组图像序列（$t$个时刻）中进行位置状态追踪。 HMM的问题处理： 假定每个对象每个时刻有$K$个可能状态 那么每个时刻的状态$X_{t} = (X_{t}^{(1)}, \ldots, X_{t}^{(D)})$具有$K^{D}$个可能取值 因此，完成推断需要的时间复杂度约为$O(T(K^D)^2)$，而空间复杂度约为$O(TK^D)$ $P(X_t \mid X_{t-1})$需要对$(K^D)^2$个参数进行判定 DBN的问题处理： HMM对于状态空间的描述使用一个单一的随机变量 $X_{t} = \{ 1, \ldots, K \}$ DBN对于状态空间的描述使用一组随机变量 $X_{t}^{(1)}, \ldots, X_{t}^{(D)}$ (分解、离散的形式表达) DBN对于$P(X_t \mid X_{t-1})$的表达采用更为紧凑的参数化图模型（parameterized graph） DBN相比于HMM，在参数个数上指数级别减少，同时其参数估计速度也是指数级地加快了 时间复杂度约为$O(TDK^{D+1})$ $P(X_t \mid X_{t-1})$需要对$DK^2$个参数进行判定 引用 本文主要内容学习整理自引用[1]。 1. 引言在上一篇中，我们对动态贝叶斯网络的基本形式、概念和模型问题进行了一定的解释。其中，大部分内容都还停留在对单独的子模型（sub-model）的问题求解（例如，两大主要问题：推断和学习）。在这篇文章中，我们将从时序模型的角度出发，考虑在时间上结点具有关联的动态贝叶斯网络的主要求解问题，也就是推断（inference）和参数学习（parameter learning）。 由于在大部分的时态系统中，对于模型结构是大体可知的，那么本文主要考虑的是在结构已知的情况下，进行概率推断和学习参数的问题。 相比于静态贝叶斯网络，动态贝叶斯网络的挑战在于，每个time slice的子模型间存在结点的相互关系，那么在上述两个问题中必须要考虑到这些时间连接关系带来的影响。 在概率推断中，我们必须要对网络模型进行展开（unrolling or rolling up），同时要保证各结点的依赖关系，以及隐结点对观测结点的影响。 在参数学习中，主要难点在于——部分隐结点的概率分布情况的正确程度（correctness）是很难获知的。 以下分别对这两个问题的具体求解进行介绍。 2. 概率推断—网络展开问题引用[2]将DBN中的结点分为三个类别： dynamic nodes（DN）：随时间进行演变的对象 static nodes（SN）：不随时间进行演变的对象 temporary nodes（TN）：不同时刻接收不同的取值的对象，也称为evidence nodes 在DBN的网络展开问题中，我们希望每个时刻的子模型结构是一定的，那么在分析时刻$t$的模型时，我们希望利用推断来计算$t-1$时刻的结点带来的影响（influence），并将其删除。这个过程可以理解为网络展开。通过网络展开操作，时刻$t$的dynamic nodes可以转变为root nodes。 实际上，网络展开的结果，其实是将上一个时刻的知识引入到当前时刻的模型中。其主要依赖的是马尔科夫特性，也即将之前观测到的结果累积到当前时刻，不断迭代下去。 网络展开的问题在于，其结点消除（node elimination）的方法，可能会引入额外的结点链接关系，并进一步复杂化网络的结构，因此，一些方法被用于更为高效的网络展开： 连接先验和转移网络（connecting prior and transition networks） 时态不变网络（temporally invariant networks） 随机模拟（stochastic simulation） 结点关系的准确表达（extrac representation of node dependencies） 2.1 连接先验和转移网络在这种类型的DBN表达中，可以将网络分为两个部分： 初始时刻的贝叶斯网络模型，即先验网络（prior network）； 连续两个时刻间结点的依赖关系，即转移网络（transition network），它是两个贝叶斯网络的结合。 那么，网络展开的问题可以通过融合上述两个网络部分来进行求解。 利用先验网络和转移网络表示网络 在上图中，$t=0$时刻的随机变量的先验概率和条件概率在左图中进行描述；$t=1, \ldots, T-1$各个时刻的条件依赖关系则由右图给出。 在先验和转移网络中，若要进行概率推断，可以分别对两部分进行求解。注意，其前提假设在于一阶马尔科夫特性，即当前时刻的情况仅仅与上一时刻的情况相关。 2.2 时态不变网络在展开操作后网络结构保持不变的网络可以被认为是时态不变网络。其前提在于，网络中只有一个temporal node接收来自于上个时刻的必要信息，也就是说网络中只有这个结点受到上个时刻的影响，而在展开中不涉及到其他结点间的关系。 时态不变网络，只有结点$X$受到上个时刻结点的影响 相比于时态不变网络，下图中给出了一个时态变化网络（temporally variant network）。 时态变化网络 在上图中，我们发现，变量$X$和$Z$都受到上一个时刻结点的影响。例如，$X_2$和$Z_2$都依赖于$X_1$，也即在它们二者之间，是非独立的。因此，在转换后，我们需要给$X$和$Z$之间加入一条边。 一个子模型中各结点完全连接的网络一定是一个时态不变网络，当然其计算非常复杂。通常这种情况下，需要将其转化为一个时态变化模型，并通过更为有效的结点消除算法来进行推断和学习。 2.3 随机模拟随机模拟过程是通过一系列采样值来逼近网络中状态结点的置信（brief）。 典型的方法是似然权重（likelihood weighting）法。它通过有限次数的试验，每次试验的权重根据观测到的证据下的似然进行权重计算。通过对特别结点上加权平均值的计算，可以获得这些结点上的概率分布信息。 2.4 结点关系的准确表达在DBN中，有三种结点类型间的关系是允许的： between dynamic nodes (DN) and temporary nodes (TN) between dynamic and dynamic nodes between static nodes (SN) and dynamic nodes (DN) 以下分别进行描述。 2.4.1 DN和TN间依赖TN也称为evidence nodes。 DN和TN间依赖，TN只受到DN的独立影响 2.4.2 DN间依赖假如上图中的TN也会随着时间进行动态变化，则可以表达为下图的形式。 DN间依赖，DN2受到DN1在不同时刻的影响 2.4.3 SN和DN间依赖一个DN可以有父SN，如下图(a)所示。 在图(b)中，DN被认为是在每个时刻创建的TN。同时，DWN被认为是不受SN影响的DN。底下的TN受到SN和DWN的影响。 图(c)中，SN也可以被看做DN，以此来简化该模型，但是可能会导致更为不准确的推断结果。 SN和TN间依赖 3. 参数学习问题在DBN的结构已知时，模型的参数也并不能完全确定，即便通过对专家知识进行获取的情况下。此时，需要根据数据的观测来对模型参数进行调节，学习到合理的参数。通常情况下，可以认为这是一个最大似然估计的问题——找到最能拟合观测数据的模型参数。 特别需要注意的是，我们并不是总能在所有的time slice观测到数据。如果模型还存在隐状态，则需要借助EM算法。 我们将$t$时刻的未观测变量写为$u_t$、观测变量写为$o_t$，那么整个DBN的所有变量的联合概率分布可以写为： $P(o_1, \ldots, o_T, u_1, \ldots, u_T) = P(u_1) P(o_1 \mid u_1) \prod_{t=2}^{T} P(u_{t} \mid u_{t-1}) P(o_t \mid u_t)$ 其对应的EM算法可以表示如下： 这个算法可以表示为 推断隐状态和最大化模型参数的迭代过程。 在引用[3]中，作者为了简化学习过程，提出了在第一阶段忽略时间依赖的影响，单独对每个固定观测网络的转移概率进行计算。虽然得到的结果是suboptimal的，但是在计算效率上得到了很大提升。 引用1.动态贝叶斯网络简述. ↩1.Dynamic Bayesian Networks: A State of the Art. ↩2.Dynamic Bayesian Network, Wikipedia. ↩2.R. Schafer and T. Weyrath. Assessing Temporally Variable User Properties with Dynamic Bayesian Networks. ↩3.A Tutorial on Dynamic Bayesian Networks. ↩3.Audio-Visual Speaker Detection using Dynamic Bayesian Networks. ↩4.Dynamic Bayesian Networks: A State of the Art. ↩]]></content>
      <categories>
        <category>因果</category>
      </categories>
      <tags>
        <tag>因果</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UpSetR包]]></title>
    <url>%2Fhexo%2F2018%2F12%2F24%2F2018-12-24_UpsetR%2F</url>
    <content type="text"><![CDATA[https://www.jianshu.com/p/324aae3d5ea4 1install.packages(UpSetR) 输入方式一： table1234567891011121314151617181920212223require(ggplot2); require(plyr); require(gridExtra); require(grid);movies &lt;- read.csv(system.file("extdata","movies.csv",package = "UpSetR"), header = TRUE, sep=";")View(movies)upset(movies, nsets = 7, nintersects = 30, mb.ratio = c(0.5, 0.5), order.by = c("freq", "degree"), decreasing = c(TRUE,FALSE))#nsets: 最多展示多少个集合数据。毕竟原来有20多种电影类型，放不完的#nintersects: 展示多少交集。#mb.ratio： 点点图和条形图的比例。#order.by： 交集如何排序。这里先根据freq，然后根据degree#decreasing： 变量如何排序。这里表示freq降序，degree升序# 用于query的函数between &lt;- function(row, min, max)&#123; newData &lt;- (row["ReleaseDate"] &lt; max) &amp; (row["ReleaseDate"] &gt; min)&#125;upset(movies, sets=c("Drama","Comedy","Action","Thriller","Western","Documentary"), queries = list(list(query = intersects, params = list("Drama", "Thriller")), list(query = between, params=list(1970,1980), color="red", active=TRUE)))#queries接受query所组成的list。然后不同query也是一个list，这个list由查询函数，和参数组成，参数也是一个list。查询函数可以用系统自带的，也可以自己写一个。比如说这里的betweenupset(movies,attribute.plots=list(gridrows=60,plots=list(list(plot=scatter_plot, x="ReleaseDate", y="AvgRating"), list(plot=scatter_plot, x="ReleaseDate", y="Watches"),list(plot=scatter_plot, x="Watches", y="AvgRating"), list(plot=histogram, x="ReleaseDate")), ncols = 2)) 输入方式二：集合交集向量1234567891011121314151617input &lt;- c( "MAQ"=144600, "FaSD"=16532, "Bcftools"=283, "GATK"=15160, "MAQ&amp;FaSD"=16323, "MAQ&amp;Bcftools"=636, "Bcftools&amp;GATK"=65435, "FaSD&amp;GATK"=33874, "MAQ&amp;FaSD&amp;Bcftools"=114, "MAQ&amp;FaSD&amp;GATK"=41858, "MAQ&amp;Bcftools&amp;GATK"=4, "FaSD&amp;Bcftools&amp;GATK"=6603, "MAQ&amp;FaSD&amp;Bcftools&amp;GATK"=8357)data &lt;- fromExperssion(input)upset(data) upsetplot()123456require(TxDb.Hsapiens.UCSC.hg19.knownGene)txdb &lt;- TxDb.Hsapiens.UCSC.hg19.knownGenepeakfile &lt;- system.file("extdata", "sample_peaks.txt", package="ChIPseeker")peakAnno &lt;- annotatePeak(peakfile, tssRegion=c(-3000, 3000), TxDb=txdb)peakAnnoupsetplot(peakAnno, vennpie=TRUE)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Win10 Linux子系统]]></title>
    <url>%2Fhexo%2F2018%2F12%2F24%2F2018-12-24_Win10_Linuxsystem%2F</url>
    <content type="text"><![CDATA[1. 开启win10内置linux子系统 选择开发人员模式 ​ 设置—针对开发人员—开发人员模式 开启windows功能中的Linux子系统功能。 ​ 控制面板&gt;启用或关闭windows功能-&gt;适用于Linux的windows子系统 重启计算机 CMD-输入bash 2. 启动LinuxCMD-输入bash即可 3. Linux子系统升级1234567##查看当前 Windows 10内置的Ubuntu系统版本，bash中输入lsb_release -a## CMD中 移除Ubuntu 14.04版lxrun /uninstall /full /y## CMD中 安装最新Ubuntu lxrun /install /y cmder的使用由于Windows自带的命令提示符cmd并不是很好用，界面也不美观，这里使用效果更好的 cmder，下载mini版的解压即可使用。 进入在cmder的标题栏或状态栏 右键 或者点击右下角menu，如下图，选择”Settings”进行环境设置：选择Command line，输入”bash -cur_console:p”，另外通过ssh连接服务器的时候，会出现中文字体相互重叠的情况，这时去设置Main总取消选择 Compress long strings to fit space。]]></content>
  </entry>
  <entry>
    <title><![CDATA[dplyr 数据框整理神器+ tidyr数据变形]]></title>
    <url>%2Fhexo%2F2018%2F12%2F23%2F2018-12-23_dplyr_tidyr%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/pilouduo1367/article/details/69675678 1 dplyr包—数据框整理神器1.1 展现数据 tbl_df()：函数tbl_df()使得大数据集规范显示出来，行列都只显示10个，跟head差不多，但是head只能控制行，无法控制列，使用tbl_df()就是为了防止数据刷屏。 1.2 筛选数据 filter()：按给定的逻辑判断筛选出符合要求的子数据集 1234567mtcars_df &lt;- tbl_df(mtcars)filter(mtcars_df, mpg == 21, cyl == 6)# A tibble: 2 × 11# mpg cyl disp hp drat wt qsec vs am gear carb# &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;#1 21 6 160 110 3.9 2.620 16.46 0 1 4 4#2 21 6 160 110 3.9 2.875 17.02 0 1 4 4 1.3 排列数据 arrange()：12#先对cyl列正序排列，再对carb列倒序排列xx= arrange(mtcars_df, cyl, desc(carb)) 1.4 选择子集 select()用列名作参数来选择子数据集 1select(mtcars_df, mpg, disp:wt, carb) # disp:wt表示从disp列选到wt列 1.5 变形 mutate()：对已有列进行数据运算并添加为新列 1mutate(mtcars_df, NO = 1:dim(mtcars_df)[1], res = hp - drat) 1.6 总览数据 summarise()：对数据框调用其它函数进行汇总操作, 返回一维的结果 12345summarise(mtcars_df, mdisp = mean(disp, na.rm = TRUE), mhp = mean(hp, na.rm = TRUE))# A tibble: 1 × 2 mdisp mhp &lt;dbl&gt; &lt;dbl&gt;1 230.7219 146.6875 1.7 数据分组 group_by()：当对数据集通过group_by()添加了分组信息后，mutate()，arrange() 和 summarise() 函数会自动对这些 tbl 类数据执行分组操作。 12345678cars &lt;- group_by(mtcars_df, cyl)(countcars &lt;- summarise(cars, count = n())) # count = n()用来计算次数# A tibble: 3 × 2 cyl count &lt;dbl&gt; &lt;int&gt;1 4 112 6 73 8 14 2 tidyr包—数据变形2.1 宽转长 gather()：使用gather()函数实现宽表转长表，语法如下： 12345678910# gather(data, key, value, …, na.rm = FALSE, convert = FALSE)# data：需要被转换的宽形表# key：将原数据框中的所有列赋给一个新变量key# value：将原数据框中的所有值赋给一个新变量value# …：可以指定哪些列聚到同一列中# na.rm：是否删除缺失值longdata &lt;- gather(mtcars_df, attribute, value, -mpg)longdata &lt;- gather(mtcars_df, attribute, value)longdata &lt;- gather(mtcars_df, attribute, value, mpg, carb) 2.2 长转宽 spread()：有时，为了满足建模或绘图的要求，往往需要将长形表转换为宽形表，或将宽形表变为长形表。如何实现这两种数据表类型的转换。使用spread()函数实现长表转宽表，语法如下： 1234567# spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE)# data：为需要转换的长形表# key：需要将变量值拓展为字段的变量# value：需要分散的值# fill：对于缺失值，可将fill的值赋值给被转型后的缺失值spread(longdata, attribute, value) 2.3 合并 unit()：123456789# unite(data, col, …, sep = “_”, remove = TRUE)# data：为数据框# col：被组合的新列名称# …：指定哪些列需要被组合# sep：组合列之间的连接符，默认为下划线# remove：是否删除被组合的列TIMES &lt;- data.frame(years = c('1990', '1991', '1992'), months = c(2,3,4), day = c(1, 2, 3))TIMESunite &lt;- unite(TIMES, information, months, day, sep= "-") 2.4 拆分 separate()：separate()函数可将一列拆分为多列，一般可用于日志数据或日期时间型数据的拆分. 12345678# separate(data, col, into, sep = “[^[:alnum:]]+”, remove = TRUE,# convert = FALSE, extra = “warn”, fill = “warn”, …)# data：为数据框# col：需要被拆分的列# into：新建的列名，为字符串向量# sep：被拆分列的分隔符# remove：是否删除被分割的列TIMESsep &lt;- separate(TIMESunite, information, c("months", "day"), sep = "-")]]></content>
  </entry>
  <entry>
    <title><![CDATA[WGCNA加权基因共表达网络分析]]></title>
    <url>%2Fhexo%2F2018%2F12%2F23%2F2018-12-23_WGCNA%2F</url>
    <content type="text"><![CDATA[原文 WGCNA分析，简单全面 学习WGCNA总结 WGCNA Background and glossary 程序跑了一遍，很多问题依然不清楚，需要继续深入研究。 GEO数据挖掘系列文-第一期-胶质母细胞瘤 GEO数据挖掘系列文-第二期-三阴性乳腺癌 GEO数据挖掘系列文-第三期-口腔鳞状细胞癌** GEO数据挖掘系列文-第四期-肝细胞癌 （WGCNA） GEO数据挖掘系列文-第五期-肝细胞癌 （多组差异分析） GEO数据挖掘-第六期-RNA-seq数据也照挖不误 外传：保姆式GEO数据挖掘演示—重现9分文章 基本概念加权基因共表达网络分析 (WGCNA, Weighted correlation network analysis)是用来描述不同样品之间基因关联模式的系统生物学方法，可以用来鉴定高度协同变化的基因集, 并根据基因集的内连性和基因集与表型之间的关联鉴定候补生物标记基因或治疗靶点。 相比于只关注差异表达的基因，WGCNA利用数千或近万个变化最大的基因或全部基因的信息识别感兴趣的基因集，并与表型进行显著性关联分析。一是充分利用了信息，二是把数千个基因与表型的关联转换为数个基因集与表型的关联，免去了多重假设检验校正的问题。 理解WGCNA，需要先理解下面几个术语和它们在WGCNA中的定义。 共表达网络：定义为加权基因网络。点代表基因，边代表基因表达相关性。加权是指对相关性值进行幂次运算 (幂次的值也就是软阈值 (power, pickSoftThreshold这个函数所做的就是确定合适的power))。 无向网络的边属性计算方式为 abs(cor(genex, geney)) ^ power； 有向网络的边属性计算方式为 (1+cor(genex, geney)/2) ^ power; sign hybrid的边属性计算方式为cor(genex, geney)^power if cor&gt;0 else 0。 这种处理方式强化了强相关，弱化了弱相关或负相关，使得相关性数值更符合无标度网络特征，更具有生物意义。如果没有合适的power，一般是由于部分样品与其它样品因为某种原因差别太大导致的，可根据具体问题移除部分样品或查看后面的经验值。 Module(模块)：高度內连的基因集。 在无向网络中，模块内是高度相关的基因。 在有向网络中，模块内是高度正相关的基因。 把基因聚类成模块后，可以对每个模块进行三个层次的分析： 1. 功能富集分析查看其功能特征是否与研究目的相符；基因富集相关文章 去东方，最好用的在线GO富集分析工具；GO、GSEA富集分析一网打进；GSEA富集分析-界面操作。 2. 模块与性状进行关联分析，找出与关注性状相关度最高的模块； 3. 模块与样本进行关联分析，找到样品特异高表达的模块。 Connectivity (连接度)：类似于网络中 “度” (degree)的概念。每个基因的连接度是与其相连的基因的边属性之和。 Module eigengene E: 给定模型的第一主成分，代表整个模型的基因表达谱。这个是个很巧妙的梳理，我们之前讲过PCA分析的降维作用，之前主要是拿来做可视化，现在用到这个地方，很好的用一个向量代替了一个矩阵，方便后期计算。(降维除了PCA，还可以看看tSNE) Intramodular connectivity: 给定基因与给定模型内其他基因的关联度，判断基因所属关系。 Module membership: 给定基因表达谱与给定模型的eigengene的相关性。 Hub gene: 关键基因 (连接度最多或连接多个模块的基因)。 Adjacency matrix (邻接矩阵)：基因和基因之间的加权相关性值构成的矩阵。 TOM (Topological overlap matrix)：把邻接矩阵转换为拓扑重叠矩阵，以降低噪音和假相关，获得的新距离矩阵，这个信息可拿来构建网络或绘制TOM图。WGCNA认为基因之间的简单的相关性不足以计算共表达，所以它利用上面的邻近矩阵，又计算了一个新的邻近矩阵。一般来说，TOM就是WGCNA分析的最终结果，后续的只是对TOM的下游注释。 基本流程该分析方法旨在寻找协同表达的基因模块(module)，并探索基因网络与关注的表型之间的关联关系，以及网络中的核心基因。推荐5组(或者15个样品)以上的数据。一般可应用的研究方向有：不同器官或组织类型发育调控、同一组织不同发育调控、非生物胁迫不同时间点应答、病原菌侵染后不同时间点应答。从方法上来讲，WGCNA分为表达量聚类分析和表型关联两部分。 1. 基因之间相关系数计算计算任意两个基因之间的相关系数（Person Coefficient）。为了衡量两个基因是否具有相似表达模式，一般需要设置阈值来筛选，高于阈值的则认为是相似的。但是这样如果将阈值设为0.8，那么很难说明0.8和0.79两个是有显著差别的。因此，WGCNA分析时采用相关系数加权值，即对基因相关系数取N次幂，使得网络中的基因之间的连接服从无尺度网络分布(scale-freenetworks)，这种算法更具生物学意义。 2. 基因模块（表达模式相似的基因分为一类，这样的一类基因称为模块）的确定通过基因之间的相关系数构建分层聚类树，聚类树的不同分支代表不同的基因模块，不同颜色代表不同的模块。基于基因的加权相关系数，将基因按照表达模式进行分类，将模式相似的基因归为一个模块。这样就可以将几万个基因通过基因表达模式被分成了几十个模块，是一个提取归纳信息的过程。 得到模块之后的下游分析 模块的功能富集 模块与性状之间的相关性 模块与样本间的相关系数 挖掘模块的关键信息 找到模块的核心基因 利用关系预测基因功能3. 共表达网络4. 模块与性状关联 示例程序数据下载 STEP1: 输入数据的准备这里主要是表达矩阵， 如果是芯片数据，那么常规的归一化矩阵即可，如果是转录组数据，最好是RPKM值或者其它归一化好的表达量。然后就是临床信息或者其它表型，总之就是样本的属性。为了保证后续脚本的统一性，表达矩阵统一用datExpr标识，临床 信息统一用datTraits标识。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354setwd('WGCNA/')# 56 breast cancer cell lines were profiled to identify patterns of gene expression associated with subtype and response to therapeutic compounds.if(F)&#123; ## linux 代码 ## https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE48213 #wget -c ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE48nnn/GSE48213/suppl/GSE48213_RAW.tar #tar -xf GSE48213_RAW.tar #gzip -d *.gz ## 首先在GSE48213_RAW目录里面生成tmp.txt文件，使用shell脚本： ## awk '&#123;print FILENAME"\t"$0&#125;' * |grep -v EnsEMBL_Gene_ID &gt;tmp.txt # 然后把tmp.txt导入R语言里面用reshape2处理即可！ a=read.table('GSE48213_RAW/tmp.txt',sep = '\t',stringsAsFactors = F) library(reshape2) fpkm &lt;- dcast(a,formula = V2~V1) rownames(fpkm)=fpkm[,1] fpkm=fpkm[,-1] colnames(fpkm)=sapply(colnames(fpkm),function(x) strsplit(x,"_")[[1]][1]) library(GEOquery) a=getGEO('GSE48213') metadata=pData(a[[1]])[,c(2,10,12)] datTraits = data.frame(gsm=metadata[,1], cellline=trimws(sapply(as.character(metadata$characteristics_ch1),function(x) strsplit(x,":")[[1]][2])), subtype=trimws(sapply(as.character(metadata$characteristics_ch1.2),function(x) strsplit(x,":")[[1]][2])) )save(fpkm,datTraits,file = 'GSE48213-wgcna-input.RData')&#125; library(WGCNA)RNAseq_voom &lt;- fpkm ## 因为WGCNA针对的是基因进行聚类，而一般我们的聚类是针对样本用hclust即可，所以这个时候需要转置。WGCNA_matrix = t(RNAseq_voom[order(apply(RNAseq_voom,1,mad), decreasing = T)[1:5000],])datExpr0 &lt;- WGCNA_matrix ## top 5000 mad genesdatExpr &lt;- datExpr0 ## 下面主要是为了防止临床表型与样本名字对不上sampleNames = rownames(datExpr);traitRows = match(sampleNames, datTraits$gsm) rownames(datTraits) = datTraits[traitRows, 1]head(datTraits) ## 56 个细胞系的分类信息，表型 gsm cellline subtypeGSM1172844 GSM1172844 184A1 Non-malignantGSM1172845 GSM1172845 184B5 Non-malignantGSM1172846 GSM1172846 21MT1 BasalGSM1172847 GSM1172847 21MT2 BasalGSM1172848 GSM1172848 21NT BasalGSM1172849 GSM1172849 21PT Basalfpkm[1:4,1:4] ## 56个细胞系的36953个基因的表达矩阵 GSM1172844 GSM1172845 GSM1172846 GSM1172847ENSG00000000003 95.21255 95.69868 19.99467 65.6863763ENSG00000000005 0.00000 0.00000 0.00000 0.1492021ENSG00000000419 453.20831 243.64804 142.05818 200.4131493ENSG00000000457 18.10439 26.56661 16.12776 12.0873135 上面代码里面的rpkm就是我们的转录组数据的表达矩阵，以rpkm为单位。而datTraits就是所有样本对应的表型信息。需要自己制作，这个是学习WGCNA的基础，本次实例代码都是基于这两个数据。 这个数据集里面的56种细胞系被分成了5组，如果要分开两两做差异分析，有10种组合，也就是说需要做10次差异分析，每个差异分析结果都需要去注释，会比较麻烦，这个时候WGCNA就派上用场啦。 当然，如果你一定要去做差异分析，我也给你代码：https://github.com/jmzeng1314/my-R/blob/master/10-RNA-seq-3-groups/hisat2_mm10_htseq.R STEP2:确定最佳BETA值选择合适软阀值（soft thresholding power）beta。关键就是理解pickSoftThreshold函数及其返回的对象，最佳的beta值就是sft$powerEstimate 12345678910111213141516171819202122powers = c(c(1:10), seq(from = 12, to=20, by=2))# Call the network topology analysis functionsft = pickSoftThreshold(datExpr, powerVector = powers, verbose = 5)#设置网络构建参数选择范围，计算无尺度分布拓扑矩阵 # Plot the results: ##sizeGrWindow(9, 5) par(mfrow = c(1,2)); cex1 = 0.9; # Scale-free topology fit index as a function of the soft-thresholding power plot(sft$fitIndices[,1], -sign(sft$fitIndices[,3])*sft$fitIndices[,2], xlab="Soft Threshold (power)",ylab="Scale Free Topology Model Fit,signed R^2",type="n", main = paste("Scale independence")); text(sft$fitIndices[,1], -sign(sft$fitIndices[,3])*sft$fitIndices[,2], labels=powers,cex=cex1,col="red"); # this line corresponds to using an R^2 cut-off of h abline(h=0.90,col="red") # Mean connectivity as a function of the soft-thresholding power plot(sft$fitIndices[,1], sft$fitIndices[,5], xlab="Soft Threshold (power)",ylab="Mean Connectivity", type="n", main = paste("Mean connectivity")) text(sft$fitIndices[,1], sft$fitIndices[,5], labels=powers, cex=cex1,col="red") img 参数beta取值默认是1到30，上述图形的横轴均代表权重参数β，左图纵轴代表对应的网络中log(k)与log(p(k))相关系数的平方。相关系数的平方越高，说明该网络越逼近无网路尺度的分布。右图的纵轴代表对应的基因模块中所有基因邻接函数的均值。 STEP3：一步法构建共表达矩阵有了表达矩阵和估计好的最佳beta值，就可以直接构建共表达矩阵了。 所有的核心就在这一步，把输入的表达矩阵的几千个基因组归类成了几十个模块。 大体思路：计算基因间的邻接性，根据邻接性计算基因间的相似性，然后推出基因间的相异性系数，并据此得到基因间的系统聚类树。然后按照混合动态剪切树的标准，设置每个基因模块最少的基因数目为30。根据动态剪切法确定基因模块后，再次分析，依次计算每个模块的特征向量值，然后对模块进行聚类分析，将距离较近的模块合并为新的模块。 123456789101112net = blockwiseModules( datExpr, power = sft$powerEstimate, maxBlockSize = 6000, TOMType = "unsigned", minModuleSize = 30, reassignThreshold = 0, mergeCutHeight = 0.25, numericLabels = TRUE, pamRespectsDendro = FALSE, saveTOMs = TRUE, saveTOMFileBase = "AS-green-FPKM-TOM", verbose = 3 ) table(net$colors) STEP4: 模块可视化这里用不同的颜色来代表那些所有的模块，其中灰色默认是无法归类于任何模块的那些基因，如果灰色模块里面的基因太多，那么前期对表达矩阵挑选基因的步骤可能就不太合适。 重点就是plotDendroAndColors函数，它接受一个聚类的对象，以及该对象里面包含的所有个体所对应的颜色。 12345678910# Convert labels to colors for plottingmergedColors = labels2colors(net$colors)table(mergedColors)# Plot the dendrogram and the module colors underneathplotDendroAndColors(net$dendrograms[[1]], mergedColors[net$blockGenes[[1]]], "Module colors", dendroLabels = FALSE, hang = 0.03, addGuide = TRUE, guideHang = 0.05)## assign all of the gene to their corresponding module ## hclust for the genes. img 对表达矩阵进行hclust之后，加上表达矩阵里面所有样本的分组信息对应的颜色，也是可以用plotDendroAndColors函数可视化—-如下面样品图（样本进行聚类的代码跟WGCNA本身关系不大。） 1234567891011121314151617181920212223#明确样本数和基因数nGenes = ncol(datExpr)nSamples = nrow(datExpr)#首先针对样本做个系统聚类树datExpr_tree&lt;-hclust(dist(datExpr), method = "average")par(mar = c(0,5,2,0))plot(datExpr_tree, main = "Sample clustering", sub="", xlab="", cex.lab = 2, cex.axis = 1, cex.main = 1,cex.lab=1)## 如果这个时候样本是有性状，或者临床表型的，可以加进去看看是否聚类合理#针对前面构造的样品矩阵添加对应颜色sample_colors &lt;- numbers2colors(as.numeric(factor(datTraits$Tumor.Type)), colors = c("white","blue","red","green"),signed = FALSE)## 这个给样品添加对应颜色的代码需要自行修改以适应自己的数据分析项目。# sample_colors &lt;- numbers2colors( datTraits ,signed = FALSE)## 如果样品有多种分类情况，而且 datTraits 里面都是分类信息，那么可以直接用上面代码，当然，这样给的颜色不明显，意义不大。#构造10个样品的系统聚类树及性状热图par(mar = c(1,4,3,1),cex=0.8)plotDendroAndColors(datExpr_tree, sample_colors, groupLabels = colnames(sample), cex.dendroLabels = 0.8, marAll = c(1, 4, 3, 1), cex.rowText = 0.01, main = "Sample dendrogram and trait heatmap") img 可以看到这些乳腺癌的细胞系的表达谱聚类情况并不是完全与其分类匹配，所以仅仅是根据样本的分组信息做差异分析并不完全准确。 STEP5:模块和性状的关系123456789101112131415161718192021222324252627design=model.matrix(~0+ datTraits$subtype)colnames(design)=levels(datTraits$subtype)moduleColors &lt;- labels2colors(net$colors)# Recalculate MEs with color labelsMEs0 = moduleEigengenes(datExpr, moduleColors)$eigengenesMEs = orderMEs(MEs0); ##不同颜色的模块的ME值矩阵(样本vs模块)moduleTraitCor = cor(MEs, design , use = "p");moduleTraitPvalue = corPvalueStudent(moduleTraitCor, nSamples)sizeGrWindow(10,6)# Will display correlations and their p-valuestextMatrix = paste(signif(moduleTraitCor, 2), "\n(", signif(moduleTraitPvalue, 1), ")", sep = "");dim(textMatrix) = dim(moduleTraitCor)par(mar = c(6, 8.5, 3, 3));# Display the correlation values within a heatmap plotlabeledHeatmap(Matrix = moduleTraitCor, xLabels = colnames(design), yLabels = names(MEs), ySymbols = names(MEs), colorLabels = FALSE, colors = greenWhiteRed(50), textMatrix = textMatrix, setStdMargins = FALSE, cex.text = 0.5, zlim = c(-1,1), main = paste("Module-trait relationships")) 通过模块与各种表型的相关系数，可以很清楚的挑选自己感兴趣的模块进行下游分析了。这个图就是把moduleTraitCor这个矩阵给用热图可视化一下。 img从上图已经可以看到跟乳腺癌分类相关的基因模块了，包括&quot;Basal&quot; &quot;Claudin-low&quot; &quot;Luminal&quot; &quot;Non-malignant&quot; &quot;unknown&quot; 这5类所对应的不同模块的基因列表。可以看到每一种乳腺癌都有跟它强烈相关的模块，可以作为它的表达signature，模块里面的基因可以拿去做下游分析。我们看到Luminal表型跟棕色的模块相关性高达0.86，而且极其显著的相关，所以值得我们挖掘，这个模块里面的基因是什么，为什么如此的相关呢？ STEP6:感兴趣性状的模块的具体基因分析性状跟模块虽然求出了相关性，可以挑选最相关的那些模块来分析，但是模块本身仍然包含非常多的基因，还需进一步的寻找最重要的基因。所有的模块都可以跟基因算出相关系数，所有的连续型性状也可以跟基因的表达值算出相关系数。主要参考资料：PDF document, R script 如果跟性状显著相关基因也跟某个模块显著相关，那么这些基因可能就非常重要。 首先计算模块与基因的相关性矩阵123456789# names (colors) of the modulesmodNames = substring(names(MEs), 3)geneModuleMembership = as.data.frame(cor(datExpr, MEs, use = "p"));## 算出每个模块跟基因的皮尔森相关系数矩阵## MEs是每个模块在每个样本里面的值## datExpr是每个基因在每个样本的表达量MMPvalue = as.data.frame(corPvalueStudent(as.matrix(geneModuleMembership), nSamples));names(geneModuleMembership) = paste("MM", modNames, sep="");names(MMPvalue) = paste("p.MM", modNames, sep=""); 再计算性状与基因的相关性矩阵12345678## 只有连续型性状才能只有计算## 这里把是否属于 Luminal 表型这个变量用0,1进行数值化。Luminal = as.data.frame(design[,3]);names(Luminal) = "Luminal"geneTraitSignificance = as.data.frame(cor(datExpr, Luminal, use = "p"));GSPvalue = as.data.frame(corPvalueStudent(as.matrix(geneTraitSignificance), nSamples));names(geneTraitSignificance) = paste("GS.", names(Luminal), sep="");names(GSPvalue) = paste("p.GS.", names(Luminal), sep=""); 最后把两个相关性矩阵联合起来,指定感兴趣模块进行分析1234567891011module = "brown" column = match(module, modNames); moduleGenes = moduleColors==module; sizeGrWindow(7, 7); par(mfrow = c(1,1)); verboseScatterplot(abs(geneModuleMembership[moduleGenes, column]), abs(geneTraitSignificance[moduleGenes, 1]), xlab = paste("Module Membership in", module, "module"), ylab = "Gene significance for Luminal", main = paste("Module membership vs. gene significance\n"), cex.main = 1.2, cex.lab = 1.2, cex.axis = 1.2, col = module) img 可以看到这些基因不仅仅是跟其对应的模块高度相关，而且是跟其对应的性状高度相关，进一步说明了基因值得深度探究。 STEP7:网络的可视化主要参考资料：PDF document, R script 首先针对所有基因画热图1234567nGenes = ncol(datExpr)nSamples = nrow(datExpr)geneTree = net$dendrograms[[1]]; dissTOM = 1-TOMsimilarityFromExpr(datExpr, power = 6); plotTOM = dissTOM^7; diag(plotTOM) = NA; #TOMplot(plotTOM, geneTree, moduleColors, main = "Network heatmap plot, all genes") 这个非常消耗计算资源和时间，所以建议选取其中部分基因作图即可，我就没有画，而且根据下面的代码选取部分基因来作图！ 然后随机选取部分基因作图123456789101112131415nSelect = 400# For reproducibility, we set the random seedset.seed(10);select = sample(nGenes, size = nSelect);selectTOM = dissTOM[select, select];# There’s no simple way of restricting a clustering tree to a subset of genes, so we must re-cluster.selectTree = hclust(as.dist(selectTOM), method = "average")selectColors = moduleColors[select];# Open a graphical windowsizeGrWindow(9,9)# Taking the dissimilarity to a power, say 10, makes the plot more informative by effectively changing# the color palette; setting the diagonal to NA also improves the clarity of the plotplotDiss = selectTOM^7;diag(plotDiss) = NA;TOMplot(plotDiss, selectTree, selectColors, main = "Network heatmap plot, selected genes") img 这个图凑数的意义居多，如果是把全部基因画上去，可以很清楚的看到各个区块颜色差异。 最后画模块和性状的关系123456789101112131415161718192021222324# Recalculate module eigengenes MEs = moduleEigengenes(datExpr, moduleColors)$eigengenes ## 只有连续型性状才能只有计算 ## 这里把是否属于 Luminal 表型这个变量用0,1进行数值化。 Luminal = as.data.frame(design[,3]); names(Luminal) = "Luminal" # Add the weight to existing module eigengenes MET = orderMEs(cbind(MEs, Luminal)) # Plot the relationships among the eigengenes and the trait sizeGrWindow(5,7.5); par(cex = 0.9) plotEigengeneNetworks(MET, "", marDendro = c(0,4,1,2), marHeatmap = c(3,4,1,2), cex.lab = 0.8, xLabelsAngle = 90) # Plot the dendrogram sizeGrWindow(6,6); par(cex = 1.0) ## 模块的聚类图 plotEigengeneNetworks(MET, "Eigengene dendrogram", marDendro = c(0,4,2,0), plotHeatmaps = FALSE) # Plot the heatmap matrix (note: this plot will overwrite the dendrogram plot) par(cex = 1.0) ## 性状与模块热图 plotEigengeneNetworks(MET, "Eigengene adjacency heatmap", marHeatmap = c(3,4,2,2), plotDendrograms = FALSE, xLabelsAngle = 90) img step8:提取指定模块的基因名123456# Select modulemodule = "brown";# Select module probesprobes = colnames(datExpr) ## 我们例子里面的probe就是基因名inModule = (moduleColors==module);modProbes = probes[inModule]; 有了基因信息，下游分析就很简单了。包括GO/KEGG等功能数据库的注释。 Step9: 模块的导出主要模块里面的基因直接的相互作用关系信息可以导出到cytoscape,VisANT等网络可视化软件。 12345678910111213# Recalculate topological overlapTOM = TOMsimilarityFromExpr(datExpr, power = 6); # Select modulemodule = "brown";# Select module probesprobes = colnames(datExpr) ## 我们例子里面的probe就是基因名inModule = (moduleColors==module);modProbes = probes[inModule]; ## 也是提取指定模块的基因名# Select the corresponding Topological OverlapmodTOM = TOM[inModule, inModule];dimnames(modTOM) = list(modProbes, modProbes)## 模块对应的基因关系矩阵 首先是导出到VisANT 1234vis = exportNetworkToVisANT(modTOM,file = paste("VisANTInput-", module, ".txt", sep=""),weighted = TRUE,threshold = 0) 然后是导出到cytoscape 123456789cyt = exportNetworkToCytoscape( modTOM, edgeFile = paste("CytoscapeInput-edges-", paste(module, collapse="-"), ".txt", sep=""), nodeFile = paste("CytoscapeInput-nodes-", paste(module, collapse="-"), ".txt", sep=""), weighted = TRUE, threshold = 0.02, nodeNames = modProbes, nodeAttr = moduleColors[inModule] ); 如果模块包含的基因太多，网络太复杂，还可以进行筛选，比如： 1234nTop = 30;IMConn = softConnectivity(datExpr[, modProbes]);top = (rank(-IMConn) &lt;= nTop)filter &lt;- modTOM[top, top] 后面就是cytoscape自身的教程了，这里不再赘述。]]></content>
  </entry>
  <entry>
    <title><![CDATA[clusterProfiler包]]></title>
    <url>%2Fhexo%2F2018%2F12%2F23%2F2018-12-23_clusterProfiler%2F</url>
    <content type="text"><![CDATA[clusterProfiler包 GO classification （GO分类，似乎不重要的功能）groupGO is designed for gene classification based on GO distribution at a specific level. 问题： GO的level，在什么研究中会涉及到？ 一般来说，level越大，GO功能越具体。level就像一棵树树主干一样，发了不同的枝叶，level越大枝叶越详细。 123456789101112131415161718data(geneList, package="DOSE")gene &lt;- names(geneList)[abs(geneList) &gt; 2]gene.df &lt;- bitr(gene, fromType = "ENTREZID", toType = c("ENSEMBL", "SYMBOL"), OrgDb = org.Hs.eg.db)head(gene.df)## ENTREZID ENSEMBL SYMBOL## 1 4312 ENSG00000196611 MMP1## 2 8318 ENSG00000093009 CDC45## 3 10874 ENSG00000109255 NMU## 4 55143 ENSG00000134690 CDCA8## 5 55388 ENSG00000065328 MCM10## 6 991 ENSG00000117399 CDC20ggo &lt;- groupGO(gene = gene, OrgDb = org.Hs.eg.db, ont = "CC", level = 3, readable = TRUE) GO over-representation test（GO分析，常用）Over-representation test(Boyle et al. 2004) were implemented in clusterProfiler. For calculation details and explanation of paramters, please refer to the vignette of DOSE. 1234567891011121314151617181920212223ego &lt;- enrichGO(gene = gene, universe = names(geneList), OrgDb = org.Hs.eg.db, ont = "CC", pAdjustMethod = "BH", pvalueCutoff = 0.01, qvalueCutoff = 0.05, readable = TRUE)head(ego)## ID Description GeneRatio## GO:0005819 GO:0005819 spindle 25/202## GO:0005876 GO:0005876 spindle microtubule 12/202## GO:0000779 GO:0000779 condensed chromosome, centromeric region 15/202## GO:0072686 GO:0072686 mitotic spindle 14/202## GO:0000775 GO:0000775 chromosome, centromeric region 18/202## GO:0000776 GO:0000776 kinetochore 15/202## BgRatio pvalue p.adjust qvalue## GO:0005819 262/11812 2.569986e-12 8.146855e-10 7.520590e-10## GO:0005876 45/11812 7.912518e-12 1.254134e-09 1.157726e-09## GO:0000779 91/11812 3.254384e-11 3.438799e-09 3.174452e-09## GO:0072686 84/11812 1.292642e-10 9.903693e-09 9.142376e-09## GO:0000775 156/11812 1.562097e-10 9.903693e-09 9.142376e-09## GO:0000776 106/11812 3.073022e-10 1.623580e-08 1.498772e-08 Any gene ID type that supported in OrgDb can be directly used in GO analyses. User need to specify the keyType parameter to specify the input gene ID type. 1234567ego2 &lt;- enrichGO(gene = gene.df$ENSEMBL, OrgDb = org.Hs.eg.db, keyType = 'ENSEMBL', ont = "CC", pAdjustMethod = "BH", pvalueCutoff = 0.01, qvalueCutoff = 0.05) Gene ID can be mapped to gene Symbol by using paramter readable=TRUE or setReadable function. 1ego2 &lt;- setReadable(ego2, OrgDb = org.Hs.eg.db) KEGG over-representation test1234kk &lt;- enrichKEGG(gene = gene, organism = 'hsa', pvalueCutoff = 0.05)head(kk) KEGG Module over-representation testKEGG Module is a collection of manually defined function units. In some situation, KEGG Modules have a more straightforward interpretation. 12mkk &lt;- enrichMKEGG(gene = gene, organism = 'hsa') GO Gene Set Enrichment AnalysisGO GSEA 与GSEA有什么区别? 12345678ego3 &lt;- gseGO(geneList = geneList, OrgDb = org.Hs.eg.db, ont = "CC", nPerm = 1000, minGSSize = 100, maxGSSize = 500, pvalueCutoff = 0.05, verbose = FALSE) KEGG Gene Set Enrichment Analysis12345678910111213141516171819202122232425262728kk2 &lt;- gseKEGG(geneList = geneList, organism = 'hsa', nPerm = 1000, minGSSize = 120, pvalueCutoff = 0.05, verbose = FALSE)head(kk2)## ID Description setSize## hsa04151 hsa04151 PI3K-Akt signaling pathway 322## hsa04510 hsa04510 Focal adhesion 188## hsa04218 hsa04218 Cellular senescence 143## hsa05166 hsa05166 Human T-cell leukemia virus 1 infection 200## hsa05169 hsa05169 Epstein-Barr virus infection 192## hsa05170 hsa05170 Human immunodeficiency virus 1 infection 189## enrichmentScore NES pvalue p.adjust qvalues rank## hsa04151 -0.3482755 -1.503715 0.001328021 0.0276644 0.01814059 1997## hsa04510 -0.4188582 -1.725829 0.001390821 0.0276644 0.01814059 2183## hsa04218 0.4153718 1.792654 0.002967359 0.0276644 0.01814059 1155## hsa05166 0.3926594 1.744570 0.003484321 0.0276644 0.01814059 1955## hsa05169 0.4343189 1.916668 0.003533569 0.0276644 0.01814059 2820## hsa05170 0.3711150 1.635182 0.003533569 0.0276644 0.01814059 3178## leading_edge## hsa04151 tags=23%, list=16%, signal=20%## hsa04510 tags=27%, list=17%, signal=23%## hsa04218 tags=17%, list=9%, signal=16%## hsa05166 tags=26%, list=16%, signal=23%## hsa05169 tags=40%, list=23%, signal=31%## hsa05170 tags=38%, list=25%, signal=29% KEGG Module Gene Set Enrichment Analysis12mkk2 &lt;- gseMKEGG(geneList = geneList, species = 'hsa') 剩下富集分析点图 直接看网页即可。]]></content>
  </entry>
  <entry>
    <title><![CDATA[R Apply函数]]></title>
    <url>%2Fhexo%2F2018%2F12%2F22%2F2018-12-2_R_applyfunction%2F</url>
    <content type="text"><![CDATA[apply Apply Functions Over ArrayMargins 对阵列行或者列使用函数 apply(X, MARGIN, FUN,…) lapply Apply a Function over a Listor Vector对列表或者向量使用函数 lapply(X, FUN,…) sapply Apply a Function over a Listor Vector对列表或者向量使用函数 sapply(X, FUN, …, simplify= TRUE, USE.NAMES = TRUE) vapply Apply a Function over a Listor Vector对列表或者向量使用函数 vapply(X, FUN, FUN.VALUE,…, USE.NAMES = TRUE) tapply Apply a Function Over aRagged Array对不规则阵列使用函数 tapply(X, INDEX, FUN = NULL,…, simplify = TRUE) eapply Apply a Function Over Valuesin an Environment对环境中的值使用函数 eapply(env, FUN, …,all.names = FALSE, USE.NAMES = TRUE) mapply Apply a Function to MultipleList or Vector Arguments对多个列表或者向量参数使用函数 mapply(FUN, …, MoreArgs =NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) rapply Recursively Apply a Functionto a List运用函数递归产生列表 rapply(object, f, classes =”ANY”, deflt = NULL,how = c(“unlist”, “replace”, “list”),…) apply(X, MARGIN, FUN,…)X阵列，包括矩阵MARGIN 1表示矩阵行，2表示矩阵列，也可以是c(1,2) lapply(X, FUN, …)X表示一个向量或者表达式对象，其余对象将被通过as.list强制转换为list sapply(X, FUN, …, simplify= TRUE, USE.NAMES = TRUE)X表示一个向量或者表达式对象，其余对象将被通过as.list强制转换为listsimplify逻辑值或者字符串，如果可以，结果应该被简化为向量、矩阵或者高维数组。必须是命名的，不能是简写。默认值是TRUE，若合适将会返回一个向量或者矩阵。如果simplify=”array”，结果将返回一个阵列。USE.NAMES 逻辑值，如果为TRUE，且x没有被命名，则对x进行命名。这是一个用户友好版本，是lapply函数的包装版。该函数返回值为向量、矩阵，如果simplify=”array”，且合适的情况下，将会通过simplify2array()函数转换为阵列。sapply(x, f, simplify=FALSE,USE.NAMES=FALSE)返回的值与lapply(x,f)是一致的。 举几个不是很常见但确实很有用的函数。 1、tapply() tapply最常用于分组求和 df = data.frame(x1 = 1:15, x2 = c(rep(1, 1), rep(2, 2), ​ rep(3, 3), rep(4, 4), rep(5, 5))) me = tapply(X = df$x1, INDEX = df$x2, FUN = mean); me 1 2 3 4 5 1.0 2.5 5.0 8.5 13.0 其实这里最强大的是FUN这个参数，因为函数完全可以自己写，比如： su = tapply(X = df$x1, INDEX = df$x2, FUN = function(x){sum(x)}); su 1 2 3 4 5 1 5 15 34 65 2、sweep() sweep函数用法有点类似apply，但是比apply更强大。 mat = matrix(1:25, ncol = 5, byrow = T) sweep(mat, 1, 1:5, FUN = “+”) ​ [,1] [,2] [,3] [,4] [,5] [1,] 2 3 4 5 6 [2,] 8 9 10 11 12 [3,] 14 15 16 17 18 [4,] 20 21 22 23 24 [5,] 26 27 28 29 30 sweep(mat, 2, 1:5, FUN = “-“) ​ [,1] [,2] [,3] [,4] [,5] [1,] 0 0 0 0 0 [2,] 5 5 5 5 5 [3,] 10 10 10 10 10 [4,] 15 15 15 15 15 [5,] 20 20 20 20 20 同样的，这里函数可以自己定义。 3、coplot() coplot这个协同图函数tiger的课程中有讲到过，简单的用法如下： studata = read.table(‘exam3.7data.txt’, header = TRUE); studata 学号 姓名 性别 年龄 身高 体重 1 1 Alice F 13 56.5 84.0 2 2 Becka F 13 65.3 98.0 3 3 Gail F 14 64.3 90.0 4 4 Karen F 12 56.3 77.0 5 5 Kathy F 12 59.8 84.5 6 6 Mary F 15 66.5 112.0 7 7 Sandy F 11 51.3 50.5 8 8 Sharon F 15 62.5 112.5 9 9 Tammy F 14 62.8 102.5 10 10 Alfred M 14 69.0 112.5 11 11 Duke M 14 63.5 102.5 12 12 Guido M 15 67.0 133.0 13 13 James M 12 57.3 83.0 14 14 Jeffrey M 13 62.5 84.0 15 15 John M 12 59.0 99.5 16 16 Philip M 16 72.0 150.0 17 17 Robert M 12 64.8 128.0 18 18 Thomas M 11 57.5 85.0 19 19 William M 15 66.5 112.0 coplot(体重 ~ 身高 | 性别 + 年龄, data = studata) 其实运用R的技巧，这个函数能做的事情还有好多，比如： panel.lm = function(x, y, …) { tmp = lm(y ~ x) abline(tmp, lwd = 2, col = 4) points(x,y, …)} coplot(体重 ~ 身高 | 性别 + 年龄, data = studata, panel = panel.lm, number = 3) 如果还有其他因素，这个函数还能显示更多： studata$team = c(rep(2, 10), rep(3, 9)) coplot(体重 ~ 身高 | 性别 + 年龄, panel = panel.lm, cex = 1.5, pch = 19, number = 3, ​ col = studata$team, data = studata) 如果把学生按组或其他因素分开，也能用散点颜色进行区分。 如果还有成绩等因素，还能按成绩等级或者平均分进行划分。。。]]></content>
  </entry>
  <entry>
    <title><![CDATA[R 字符串处理]]></title>
    <url>%2Fhexo%2F2018%2F12%2F21%2F2018-12-2_String_process%2F</url>
    <content type="text"><![CDATA[字符翻译 tolower(), toupper() chartr() 12dna &lt;-"AgCTaaGGGcctTagct"chartr("T", "U", dna)#将T碱基替换成U碱基 字符串连接 paste (…, sep = “ “, collapse = NULL) paste0(…, collapse = NULL) sep: a character string to separate the terms. collapse: an optional character string to separate the results. 12345678910paste("control", 1:3, sep ="_")## [1] "control_1" "control_2" "control_3"x &lt;- list(a ="aa", b ="bb")y &lt;- list(c =1, d =2)paste(x, y, sep ="-")## [1] "aa-1" "bb-2"paste(x, y, sep ="-", collapse =";")## [1] "aa-1;bb-2"paste(x, collapse =":")## [1] "aa:bb" 特殊应用场景：对应一个矩阵x, 按行连接矩阵元素(以“#”分隔)并生成一个向量. 1paste(x, collapse = '#') 字符串替换 gsub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE) gsub()是对向量里面的每个元素进行搜素，如果发现元素里面有多个位置匹配了模式，则全部进行替换. 12345text &lt;- c("we are the world", "we are the children") gsub("w", "W", text)# [1] "We are the World" "We are the children" gsub(" ", "", "abc def ghi")# [1] "abcdefghi" 字符串查找 grep(pattern, x, ignore.case= F, perl= F, value= F, fixed= F, useBytes= F, invert= F 返回向量x中哪个元素匹配了模式pattern（即返回了向量x的某些下标）或者具体哪个元素匹配了模式（通过设置value参数来完成） grepl(pattern, x, ignore.case= F, perl= F, fixed= F, useBytes= F) 返回一系列逻辑值，其长度等同于向量x的长度，表示向量x中的元素是否匹配了模式。 以上两个函数都没有提供具体的位置信息，即向量x中的元素在哪个位置匹配了模式。 regexpr(pattern, text, ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE) 查找每个向量中每个元素第一个匹配的位置，返回值是向量 regexec(pattern, text, ignore.case = FALSE, fixed = FALSE, useBytes = FALSE) 查找每个向量中每个元素第一个匹配的位置，返回值是list gregexpr(pattern, text, ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE) 查找向量中每个元素所有能匹配的位置，返回值是list。 这个函数最常用☆ ​ 以上三个返回的结果中包含了匹配的具体位置和字符串长度信息（因此可用于字符串的提取操作中去） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253## 非常清晰的示例程序text &lt;- c("We are the world", "we are the children")grep("We", text)#向量text中的哪些元素匹配了单词'We'## [1] 1grep("We", text, invert =T)#向量text中的哪些元素没有匹配单词'We'## [1] 2grep("we", text, ignore.case =T)#匹配时忽略大小写## [1] 1 2grepl("We", text)#向量text中的每个元素是否匹配了单词'We'，即只返回TRUE或FALSE## [1] TRUE FALSEregexpr("e", text)## [1] 2 2## attr(,"match.length")## [1] 1 1## attr(,"useBytes")## [1] TRUEregexec("e", text) ## [[1]]## [1] 2## attr(,"match.length")## [1] 1## [[2]]## [1] 2## attr(,"match.length")## [1] 1gregexpr("e", text)## [[1]]## [1] 2 6 10## attr(,"match.length")## [1] 1 1 1## attr(,"useBytes")## [1] TRUE## [[2]]## [1] 2 6 10 18## attr(,"match.length")## [1] 1 1 1 1## attr(,"useBytes")## [1] TRUE##另外一个例子gregexpr(pattern ='2',"the2quickbrownfoxeswere2tired")library(stringr)str_locate_all(pattern ='2', "the2quickbrownfoxeswere2tired")##[[1]]## start end##[1,] 4 4##[2,] 24 24lapply(strsplit(x, ''), function(x) which(x == '2')) 完全匹配 match(x, table, nomatch= NAinteger, incomparables) 只有参数x的内容被完全匹配，函数才会返回参数x所在table参数中的下标，否则的话会返回nomatch参数中定义的值（默认是NA） 此外还有一个charmatch()或者pmatch()，其命令形式类似于match。同样该函数也会返回其匹配字符串所在table中的下标，该函数在进行匹配时，会从table里字符串的最左面（即第一个字符）开始匹配，如果起始位置没有匹配则返回NA；如果同时部分匹配和完全匹配，则会优先选择完全匹配；如果同时有多个完全匹配或者多个部分匹配时，则会返回0；如果以上三个都没有，则返回NA。 12345678910111213141516171819202122text &lt;- c("We are the world", "we are the children", "we") match("we", text)# [1] 3 match(2, c(3, 4, 2, 8))# [1] 3 match("xx", c("abc", "xxx", "xx", "xx"))#只会返回第一个完全匹配的元素的下标# [1] 3 match("xx", c("abc", "xxx"))# 没有完全匹配的，因此返回NA# [1] NA charmatch("xx", c("abc", "xxa"))# [1] 2 charmatch("xx", c("abc", "axx"))# 从最左面开始匹配# [1] NA charmatch("xx", c("xxa", "xxb"))# 不唯一# [1] 0 charmatch("xx", c("xxa", "xxb", "xx"))# 优先选择完全匹配，尽管有两个部分匹配# [1] 3 charmatch(2, c(3, 4, 2, 8))# [1] 3 charmatch(2, c(3, 4, 2, 8, 2))# [1] 0 多重查找12345678x &lt;- sample(1:10,20,replace=TRUE)x # [1]8 6 9 9 7 3 2 5 5 1 6 8 5 2 9 3 5 1 0 8 2inds &lt;- which(x %in% c(8,9))inds# [1]1 3 4 1 2 1 5 1 9x[inds]# [1]8 9 9 8 9 8 查找所有重复项123vec &lt;- c("a","b","c","c","c") vec[duplicated(vec)| duplicated(vec, fromLast=TRUE)]## [1] "c" "c" "c" 比较两个向量123#compare two vector in Rall( vector1, vetor2 )isTRUE(all.equal(D,E)) 字符串分割1strsplit(x, split, fixed = FALSE, perl = FALSE, useBytes = FALSE) 参数x为字符串格式向量，函数依次对向量的每个元素进行拆分 参数split为拆分位置的字串向量，即在哪个字串处开始拆分；该参数默认是正则表达式匹配；若设置fixed= T则表示是用普通文本匹配或者正则表达式的精确匹配。用普通文本来匹配的运算速度要快些。 参数perl的设置和perl的版本有关，表示可以使用perl语言里面的正则表达式。如果正则表达式过长，则可以考虑使用perl的正则来提高运算速度。 *参数useBytes表示是否逐字节进行匹配，默认为FALSE，表示是按字符匹配而不是按字节进行匹配 strsplit()的返回结果是list类型，如果想将其转换成字符串类型，则可以使用上面提到的unlist()和as.character()。有一种特殊情况，如果strsplit()的split参数为空字符串的话，得函数的返回结果是一个个字符。 特殊应用场景：对于输入是一个字符串vector, 取每一个元素拆分的第二位置的数据。 123library(plyr)gene=c("1 KRT19", "2 AGR2","3 RAB25", "4 CDH1","5 ERBB3")gene=ldply(strsplit(gene, split = " "))[[2]] 字符串提取 substr(x, start, stop) 返回字符串个数等于第一个向量的长度 substring(text, first, last) ​ 返回的字符串个数等于其三个参数中长度最长的那个参数的长度 12345678910x &lt;-"123456789" substr(x, c(2, 4), c(4, 5, 8))# [1] "234" substring(x, c(2, 4), c(4, 5, 8))# [1] "234" "45" "2345678" y &lt;- c("12345678", "abcdefgh")substr(y, c(2, 4), c(4, 5, 8))# [1] "234" "de" substring(y, c(2, 4), c(4, 5, 8))# [1] "234" "de" "2345678" 从上面的输出结果来，向量x的长度为1，substr()不管后面的两个参数的长度如何，它只会用到这两个参数的第一个数值，即分别为2和4，表示提取的起始和终止位置分别为2和4，返回的结果则是字符串“234”。而用substring()来提取时，则会依据参数最长的last参数，此外还需要注意的是first和last两个参数的长度不等，这时会用到R里面的“短向量循环”原则，参数first会自动延长为c(2, 4, 2)，函数会依次提取从2到4，从4到5，从2到8这三个字符串。 123456#用substing()可以很方便地把DNA或RNA序列进行翻译（三个碱基转换成一个密码子）。dna &lt;- paste(sample(c("A", "G", "C", "T"), 12, replace =T), collapse ="") dna # [1] "ATAACGCGTGGG" substring(dna, seq(1, 10, by =3), seq(3, 12, by =3))# [1] "ATA" "ACG" "CGT" "GGG" 字符串的定制输出 strtrim(x, width) 将字符串修剪到特定的显示宽度 strwrap(x, width, indent= 0, exdent= 0, prefix= “”, simplify= T, initial= prefix) 把字符串当成一个段落来处理（不管段落中是否有换行），按照段落的格式进行缩进和分行，返回结果就是 一行行的字符串. 1234strtrim(c("abcde", "abcde", "abcde"), c(1, 5, 10))## [1] "a" "abcde" "abcde"strtrim(c(1, 123, 12345), 4)#短向量循环## [1] "1" "123" "1234" 其他处理技巧汇总1,查找 NA 12which(is.na(vector))any(is.na(data)) 2, 生成一个限定长度带名字的list 123mylist.names &lt;- c("a", "b", "c")mylist &lt;- vector("list", length(mylist.names))names(mylist) &lt;- mylist.names 3, 矩阵和向量按行乘 123456MyMatrix &lt;- matrix(c(1,2,3,11,12,13), nrow =2, ncol=3, byrow=TRUE)MyVector &lt;- c(1:3)# You could use either:t(t(MyMatrix)* MyVector)# or:MyMatrix %*% diag(MyVector) 4,通过字符串取对象的值 1234567891011ret_series &lt;- c(1,2,3);x &lt;-"ret_series";get(x)#相反功能assign(name[i],feature_cox_result_order)a =1get('a')#To use it with save:to_be_saved_obj = paste("mat", a, sep ="_") save(list = to_be_saved_obj, file ='mat.Rdata')]]></content>
  </entry>
  <entry>
    <title><![CDATA[GSVA基因集合变异分析]]></title>
    <url>%2Fhexo%2F2018%2F12%2F21%2F2018-12-21_GSVA%2F</url>
    <content type="text"><![CDATA[GSVA 微阵列和RNA-Seq数据的基因集合变异分析GSVA: gene set variation analysis for microarray and RNA-Seq data GSVA和GSEA的不同 GSVA是先将基因对应到gene set上,得到不同的gene set的得分,然后再根据分组,对gene set的得分进行差异分析. GSEA是先根据分组对基因进行差异分析,再将基因富集到gene set中. GSVA分析中需要用的文件只有两个:(1)基因表达矩阵(gene expression matrix),SYMBOL或者INTERID格式的均可，但要做均一化处理(如TPM值即可) (2)geneset:可以直接从MSigDB下载(一般是.gmt格式的文件)，或者自己制作。下载时，要选择合适的geneID，和表达矩阵保持一致即可。（一般都是SYMBOL格式，因为比较容易供使用者观看） MSiDB中的数据官网主页： http://software.broadinstitute.org/gsea/msigdb/index.jsp 比如我们需要用KEGG相关的的gene set,选择C2,单击C2进入详细介绍页面,就可以看到我们要下载的文件链接了,记得下载和自己的gene ID 格式相同的gmt文件.下载下来之后,可以看到gmt文件的内容也很单纯,每一行表示一个gene set.第一列是gene set的名字,第三列及之后是每个gene set中含有的gene,所以自己制作gene set时,先对gene set命名然后再添加基因就可以了,但要保证其合理性. 123456789101112gene &lt;- read.csv("XXX.csv",header = TRUE,sep = ",")genesets &lt;- getGmt("c2.cp.kegg.v6.1.symbols.gmt")nGrp1 &lt;-nnGrp2 &lt;- m #n和m是对照组和处理组的组数。 design &lt;- cbind(sampleGroup1=1, sampleGroup2vs1=c(rep(0, nGrp1), rep(1, nGrp2)))library(limma)library(GSVA)#根据需要调整一下表达矩阵rownames(gene) &lt;- gene[,1] #添加gene中的行标题gene &lt;- gene[,-1] #删除第一列topMatrixGSVA &lt;- gsva(data.matrix(gene), genesets, min.sz=1, max.sz=999999, kcdf="Poisson", abs.ranking=FALSE, verbose=TRUE) #得到GSVA在这些通路上的分值。 GSVA分析https://bioconductor.org/packages/release/bioc/vignettes/GSVA/inst/doc/GSVA.pdf123456789101112131415161718192021222324library(GSVA)p &lt;- 20000 ## number of genesn &lt;- 30 ## number of samplesnGS &lt;- 100 ## number of gene setsmin.sz &lt;- 10 ## minimum gene set sizemax.sz &lt;- 100 ## maximum gene set sizeX &lt;- matrix(rnorm(p*n), nrow=p, dimnames=list(1:p, 1:n))dim(X)gs &lt;- as.list(sample(min.sz:max.sz, size=nGS, replace=TRUE)) ## sample gene set sizesgs &lt;- lapply(gs, function(n, p) sample(1:p, size=n, replace=FALSE), p) ## sample gene setses.max &lt;- gsva(X, gs, mx.diff=FALSE, verbose=FALSE, parallel.sz=1)es.dif &lt;- gsva(X, gs, mx.diff=TRUE, verbose=FALSE, parallel.sz=1)###################################################### code chunk number 3: maxvsdif###################################################par(mfrow=c(1,2), mar=c(4, 4, 4, 1))plot(density(as.vector(es.max)), main="Maximum deviation from zero", xlab="GSVA score", lwd=2, las=1, xaxt="n", xlim=c(-0.75, 0.75), cex.axis=0.8)axis(1, at=seq(-0.75, 0.75, by=0.25), labels=seq(-0.75, 0.75, by=0.25), cex.axis=0.8)plot(density(as.vector(es.dif)), main="Difference between largest\npositive and negative deviations",xlab="GSVA score", lwd=2, las=1, xaxt="n", xlim=c(-0.75, 0.75), cex.axis=0.8)axis(1, at=seq(-0.75, 0.75, by=0.25), labels=seq(-0.75, 0.75, by=0.25), cex.axis=0.8)]]></content>
  </entry>
  <entry>
    <title><![CDATA[GSEA富集分析]]></title>
    <url>%2Fhexo%2F2018%2F12%2F21%2F2018-12-21-GSEA%2F</url>
    <content type="text"><![CDATA[富集是将基因根据一些先验的知识（也就是常见的注释）进行分类的过程。我们一般会想到最常见的是GO/KEGG富集，其思路是先筛选差异基因，然后确定这些差异基因的GO/KEGG注释，然后通过超几何分布计算出哪些通路富集到了，通常会选择一个阈值来卡一下，比如p值和FDR等。因此这会涉及到人为的阈值选择，具有一定的主观性，而且只能用于差异较大的基因，所以结果可能有一定的局限性。 GSEA（Gene Set Enrichment Analysis），其思路是发表于2005年的Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles， 主要是要有两个概念： 预先定义的基因集S（基于先验知识的基因注释信息）和待测排序基因集L（一般是表达矩阵）；然后GSEA目的就是为了判断S基因集中的基因是随机分布于L（排序后的数据集），还是聚集分布在L的顶部或者底部（这也就是富集）。如果待测基因集中的某些基因显著富集在L的顶部或者底部，这说明这些基因的表达（因为其是根据表达谱数据）对你定义的分组（预先分组，表型）的差异有显著影响（一致性），从而找到我们关注的基因集。待测基因集的排序依据是其在不同表型状态下的表达差异。从而衡量基因全局表达量的变化是否有某些特定的基因集合的倾向性。 给定一个排序的基因表L和一个预先定义的基因集S (比如编码某个代谢通路的产物的基因, 基因组上物理位置相近的基因，或同一GO注释下的基因)，GSEA的目的是判断S里面的成员s在L里面是随机分布还是主要聚集在L的顶部或底部。这些基因排序的依据是其在不同表型状态下的表达差异，若研究的基因集S的成员显著聚集在L的顶部或底部，则说明此基因集成员对表型的差异有贡献，也是我们关注的基因集。 在富集分析的理论中，GSEA可以认为是第二代，即Functional Class Scoring (FCS) Approaches。GSEA（Gene Set Enrichment Analysis）方法是目前在pathway analysis方法中的state of the art. GSEAGene Set Enrichment Analysis (基因集富集分析)用来评估一个预先定义的基因集的基因 在与表型相关度排序的基因表中的分布趋势，从而判断其对表型的贡献。软件会对基因根据其于表型的关联度(可以理解为表达值的变化)从大到小排序，然后判断基因集内每条注释下的基因是否富集于表型相关度排序后基因表的上部或下部，从而判断此基因集内基因的协同变化对表型变化的影响。 输入数据包含两部分: 已知功能的基因集 (可以是GO注释、MsigDB的注释或其它符合格式的基因集定义) 表达矩阵 The gene sets are defined based on prior biological knowledge, e.g., published information about biochemical pathways or coexpression in previous experiments. The goal of GSEA is to determine whether members of a gene set S tend to occur toward the top (or bottom) of the list L, in which case the gene set is correlated with the phenotypic class distinction. 这与GO富集分析不同。GO富集分析是先筛选差异基因，再判断差异基因在哪些注释的通路存在富集；这涉及到阈值的设定，存在一定主观性并且只能用于表达变化较大的基因，即我们定义的显著差异基因。而GSEA则不局限于差异基因，从基因集的富集角度出发，理论上更容易囊括细微但协调性的变化对生物通路的影响。 GSEA原理GSEA计算中几个关键概念： 样本表型(phenotypes)：即为样本分组信息，表达矩阵的基因排序是根据该基因在两个group之间的差异来排序！GSEA会根据Signal2Noise metric 来对基因进行排序。值得注意的是如果要想计算Signal2Noise ，每个group必须要有3个及以上的samples. 富集得分 (ES, enrichment score). ES反应基因集成员s在排序列表L的两端富集的程度。计算方式是，从基因集L的第一个基因开始，计算一个累计统计值。当遇到一个落在s里面的基因，则增加统计值。遇到一个不在s里面的基因，则降低统计值。每一步统计值增加或减少的幅度与基因的表达变化程度（更严格的是与基因和表型的关联度）是相关的。富集得分ES最后定义为最大的峰值。正值ES表示基因集在列表的顶部富集，负值ES表示基因集在列表的底部富集。 评估富集得分(ES)的显著性。通过基于表型而不改变基因之间关系的排列检验 (permutation test)计算观察到的富集得分(ES)出现的可能性。若样品量少，也可基于基因集做排列检验 (permutation test)，计算p-value。 多重假设检验矫正。首先对每个基因子集s计算得到的ES根据基因集的大小进行标准化得到Normalized Enrichment Score (NES)。随后针对NES计算假阳性率。（计算NES也有另外一种方法，是计算出的ES除以排列检验得到的所有ES的平均值） Leading-edge subset，对富集得分贡献最大的基因成员。 GSEA分析- 软件界面操作https://blog.csdn.net/qazplm12_3/article/details/78561937 pdf文档 ES图 中间从红色到蓝色的过渡“带”表示基因从上调到下调排列（排序可以按照fold change,也可以是p-value)。 黑色像条形码的竖线表示该位置的基因属于某个指定通路。 绿色有波动的曲线表示富集分数，从0开始计算，属于基因通路增加，不属于则减少。 最后看下黑色的条形码是不是富集在一端。 示例说明： 红蓝颜色条 设定一个衡量差异表达程度的统计量，简单起见用log Fold Change，来把基因排序。上面那个颜色条就表示一共有17425个基因表达，下方数字表示该处对应的基因所在的序号，红色到蓝色，表示从上调到下调。 黑色的杠杠 表示在该位置处的基因属于Myc靶基因，那一共就是有188个杠杠。颜色条上方有个数字9109，它表示这这里，基因表达从上调转变成下调。那个位置颜色是白色的，也就是说，倍数差异接近0了。 深绿色曲线: 在GSEA这个检验里面，我们实际上就是在检验上面颜色条里黑色的杠杠，是否有往颜色条一端富集的趋势。实际在做这个检验时，我们是从红色的序号为1的基因出发到蓝色的序号为17425的基因，这个过程中，遍历每一个基因，每次都查看下当前基因是否是Myc靶基因，如果是，则累加一点分数，否，则扣掉一点分数。这个分数的轨迹就是ES曲线。 显著性检验统计假设检验的本质就是先生成一个零假设的数据分布，然后观察实际数据在这个零假设分布下，是不是在尾端。好了，我们把这句话具现到我们这个GSEA的例子中来。我们有三种方式： 颜色条不动它，把黑色的杠杠，从颜色条上拿起来，然后再随机的放到颜色条上 把样本的分组打乱，随机分组，重新计算排序统计量，然后排序 即做1，又做2 第一种方式，对算力要求最低，对样本容量没有要求，但是不考虑基因间的相关性，可能导致一定的假阳性。第二种方式，对算力要求较高，要求一定的样本容量（每组重复数）以保证有效置换次数，可以保持基因间的协方差结构，但power会略低。第三种方式，算力要求最高。这里假设我们随便选一种方式，重排一次以后，可以按照原先绘制绿色曲线的方法绘制一条新的曲线（零假设的数据），重复这个过程千万次的话，就可以比较精确的得到零假设的覆盖区域了，求取这个阴影的第5到95百分位数的区间，即可绘出结果图中的浅灰色阴影了。这样统计检验的显著性，就可视化成为观察绿色曲线与灰色阴影的偏离程度了。绿色曲线离x轴最大的偏离值即为该检验的Enrichment Score (ES)，把它对Myc靶基因的数量再校正一下，就可以得到 Normalized Enrichment Score (NES)。这里我们看下结果，非常显著，这个节奏和刚才用Fisher ‘s exact test的结果，明显不一样，这又是为什么呢？请仔细观察GSEA结果图里，排序统计量和颜色条上黑杠杠的分布。可以发现，绝大部分的Myc靶基因，分布在浅蓝色区域，即绝大部分Myc靶基因都是下调，但是只是微弱的下调，所以它们没有在Fisher exact test中被计入为差异基因。 作者：hoptop链接：http://www.jianshu.com/p/199b44974480來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 另一个生信技能树的例子https://vip.biotrainee.com/d/345-gsea-r 假设芯片测到了2万个基因，这两万个基因在case和control组的差异度量(六种差异度量，默认是signal 2 noise，GSEA官网有提供公式，也可以选择大家熟悉的foldchange)肯定不一样,那么根据它们的差异度量，就可以对它们进行排序，并且Z-score标准化，在ES图的最底端展示的就是signal2noise, 红蓝色条就是signal2noise值的热图。 图中间部分黑色竖条码，就是我们每个gene set里面的基因在所有的2万个排序好基因的位置，如果gene set里面的基因集中在2万个基因的前面部分，就是在case组里面富集，如果集中在后面部分，就是在control组里面富集。 最上面图的ES score 每个基因在每个gene set里面的ES score取决于这个基因是否属于该gene set，还有就是它的差异度量，上图的差异度量就是FC（foldchange）,对每个gene set来说，所有的基因的ES score都要一个个加起来，叫做running ES score，在加的过程中，什么时候ES score达到了最大值，就是这个gene set最终的ES score！所谓的GSEA分析，就是一个个遍历探索已知的基因集合，在我们的表达矩阵里面是否出现了某种统计学显著的扰动，如上图所示，要深入理解，请看我在生信菜鸟团写的另外3个教程： java版本GSEA软件的ES score图片的修改 http://www.bio-info-trainee.com/2105.html GSEA的统计学原理试讲 http://www.bio-info-trainee.com/2102.html 制作自己的gene set文件给gsea软件 http://www.bio-info-trainee.com/2144.html ssGSEA 单样本GSEAssGSEA顾名思义是一种特殊的GSEA，它主要针对单样本无法做GSEA而提出的一种实现方法，原理上与GSEA是类似的，不同的是GSEA需要准备表达谱文件即gct，根据表达谱文件计算每个基因的rank值，再进行后续的统计分析。而单个样本则无法计算rank，当然GSEA也支持你直接提供rank来进行富集分析的，这个不在此讨论，我们这里主要介绍ssGSEA用来对单个样本做GSEA富集分析的原理。 关于R的实现可以参考：https://support.bioconductor.org/p/98463/ 关于genepattern工具在这里：http://software.broadinstitute.org/cancer/software/genepattern/modules/docs/ssGSEAProjection/4 首先假设我们有一个样本的表达数据，那么他应该是这样的 第一列为基因，第二列为表达值，这样的两列的数据矩阵 首先我们对我们的样本的所有基因的表达水平进行排序获得其在所有基因中的秩次rank，这些基因的集合为BG 假设我们要对其进行KEGG的分析，首先我们需要在GSEA官网找到KEGG对应的gmt文件 gmt文件主要格式是：每行表示一个通路，第一列为通路ID,第二列为通路对应的描述，第三列开始到最后一列为该通路中的基因 那么对于任意的一个通路A,我们可以拿到这个通路的基因列表GL 我们从GL中寻找BG里存在的基因并计数为NC,并将这些基因的表达水平加和为SG 开始计算ES： 对于任意一个表达谱中的基因 G: 如果G是集合GL中的基因则他的ES等于 该基因的表达水平除以SG 否则 记该基因的ES等于 1除以（基因集合BG总个数减去NC） 依次计算每个BG中的基因的ES值，找到其中绝对值最大的ES作为通路A的A.ES 到此 通路A的ES计算完毕，我们需要一个统计学方法来评估该ES是否是显著的，即非随机的 我们按照上述计算ES的方法，先随机打乱表达谱中基因的表达顺序，然后再依次计算ES值，如此重复一千次，得到一千个ES值，我们根据这一千个ES值的分布，来计算A.ES在这个分布中所处的位置及出现在该位置时的概率即得到了p值 依次我们分别计算每个通路的ES及p值，然后使用多重检验矫正得到每个通路的FDR 以上即是整个ssGSEA算法的整体思路。 [作者：ingene 链接：https://www.jianshu.com/p/3467077ff60f]]]></content>
  </entry>
  <entry>
    <title><![CDATA[同源染色体和等位基因]]></title>
    <url>%2Fhexo%2F2018%2F11%2F08%2F2018-11-08_allele%2F</url>
    <content type="text"><![CDATA[1.同源染色体有丝分裂间期：染色质（呈松散状）自我复制形成两条完全一样的姐妹染色单体。 前期: 染色质高度螺旋化形成染色体。 “X”中有两条染色单体，一条染色体，两个DNA分子 当“X”分裂成“|”和“|”后，这时没有染色单体了（“|”不能称为一条染色单体，只有在“X”这个形态时才能说其中有两条染色单体） 染色单体的计算根据着丝点，一个着丝点有两个染色单体。 “|”没有染色单体，一条染色体，一个DNA分子。 末期：染色体解螺旋形成染色质。 减数分裂得到一套 单倍体（DNA仍然是双链） 同源染色体同源染色体是在二倍体生物细胞中，形态、结构基本相同的染色体，并在减数第一次分裂（参考减数分裂）的四分体时期中彼此联会（若是三倍体及其他奇数倍体生物细胞，联会时会发生紊乱），最后分开到不同的生殖细胞（即精子、卵细胞）的一对染色体，在这一对染色体其中的一个来自母方，另一个来自父方。人类有23对染色体，每一对染色体都对应有一对等位基因。 2.基因座在生物学与演化运算（evolutionary computation）中，基因座（英语：locus），也称为“基因位点”或“位点”，是指染色体上的固定位置，例如某个基因的所在。而基因座上的DNA序列可能有许多不同的变化，各种变化形式称为等位基因（allele）。基因座在基因组中的排列位置称为基因图谱（genetic map），基因作图（Gene mapping）则是测定基因座与特定性状关系的过程。 二倍体与多倍体细胞的某些染色体上，在同一基因座上有相同的等位基因，这类细胞称为纯合子/同型合子（homozygous）。若是相同基因座上含有不同的等位基因，则称作杂合子/异型合子（heterozygous）。 命名 ：一个典型的基因座可能写成“6p21.3”，解释如下 6 = 6号染色体 p = 短臂 21 = 2区, 1带 3 = 子带3 区、带和子带是从着丝粒向外朝端粒开始编号。 符号 解释 6 染色体编号。 p 染色体上的短臂（p）或长臂（q）位置。p来自法文petit (小)；q则是选定p的下一个字母，也有来自法文queue (尾巴)一说。 21.3 染色体臂上的所在位置，21表示2区（region）1带（band），3表示子带（sub-band），染色体带经过适当染色之后，可于显微镜底下观察。每个带皆有编号，最靠近着丝粒（centromere）的为1号。 3.等位基因在一个生物体里，某个基因的基因型是由该基因所拥有的一对等位基因所决定。例如在人和其他二倍体生物，也就是每条染色体都有两套的生物，其等位基因的两个位点决定了该基因的基因型。等位基因两个位点来自父辈和母辈的遗传，其基因型决定了生物的表现型。 生物的表现型由一对等位基因的一个位点决定的，称显性基因；而由两个位点决定的，则称为隐性基因。例如等位基因一个位点的突变，可产生癌基因，而两个位点的突变或丢失，则可导致肿瘤抑制基因，或抑癌基因的突变。这些基因的改变是肿瘤发生的分子基础。 4. 基因变异 Germline variant：与生俱来的与参考基因组不同的基因组位点； Somatic variant：后天环境中新产生的基因组变异位点；(肿瘤发生的) LoH variant : 杂合缺失变异 Allele frequency：变异位点等位基因频率]]></content>
  </entry>
  <entry>
    <title><![CDATA[GISTIC2.0]]></title>
    <url>%2Fhexo%2F2018%2F11%2F08%2F2018-11-08_GISTIC2%2F</url>
    <content type="text"><![CDATA[https://www.jianshu.com/p/eafa7e266806 http://www.bio-info-trainee.com/1648.html GISTIC2.0用以分析癌症拷贝数变异的一个数据处理模块。这个软件在TCGA计划里面被频繁使用者，用这个软件的目的很简单，就是你研究了很多癌症样本，通过芯片得到了每个样本的拷贝数变化信息，芯片结果一般是segment结果，可以解释为CNV区域，需要用GISTIC把样本综合起来分析，寻找somatic的CNV，并且注释基因信息。 Summary: The GISTIC module identifies regions of the genome that are significantly amplified or deleted across a set of samples. Each aberration is assigned a G-score that considers the amplitude of the aberration as well as the frequency of its occurrence across samples. False Discovery Rate q-values are then calculated for the aberrant regions, and regions with q-values below a user-defined threshold are considered significant. For each significant region, a “peak region” is identified, which is the part of the aberrant region with greatest amplitude and frequency of alteration. In addition, a “wide peak” is determined using a leave-one-out algorithm to allow for errors in the boundaries in a single sample. The “wide peak” boundaries are more robust for identifying the most likely gene targets in the region. Each significantly aberrant region is also tested to determine whether it results primarily from broad events (longer than half a chromosome arm), focal events, or significant levels of both. The GISTIC module reports the genomic locations and calculated q-values for the aberrant regions. It identifies the samples that exhibit each significant amplification or deletion, and it lists genes found in each “wide peak” region. References: Mermel C, Schumacher S, et al. (2011). “GISTIC2.0 facilitates sensitive and confident localization of the targets of focal somatic copy-number alteration in human cancers.” Genome Biology, 12:R41. Beroukhim R, Mermel C, et al. (2010). “The landscape of somatic copy -number alteration across human cancers.” Nature, 463:899-905. Beroukhim R, Getz G, et al. (2007). “Assessing the significance of chromosomal abberations in cancer: Methodology and application to glioma.” Proc Natl Acad Sci, 104:20007-20012. 输出数据解读all_data_by_genes.txt 代表了基因（包括非编码RNA如miRNA，lncRNA）在样本中具体的拷贝数值。 all_lesions.conf_90.txt 代表识别的拷贝数扩增和缺失Peak区域。 all_thresholded.by_genes.txt 代表离散化之后的数值，如-2代表丢失两个拷贝，-1代表丢失一个拷贝,0代表拷贝数正常,1代表增加一个拷贝，2代表扩增两个拷贝。 broad_significance_results.txt代表显著发生拷贝数变异的broad区域。 broad_values_by_arm.txt 代表染色体臂在样本中的拷贝数数值。 scores.gistic代表通过该方法打分之后的结果。]]></content>
  </entry>
  <entry>
    <title><![CDATA[DNA甲基化]]></title>
    <url>%2Fhexo%2F2018%2F11%2F07%2F2018-11-07_DNAmethylation%2F</url>
    <content type="text"><![CDATA[1. 甲基化DNA甲基化是科学家们最早发现的DNA修饰途径之一，即在甲基转移酶的催化下，DNA的CG两个核苷酸的胞嘧啶被选择性地添加甲基，形成5－甲基胞嘧啶。 DNA甲基化通常抑制基因表达,去甲基化则诱导了基因的重新活化和表达。 2. CpG岛— 不易被甲基化在哺乳动物中CpG以两种形式存在：一种是分散于DNA序列中；另一种呈现高度聚集状态，人们称之为CpG岛（CpG island）。 CpG双核苷酸在人类基因组中的分布很不均一，而在基因组的某些区段，CpG保持或高于正常概率。CpG岛主要位于基因的启动子（promotor）和外显子区域，是富含CpG二核苷酸的一些区域，长度为300—3000bp。这里CpG是胞嘧啶（C）—磷酸（p）—鸟嘌呤（G）的缩写。CpG岛常位于基因转录调控区附近，与56%的人类基因组编码基因相关，因此基因转录区CpG岛的甲基化状态的研究就显得十分重要。人类基因组序列草图分析结果表明，人类基因组CpG岛约为28890个，大部分染色体每1Mb就有5-15个CpG岛，平均值为每Mb含10.5个CpG岛，CpG岛的数目与基因密度有良好的对应关系。 CpG岛经常出现在真核生物的编码基因的调控区，在其它地方出现时会由于CpG中的C易被甲基化而形成5’-甲基胞嘧啶，脱氨基后形成胸腺嘧啶 T，由于T本身就会存在于DNA中，因此不易被修复，所以被淘汰。故CpG在基因组中是以岛的形式分布的。CpG岛，其中G在DNA链中紧随C后。在许多基因的启动子（promotor）或“起始”区域周围，甲基化经常被抑制。这些区域包含浓度相对较高的CpG对，与此段区域对应的染色体区段一起被称作CpG岛，其长度通常在几百到几千核苷酸的长度内变化。 在正常组织里，70%～90%散在的CpG是被甲基修饰的，而与之相反，大小为100-1000bp左右且富含CpG二核苷酸的CpG岛，则往往非甲基化的。在肿瘤发生时,抑癌基因CpG岛以外的CpG序列非甲基化程度增加,CpG岛中的CpG则呈高度甲基化状态,导致抑癌基因表达的下降。 当DNA的CpG岛处于高甲基化水平时，基因表达水平被抑制。当DNA的CpG岛处于低甲基化水平时，基因得以正常表达。 许多基因，尤其是管家基因的启动子区，其中通常存在一些富含双核苷酸“CG”的区域，称为“CpG岛”（CpG island）。研究碱基G和C在整个基因组内的含量和分布有十分重要的意义。例如在人类基因组内，GC的含量大约为40%；这些GC并不是平均分布在基因组内，在某些DNA片段上其含量可高达60%以上，而在另一些区域则只有33%左右。这种GC含量的差别，在基因表达的调控和基因突变上都可能扮演着重要的角色。在人类基因组内，存在有近3万个CpG岛；在大多数染色体上，平均每100万碱基含有5～15个CpG岛，其中有1.8万多个CpG岛片段的GC含量为60%～70%。通常，这些CpG岛不仅是基因的一种标志，而且还参与基因表达的调控和影响染色质的结构。例如， 除定位于失活X染色体上的基因、印迹基因和非表达的组织特异基因（奢侈基因）外，正常细胞的CpG岛由于被保护而处于非甲基化状态。 CpG岛总结 CpG岛主要位于基因的启动子区，少量位于基因的第一个外显子区；其甲基化状态直接影响基因表达。 甲基化的 CpG 双核苷酸通过募集转录抑制因子或者阻碍转录激活因子的结合抑制基因的表达。 CpG岛一般是非甲基化的，而在失活 X染色体、印记基因和非表达的组织特异基因中则是甲基化的。 成体基因组通常中，奢侈基因呈现高密度甲基化，而含有丰富 CpG 岛的管家基因则呈非甲基化 ================================================================================ Illumina HumanMethylation BeadChip简介Illumina最早的甲基化芯片是27K（K代表1000，表示大概可以测到的CpG位点数）的数据，后来增加到了450K（主流的甲基化芯片），而目前illumina已经出了新一代产品EPIC（我习惯称之为850K），但是技术核心在450K已经成熟了，所以分析流程这里以450K为主（也是目前数据库主流的甲基化芯片数据）。 芯片 一张芯片包括12个array，也就是一张芯片可以做12个sample，一台机子一次可以跑8张芯片，也就是一共96个sample，每个样本可以测到超过450，000个CpG位点的甲基化信息（大概人所有甲基化位点的1%，但是覆盖了多数CpG岛和启动子区），芯片本身包含一些控制探针可以做质控。 原理 简而言之，基于亚硫酸盐处理后的DNA序列杂交的信号探测。亚硫酸盐是甲基化探测的“金标准”，不管是芯片或者甲基化测序，都要先对DNA样品进行亚硫酸盐处理，使非甲基化的C变成U，而甲基化的C保持不变，从而在后续的测序或者杂交后区分出来。450K采用了两种探针对甲基化进行测定，Infinium I采用了两种bead（甲基化M和非甲基化U，如图显示），而II只有一种bead（即甲基化和非甲基化在一起），这也导致了它们在后续荧光探测的不同，450K采用了两种荧光探测信号（红光和绿光）。 DNA甲基化指的是包裹在DNA上CG碱基（又叫CpG）的外周的一些蛋白发生变化，进而导致基因或者一些调控因子的表达出现变化，进而导致那些基因或者调控因子控制的表型出现变化，进而导致疾病或者差异的发生。DNA甲基化被认为是表观遗传调控的一种方式，如Cytosine methylation (5-mC)是研究最多的，被认为是哺乳动物中常见的甲基化方式, 最近有一些研究也发现了其他形式的甲基化，如2016年Nature上发表了一篇关于鼠的胚胎干细胞的m6A（N6-methyladenine）形式的甲基化。DAN甲基化被认为对基因表达，染色质重塑，细胞分化，疾病等都有重要影响。 甲基化的检测方法 目前甲基化检测的方法可以概括为三种：芯片、测序、免疫沉淀。具体选择何种方法主要还是根据实验目的和实验室条件了。但目前来说，甲基化芯片技术从覆盖度，检测灵敏度和价格综合考虑，还是性价比相对高。 甲基化芯片常见的Glossary CpG island: Defned as regions 500 bp, 55% GC and expected/observed CpG ratio of 0.65. 40% of gene promoters contain islands. CpG shelves: ~4Kb from islands. CpG shores: ~2Kb from islands, 75% of tissuespecifc differentially methylated regions found in shores. Methylation in shores shows higher correlation with gene expression than CpG islands. Differentially methylated regions (DMR): Cell-, tissue-, and condition- specifc differences in methylation. Enhancer: A short region of DNA that can activate transcription and is often regulated by methylation. Hypermethylation: Most cytosines are methylated. Hypomethylation: Most cytosines do not have 5-mC. Euchromatin and active gene promoters are hypomethylated. Beta value:通常的甲基化衡量方法被称为“Beta”值; 等于甲基化百分比，并定义为“Meth”除以“Meth + Unmeth”。 CGI: CpG island 即甲基化岛。 分析需要考虑的问题 背景校正 红光和绿光的校正 控制芯片的使用（illumina450K本身有一些控制芯片，可以用来做质控，如亚硫酸盐处理效率） 探针类型（I型和II型）的校正（不同探针类型产生的数据不同）这个问题我们之前关注很多，这里附上两篇文献供大家参考，最终我们选择BMIQ的方法（基于ebayes的原理将II型探针的甲基化水平拉伸到I型水平，如下图显示）来做矫正。(图片来源于第1篇文献)文献1： http://bioinformatics.oxfordjournals.org/content/29/2/189.short文献2： http://www.tandfonline.com/doi/abs/10.4161/epi.24008 位置的校正（芯片上的不同位置产生的数据可能会有偏差） 批次的校正（不同的批次做的数据会有偏差） 探针序列本身是否可靠（有些探针本身位于repeat区或者包含snp等就会影响杂交及最后的结果，应该去除，附上一片参考文献，里边有list可以用来去除不好的探针）文献：http://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-15-51 数据处理 GenomeStudioGS是illumina开发的软件，基于图形界面的操作处理，适合于没有R及编程基础的人使用，但是使用GS需要权限，本人试过挺难破解的，如果有人可以破解，可以提供给论坛小伙伴使用。 基于R和bioconductor的pipelinebioconductor里开发了很多package供大家使用，如果你会R，那么处理这个甲基化芯片的数据将变得简单。可以处理450K芯片的package有lumi、minfi、wateRmelon、ChAMP等，没有哪一种就特别好，大家都在不断改进，所以只要你知道大概的流程和需要注意的问题，那么你也可以自己写代码处理，只是package可以帮你省很多事情。下边我会附上我的处理流程图和详解，还有我的代码，我的代码是基于minfi的，我再次强调，代码只是手段，你可以用其他的package（例如我的同事很多使用ChAMP），也许会更好。 流程图详解： 蓝色部分代表的是有关于数据的准备部分 红色部分代表的是数据分析部分 黄色部门代表数据可视化部分 箭头指示了分析流程，首先是load数据，然后是QC（quality control），然后是normalization，然后是SVD分析（看有没有batch effect）。准备完毕之后，黑点代表了一批质量有保证，经过处理，可以直接上马进行数据分析的数据了。之后的分析，DMP代表找出Differential Methylation Probe（差异化CpG位点），DMR代表找出Differential Methylation Region（差异化CpG区域），Block代表Differential Methylation Block（更大范围的差异化region区域），RefFree代表细胞差异被修正过后再找的DMP，EpiMod是基于基因作用网络的差异化分析。 我们一般下载或者iscan后的原始数据格式为Idat，首先可以得到每个sample的每个probe的p值和bead数，根据p值和bead数可以进行样本和探针的过滤，过滤之后需要用BMIQ的方法进行I和II型探针的校正，矫正之后去掉那些包含snp之类的不好的探针，最后对数据做batch的校正。校正之后的数据就是预处理后的数据了，可以用于更下游的分析，如差异甲基化和甲基化与表达的关联分析等。我的代码： https://github.com/wkl1990/illumina-450K-analysis 。 数据分析 Illumina HumanMethylation450 BeadChip甲基化450k芯片预处理初探 850K甲基化芯片数据的分析 Bioconductor ChAMP包 Shiny和Plotly实现可交互DNA甲基化分析包ChAMP]]></content>
  </entry>
  <entry>
    <title><![CDATA[染色质 DNaseⅠ超敏感位点]]></title>
    <url>%2Fhexo%2F2018%2F11%2F07%2F2018-11-07_DNaseI%2F</url>
    <content type="text"><![CDATA[1. 染色质是什么 2. 染色质 DNaseⅠ超敏感位点（(DNaseⅠ hypersensitive sites，DHSs)）基因活化时其染色质一般呈开放的疏松型构象，更易被 DNaseⅠ降解，形成 DNaseⅠ超敏感位点。当一个基因处于转录活性状态时，含有这个基因的染色质区域对DNase（一种内切块酶）降解的敏感性要比无转录活性区域高得多。仔细分析具有转录活性基因周围的DNA区域，表明有一个中心区域存在，称为超敏感区域（hypersensitive region）或超敏感位点（hypersensitive site），它对DNaseⅠ是高敏感的。这些位点或区域将首先受到DNaseⅠ的剪切。 ENCODE 计划研究确定 2890742 个高可信度 DHSs中，约 3% (75 575 个)的 DHSs 分布在 TSS 区，5%(135735 个)位于 TSS 区 2． 5 kb 内，95% DHSs 距离 TSS 区较远，称其为远端 DHSs。 中文文章： 染色质 DNaseⅠ超敏感位点的定位及其转录调控功能的研究进展 2016.9.281. DNase I足迹法http://blog.sina.com.cn/s/blog_550f64320100hf0t.html paper: 染色质DNaseⅠ超敏感位点的定位及其转录调控功能的研究进展 2016.9.29 miRNA 主要是通过 5′端被称为种子序列(Seed Sequence)的 7 nt 序列与位于靶 mRNA 3′UTR 的miRNA 调控元件(miRNA Regulatory Element, MRE)相互作用, 识别靶mRNA[11] . paper: MicroRNA 作用机制研究的新进展 ==转染(transfection)== 指真核细胞由于外源DNA掺入而获得新的遗传标志的过程。DNA转染技术的发展对现代分子生物学产生了巨大的影响。 2016.9.30 CpG岛甲基化表型 (CpG island methylator phenotype,CIMP)涉及到多个基因启动子同时甲基化,具有肿瘤特异性,与多种肿瘤的发生或预后相关,但有关肝癌CPG岛甲基化表型的研究罕见报道。 DNA甲基化是表观遗传修饰的主要机制，其不改变DNA序列，但影响基因转录调控，是导致抑癌基因失活的重要原因[2]。CpG岛甲基化表型(CIMP)的概念于1999年首次被提出，研究发现某些结直肠癌CpG岛位点启动子被广泛甲基化，造成某些抑癌基因和其他一些肿瘤相关基因的失活[3]。这些CIMP被发现有不同临床表现和分子特征，可以用于研究和评估CIMP相关肿瘤[4]。肺癌起源于一系列遗传和表观遗传的变化，也是CIMP相关肿瘤之一[5]。目前肺癌CIMP的相关报道较少，究竟哪些甲基化基因可以作为肺癌CIMP分型的基因标记尚不清楚。 CpG位点（英语：CpG sites，或称为CG位点）是指DNA的某个区域，其上的碱基序列以胞嘧啶接着鸟嘌呤出现。==“CpG”是“—C—磷酸—G—”的缩写== ，指磷酸二酯键连接了胞嘧啶和鸟嘌呤，其中C位于5’端而G位于3’端。在CpG位点中的胞嘧啶可以被甲基化为5-甲基胞嘧啶。在哺乳动物中，基因内CpG位点的甲基化会改变此基因的表达，对这一表达调控的研究是表观遗传学的重要组成部分。涉及添加甲基基团的酶称为DNA甲基转移酶。 【CpG岛甲基化表型】在正常组织里，70%～90%散在的CpG是被甲基修饰的，而CpG岛则是非甲基化的。正常情况下，人类基因组“垃圾”序列的CpG二核苷酸相对稀少，并且总是处于甲基化状态，与之相反，人类基因组中大小为100-1000bp左右，富含CpG二核苷酸的CpG岛则总是处于未甲基化状态。当CpG岛出现甲基化表型，涉及到多个基因启动子同时甲基化，具有肿瘤特异性，与多种肿瘤的发生或预后相关。 【CpG岛(CpG island)】：CpG双核苷酸在人类基因组中的分布很不均一，而在基因组的某些区段，CpG保持或高于正常概率，这些区段被称作CpG岛，在哺乳动物基因组中的1~2kb的DNA片段，它富含非甲基化的CpG双倍体。CpG岛主要位于基因的启动子（promotor）和第一外显子区域，约有60%以上基因的启动子含有CpG岛. 比较基因组杂交（ ==aCGH== ：==array-based Comparative Genomic Hybridization==）是通过在一张芯片上用标记不同荧光素的样品（病例样品和对照样品）进行共杂交可检测样本基因组相对于对照基因组的DNA拷贝数变化（CNV），常用于肿瘤或遗传性疾病全基因组CNV检测，直观地表现出肿瘤及遗传性疾病基因组DNA在整个染色体组的缺失或扩增。对肿瘤而言缺失片段可能包含抑癌基因，而扩增片段则可能存在致癌基因。[1] 2017.3.21在锐博网页上有关于各种高通量测序的技术介绍，有空好好看看，做点笔记，就能对这些测序技术有个大概全面的了解。http://www.ribobio.com/sitecn/service10_411.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[CNV学习]]></title>
    <url>%2Fhexo%2F2018%2F11%2F01%2F2018-11-01_CNV%2F</url>
    <content type="text"><![CDATA[1. Germline和Somatic 的区别对于DNA测序而言，主要识别SNP和CNV 两大类型的变异，每种变异类型又有Germline和Somatic的区别。 Germline指的是在胚胎发育早起出现的变异，这种变异会在所有细胞中广泛存在，是可以遗传给后代的变异；Somatic指的是体细胞变异，身体特定区域或者组织中出现的变异。通常不会遗传给后代。 12作者：庐州月光链接：https://www.jianshu.com/p/825e7d618838 2. TCGA SNP 数据1、 明白什么是CNVCNV是相对参考基因组而言的概念。对正常人来说，基因组应该是二倍体的，所以凡是测到非2倍体的地方都是CNV。但是CNV本身就是人群遗传物质多样性的体现，所以对癌症样本来说，是需要过滤掉正常人体内的germline的CNV，得到somatic的CNV。 CNV（copy-number variant）是指拷贝数目变异，也称拷贝数目多态性（copy-number polymorphism，CNP），是一个大小介于1kb至3MB的DNA片段的变异，在人类及动植物基因组中广泛分布，其覆盖的核苷酸总数大大超过单核苷酸多态性（SNP）的总数，极大地丰富了基因组遗传变异的多样性。按照CNV是否致病可分为致病性CNV、非致病性CNV和不明临床意义CNV。 2、TCGA的CNV测量及计算 TCGA里面主要是通过Affymetrix SNP6.0 array这款芯片来测拷贝数变异。值得注意的是，并不是只有TCGA利用了SNP6.这个芯片数据，著名的CCLE计划也对一千多细胞系处理了SNP6.0芯片，数据也是可以下载的。 对SNP6.0的拷贝数芯片来说，通常是用PICNIC等软件处理原始数据，就可以得到的segment记录文件，每个样本一个结果，下面是示例结果： Chromosome Start End Num_Probes Segment_Mean1 6173515108012 26 -0.039711 627918 167260317-0.9211 6875871615349781760.007711 6153536161539255-2.744111 6154201161550104-0.871111 616566172768498346300.004817 27689167281114846-1.739417 281190495674710149010.002619 5676511956765182-1.6636 表明了某条染色体的某个区域内，SNP6.0芯片设计了多少个探针，芯片结果的拷贝数值是多少(这个区域的拷贝数用Segment_Mean)。 通常二倍体的Segment_Mean值为0，可以用-0.2和0.2来作为该区域是否缺失或者扩增。 具体数据处理流程见NIH的TCGA官网： https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/CNV_Pipeline/ 参考文献：http://mcr.aacrjournals.org/content/12/4/485.long 3、 TCGA的CNV数据下载 众所周知，TCGA的数据的开放程度分成了4个等级，一般人都是下载level 3 的数据，对CNV数据也是如此。 我比较喜欢去broad institute下载TCGA的数据，所有的文件都以目录的形式存放着： https://gdac.broadinstitute.org/runs/stddata__latest/ 如果要下载level3的数据，就用stddata__latest 这个url即可，打开可以看到里面列出了所有的癌症种类，假如我们感兴趣的是BRCA，就直接点击进入，用下面的url即可。 https://gdac.broadinstitute.org/runs/analyses__latest/data/BRCA/20160128/ 打开url可以看到非常多的文件，这里我们感兴趣的是snp6芯片的拷贝数结果，而且一般是基于hg19版本的。 1wget -c -r -np -nH -k -L --cut-dirs 6 -p -A "*snp_6*hg19*Level_3*" http://gdac.broadinstitute.org/runs/stddata__2016_01_28/data/BRCA/20160128/ 如果要下载其它癌症种类，只需要改变url里面的BRCA即可。 如果要下载其它类型的数据，只需要改变-A 后面的匹配规则即可，其实就是打开上面url看到的几十个文件的文件名的规律。 “snp_6hg19Level_3“ 几分钟就下载完数据啦，然后你就会看到下面两个截然不同的： 12Merge_snp__genome_wide_snp_6__broad_mit_edu__Level_3__segmented_scna_hg19__seg Merge_snp__genome_wide_snp_6__broad_mit_edu__Level_3__segmented_scna_minus_germline_cnv_hg19__seg 其中minus了germline的CNV的就是我们想要的癌症相关的somatic CNV咯!]]></content>
  </entry>
  <entry>
    <title><![CDATA[陈巍学基因]]></title>
    <url>%2Fhexo%2F2018%2F08%2F31%2F2018-08-30_chenwei_gene%2F</url>
    <content type="text"><![CDATA[1、视频61、【视频61：亲子鉴定】 60、【视频60：10X genomics 分析基因组结构变异】 59、【视频59：Nanopore测序】 58、【视频58：基因与疾病的关系】 57、【视频57：基因是什么？】 56、【视频56:10X genomics 分析单细胞表达】 55、【视频55：明码云计算服务】 54、【视频54：TruSeq RNA Access建库】 53、【视频53：POLG基因相关疾病】 52、【视频52：Citrin蛋白缺乏症】 51、【视频51：G6PD缺乏症】 50、【视频50：地中海贫血】 49、【视频49：用MLPA方法测基因拷贝数变异】 47、【视频47：苯丙酮尿症和相关基因】 46、【视频46：帕金森病相关基因】 45、【视频45：老年痴呆症基因】 44、【视频44：彩色视觉】 43、【视频43：耳聋基因】 42、【视频42：大脑学习】 41、【视频41：PI3K/AKT信号通路与肿瘤】 40、【视频40：HER基因与肿瘤】 39、【视频39：ALK基因与肿瘤】 38、【视频38：BRAF基因与黑色素瘤】 37、【视频37：P53基因与肿瘤】 36、【视频36：KRAS与肿瘤靶向治疗】 35、【视频35：EGFR基因靶向治疗】 34、【视频34：判断基因变异的致病性】 33、【视频33：用血小板RNA-seq测肿瘤】 32、【视频32：用CRISPR寻找新肿瘤药】 31、【视频31：Tagrisso的液体活检伴随诊断】 30、【视频30：CellSearch检测CTC】 29、【视频29：Agilent生物芯片原理】 28、【视频28：Affymetrix芯片原理】 27、【视频27：Illumina的SNP芯片原理】 26、【视频26：循环肿瘤DNA测序】 25、【视频25：第一代DNA测序】 24、【视频24：GCBI 生物云平台】 23、【视频23：SureSelect 定制靶向测序】 21、【视频21：美貌的遗传本质】 20、【视频20：肺癌融合基因检测】 19、【视频19：NanoString】 18、【视频18：抽血测EGFR突变】 17、【视频17：数字PCR】 16、【视频16：BRCA基因检测】 15、【视频15：RiboZero和方向性RNA文库】 14、【视频14：Moleculo长测序】 13、【视频13：Ion Torrent测序】 12、【视频12：甲基化测序】 11、【视频11：单细胞mRNA测序】 10、【视频10：单细胞DNA测序】 09、【视频9：small RNA 测序】 08、【视频8：外显子测序】 07、【视频7：RNA-seq方法和应用】 06、【视频6：人全基因组测序】 05、【视频5：无创产前检测】 04、【视频4：基因投错票，导致肿瘤】 03、【视频3：PacBio单分子超长测序】 02、【视频2：HiSeq测序仪工作原理】 01、【视频1：Illumina测序化学原理】 Nanopore、【Nanopore测序原理】 CSA、【视频：NextCODE公司的CSA遗传病分析软件】 CRISPR、【视频：CRISPR/CAS 9 活细胞基因编辑】 AKT、【视频：PI3K/AKT信号通路】 PD-1、【视频：PD-1/PD-L1抗体治疗肿瘤】 AI、【神经网络与深度学习】 EAR、【人耳听觉工作原理】 HEART、【心血管疾病】 A00、【人工智能识别图像工作原理】 A01、【如何下载并读取一个基因组】 A02、【操作测序read】 A03、【按碱基位置分析GC含量】 A04、【简单序列比对】 A05、【简单序列比较法的效率】 A06、【基本Boyer-Moore算法】 A07、【应用Boyer-Moore二规则】 A08、【为什么要预处理参考基因组】 A09、【把参考基因组做成索引】 A10、【索引的排序结构】 A11、【实现 Boyer-Moore 算法】 A12、【实现k-mer索引算法】 A13、【近似匹配、汉明距离、编辑距离】 A14、【把索引做成哈希表】 A15、【用鸽子洞原理找近似匹配】 A16、【近似匹配的编辑距离】 A17、【实现鸽子洞规则】 A18、【比对真实的测序序列】 A19、【转移—重复元素】 A20、【递归法计算编辑距离】 A21、【用动态规划来算编辑距离】 A22、【实现动态规划算编辑距离】 A23、【全局比对和局部比对】 A24、【实现全局比对】 A25、【在实践中应用基因序列比对算法】 A26、【De Novo基因组组装序言】 A27、【组装基因组的第一、和第二定律】 A28、【计算两条序列交叠的代码】 A29、【找多个序列的重叠区】 A30、【交叠的有向图】 A31、【最短共同字符串】 A32、【实现最短共同字符串】 A33、【贪心最短共同超级字符串算法】 A34、【实现贪心最短共同超级字符串算法】 A35、【重复序列让组装基因组变得困难】 A36、【De Bruijn图和欧拉行走】 A37、【欧拉路径也可能出错】 A38、【实现De Bruijn图的代码】 A39、【在实践中组装基因组】 A40、【未来组装基因组将依赖于更长的测序读长】 A41、【K-mer Index的变种】 A42、【基因组研究中用到的索引】 A43、【Burrows-Wheeler 转换】 A44、【FM 索引】 A45、【用后缀做索引】 B01、【测序序列比对和查找突变（一）】 B02、【基本Linux命令】 B03、【测序序列比对和查找突变（二）】 C01、【Sentieon—比GATK更好的找突变软件】 C02、【把自制的软件放到DNAnexus云上】 C03、【在DNAnexus上建流程、跑分析】 C04、【在DNAnexus上进行协作、和分享】 C05、【如何向DNAnexus上传数据并存储】 C06、【对DNAnexus中的组织账户的简介】 C07、【如何在DNAnexus上管理组织项目】 C08、【如何在DNAnexus上管理成员】 C09、【如何在DNAnexus上删除成员】 C10、【如何在DNAnexus上设立新成员】 D01、【2015 GATK讲座 高通量测序的文件格式简介】 D02、【2015 GATK讲座 插入缺失变异的重新比对】 D03、【2015 GATK讲座 碱基质量的重新校正】 D04、【2015 GATK讲座 基因变异的注释和评价】 48、【视频48：制作视频入门】 E01、【视频：Bionano基因组分子图谱技术】 2、测序原理、方法：原理、《第一、二、三代测序原理视频》 问答、《高通量测序常见问题及回答》 比较、《所有测序仪的价格、费用、性能比较》 建库、《汇总高通量测序的特殊应用、和相应的建库方案》 超长、《10Kb 长测序 —Moleculo专利原理》 万一、《检出1/10,000突变比例的测序方法》 千元、 《HiSeq X 10, 千元基因组测序》 文库、《Illumina的全套建库方法图解资料》 太平洋、《PacBio单分子超长测序》 筛药、《用带DNA分子标签的组合化合物库来筛新药》 Xten、《X10测序常见问题及回答》 RPA、《RPA技术让HiSeq X 10打败泊松分布》 V4、《HiSeq V4 PE125 测序有些什么新应用？》 化学、《Illumina测序化学方法》 3、商业服务产业、《基因产业发展之路》 清单、《中国高通量测序、生物信息服务公司清单》 品牌、《高通量测序试剂品牌》 上下、《高通量测序的上下游服务商》 数据、《ICGC提供一万个肿瘤基因组数据》 CRO、《中国CRO企业名录》 一万、《人全基因组测序降到一万元会带来什么改变》 4、诊断、检测：FDA、《FDA批准的高通量基因分析诊断方案》 拯救、《二代测序拯救一个年轻的生命》 HLA、《FDA许可Biofortuna的冻干HLA分型试剂》 ROC、《ROC分析，评价临床检验方法的客观标准》 检测、《最强悍的感染检测手段—高通量测序—正飞驰而来》 代谢一、《个体化用药中与代谢相关的基因（一）》 代谢二、《个体化用药中与代谢相关的基因（二）》 5、测序之外的分子生物学方法荧光、 《视频：三种有趣的荧光定量PCR方法》 数字、《NanoString—全新的高通量数字表达谱》 定量、 《4个品牌的数字定量PCR原理》 SNP、《SNP检测方法汇总》 化学、《Illumina测序中的化学方法》 《LNA锁核酸技术》 《PNA肽键核苷技术》 《BigDye测序的化学》 多重、《高通量测序中的多重PCR方法》 CNV、《检测CNV的方法汇总》 甲基、《DNA甲基化分析方法汇总》 6、肿瘤及相关知识标记、 《高通量测序发现肿瘤核酸生物标记物的新方法》 靶向、 《13种肿瘤抗体靶向药物》 五十、《50个核心肿瘤基因》 裸鼠、《用裸鼠选择肿瘤药物——癌症靶向治疗新方案》 7、遗传疾病基因、《高通量测序分析单基因遗传病标准方法》 多囊、《多囊肾疾病的相关基因》 马凡、《马凡氏综合征的相关致病基因：FBN1》 甲低、《先天性甲状腺功能低下的相关基因》 ALS、《渐冻症（ALS）的主要致病基因：SOD1》 老年、《阿茨海默病的相关基因》 异常、《差了一个条性染色体的女人、和男人》 蚕豆、《蚕豆病的致病基因：G-6-PD》]]></content>
  </entry>
  <entry>
    <title><![CDATA[奇异值分解SVD——python]]></title>
    <url>%2Fhexo%2F2018%2F08%2F29%2F2018-08-29_SVD_python%2F</url>
    <content type="text"><![CDATA[原文 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110"""@author: 蔚蓝的天空TomTalk is cheap, show me the codeAim:svd分解降维应用示例的代码实现CSDN URL:https://mp.csdn.net/postedit/80450590""" import numpy as npfrom numpy import linalg as LA class CSVD(object): ''' 实现SVD分解降维应用示例的数学求解过程的Python代码 ''' def __init__(self, data): self.data = data #用户数据 self.S = [] #用户数据矩阵的奇异值序列 singular values self.U = [] #svd后的单位正交向量 self.VT = [] #svd后的单位正交向量 self.k = 0 #满足self.p的最小k值(k表示奇异值的个数) self.SD = [] #对角矩阵，对角线上元素是奇异值 singular values diagonal matrix #svd奇异值分解 self._svd() def _svd(self): ''' 用户数据矩阵的svd奇异值分解 ''' u,s,v = np.linalg.svd(self.data) (self.U, self.S, self.VT) = (u, s, v) return self.U, self.S, self.VT def _calc_k(self, percentge): '''确定k值：前k个奇异值的平方和占比 &gt;=percentage, 求满足此条件的最小k值 :param percentage, 奇异值平方和的占比的阈值 :return 满足阈值percentage的最小k值 ''' self.k = 0 #用户数据矩阵的奇异值序列的平方和 total = sum(np.square(self.S)) svss = 0 #奇异值平方和 singular values square sum for i in range(np.shape(self.S)[0]): svss += np.square(self.S[i]) if (svss/total) &gt;= percentge: self.k = i+1 break return self.k def _buildSD(self, k): '''构建由奇异值组成的对角矩阵 :param k,根据奇异值开放和的占比阈值计算出来的k值 :return 由k个前奇异值组成的对角矩阵 ''' #方法1：用数组乘方法 self.SD = np.eye(self.k) * self.S[:self.k] #方法2：用自定义方法 e = np.eye(self.k) for i in range(self.k): e[i,i] = self.S[i] return self.SD def DimReduce(self, percentage): ''' SVD降维 :param percentage, 奇异值开方和的占比阈值 :return 降维后的用户数据矩阵 ''' #计算k值 self._calc_k(percentage) print('\n按照奇异值开方和占比阈值percentage=%d, 求得降维的k=%d'%(percentage, self.k)) #构建由奇异值组成的对角矩阵singular values diagonal self._buildSD(self.k) k,U,SD,VT = self.k,self.U, self.SD, self.VT #按照svd分解公式对用户数据矩阵进行降维，得到降维压缩后的数据矩阵 print('\n降维前的U,S,VT依次为:') print(np.shape(U), 'U:\n', U) print(np.shape(self.S), 'S:\n', self.S) print(np.shape(VT), 'VT:\n', VT) print('\n降维后的U,SD,VT依次为:') print(np.shape(U[:len(U),k]), 'U=U[:%d,:%d]:\n'%(len(U),k), U[:len(U), :k]) print(np.shape(SD), 'SD=SD[:%d, :%d]:\n'%(k,k), SD[:k, :k]) print(np.shape(VT[:k, :len(VT)]), 'VT=VT[:%d, :%d]:\n'%(k, len(VT)), VT[:k, :len(VT)]) a = U[:len(U), :k] b = np.dot(SD, VT[:k, :len(VT)]) newData = np.dot(a,b) return newData def CSVD_manual(): ##训练数据集，用户对商品的评分矩阵，行为多个用户对单个商品的评分，列为用户对每个商品的评分 data = np.array([[5, 5, 0, 5], [5, 0, 3, 4], [3, 4, 0, 3], [0, 0, 5, 3], [5, 4, 4, 5], [5, 4, 5, 5]]) percentage = 0.9 svdor = CSVD(data) ret = svdor.DimReduce(percentage) print('====================================================') print('原始用户数据矩阵:\n', data) print('降维后的数据矩阵:\n', ret) print('====================================================') if __name__=='__main__': CSVD_manual()]]></content>
  </entry>
  <entry>
    <title><![CDATA[无监督特征选择方法]]></title>
    <url>%2Fhexo%2F2018%2F08%2F28%2F2018-08-28_unsupervised_featureselection%2F</url>
    <content type="text"><![CDATA[1. Laplacian ScoreLaplacian Score (LSCORE) is an unsupervised/supervised linear feature extraction method. For each feature/variable, it computes Laplacian score based on an observation that data from the same class are often close to each other. Its power of locality preserving property is used, and the algorithm selects variables with largest scores. Rdimtools 12Rdimtools：do.lscore(X, ndim = 2, type = c("proportion", 0.1), preprocess = c("null","center", "scale", "cscale", "whiten", "decorrelate"), t = 10) 2.Package ‘IDmining’ Intrinsic Dimension for Data Mining https://cran.r-project.org/web/packages/IDmining/IDmining.pdf 3.Unsupervised Learning in Rhttps://rpubs.com/williamsurles/310847 4. python包scikit-featurehttp://featureselection.asu.edu/index.php 5.Unsupervised dimensionality reduction in sklearnhttp://scikit-learn.org/stable/modules/unsupervised_reduction.html 6. 7 Machine Learning techniques for Dimensionality Reduction]]></content>
  </entry>
  <entry>
    <title><![CDATA[特征选择方法]]></title>
    <url>%2Fhexo%2F2018%2F08%2F28%2F2018-08-28_feature_selection%2F</url>
    <content type="text"><![CDATA[有监督特征选择方法1. 卡方检验 Chi-square卡方检验是一种用途很广的计数资料的假设检验方法。它属于非参数检验的范畴，主要是比较两个及两个以上样本率( 构成比）以及分类变量的关联性分析。其根本思想就是在于比较理论频数和实际频数的吻合程度或拟合优度问题。 img 其中，A为实际值，T为理论值。 x2用于衡量实际值与理论值的差异程度（也就是卡方检验的核心思想），包含了以下两个信息： 1.实际值与理论值偏差的绝对大小（由于平方的存在，差异是被放大的） 2. 差异程度与理论值的相对大小 12345678910111213## python from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2from sklearn.datasets import load_iris#导入IRIS数据集iris = load_iris()iris.data#查看数据model1 = SelectKBest(chi2, k=2)#选择k个最佳特征model1.fit_transform(iris.data, iris.target)#iris.data是特征数据，iris.target是标签数据，该函数可以选择出k个特征 model1.scores_ #得分model1.pvalues_ #p-values 2. T-test]]></content>
  </entry>
  <entry>
    <title><![CDATA[R语言机器学习包]]></title>
    <url>%2Fhexo%2F2018%2F08%2F27%2F2018-08-27_machinelearing_rpackages%2F</url>
    <content type="text"><![CDATA[mlr包 CARET DMwR]]></content>
  </entry>
  <entry>
    <title><![CDATA[Entropy、 交叉熵、相对熵、巴氏距离（Bhattacharyya distance）]]></title>
    <url>%2Fhexo%2F2018%2F08%2F26%2F2018-08-26_Entropy%2F</url>
    <content type="text"><![CDATA[原文1 原文2 原文3 1. 信息量(自信息)定义：假设X是一个离散型随机变量，其取值集合为χ，概率分布函数为p(x)=P(X=x),x∈χ,我们定义事件X=x0的自信息为： I(x0)=−log(p(x0))一个事件发生的概率越大，则它所携带的信息量就越小，而当p(x0)=1时，熵将等于0，也就是说该事件的发生包含的信息量小。 举个例子，小明平时不爱学习，考试经常不及格，而小王是个勤奋学习的好学生，经常得满分，所以我们可以做如下假设：事件A：小明考试及格，对应的概率P(xA)=0.1，信息量为I(xA)=−log(0.1)=3.3219.事件B：小王考试及格，对应的概率P(xB)=0.999，信息量为I(xB)=−log(0.999)=0.0014 可以看出，结果非常符合直观：小明及格的可能性很低(十次考试只有一次及格)，因此如果某次考试及格了（大家都会说：XXX竟然及格了！），必然会引入较大的信息量，对应的I值也较高。而对于小王而言，考试及格是大概率事件，在事件B发生前，大家普遍认为事件B的发生几乎是确定的，因此当某次考试小王及格这个事件发生时并不会引入太多的信息量，相应的I值也非常的低。 2. 信息熵（Entropy）定义：对于一个随机变量XX而言，它的所有可能取值的信息量的期望\(（E[I(x)] \)就称为熵。当XX是离散的： H(X)=E[I(x)]=−∑_{x∈X}p(x)logp(x)当X是连续的随机变量：熵定义为： H(X)=−∫_{x∈X}p(x)logp(x)dx自信息只能处理单个的输出。而熵是对整个概率分布中不确定性总量进行量化。 约定：p=0时，定义0log0=0。通常对数以2为底或者e为底，这是熵的单位称作比特（bit）或者纳特（nat）当随机变量只取两个值的时候，即X分布为：P(X=1)=p，P(X=0)=1−p，0&lt;=p&lt;=1。 img 例子假设小明的考试结果是一个0-1分布X只有两个取值{0：不及格，1：及格}，在某次考试结果公布前，小明的考试结果有多大的不确定度呢？你肯定会说：十有八九不及格！因为根据先验知识，小明及格的概率仅有0.1,90%的可能都是不及格的。怎么来度量这个不确定度？求期望！不错，我们对所有可能结果带来的额外信息量求取均值（期望），其结果不就能够衡量出小明考试成绩的不确定度了吗。即：HA(x)=−[p(xA)log(p(xA))+(1−p(xA))log(1−p(xA))]=0.4690对应小王的熵：HB(x)=−[p(xB)log(p(xB))+(1−p(xB))log(1−p(xB))]=0.0114虽然小明考试结果的不确定性较低，毕竟十次有9次都不及格，但是也比不上小王（1000次考试只有一次才可能不及格，结果相当的确定）我们再假设一个成绩相对普通的学生小东，他及格的概率是P(xC)=0.5,即及格与否的概率是一样的，对应的熵：HC(x)=−[p(xC)log(p(xC))+(1−p(xC))log(1−p(xC))]=1其熵为1，他的不确定性比前边两位同学要高很多，在成绩公布之前，很难准确猜测出他的考试结果。可以看出，熵其实是信息量的期望值，它是一个随机变量的不确定性的度量。熵越大，随机变量的不确定性越大。 3. 条件熵X给定条件下Y的条件分布的熵对X的数学期望，在机器学习中为选定某个特征后的熵，公式如下： img 一个特征对应着多个类别Y，因此在此的多个分类即为X的取值x。 或者 Info_A(D) = \sum\limits_{j=1}^v\frac{|D_j|}{|D|}Info(D_j)Info_A(D) = \sum\limits_{j=1}^v\frac{|D_j|}{|D|}Info(D_j)4. 信息增益信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好，在概率中定义为：待分类的集合的熵和选定某个特征的条件熵之差（这里只的是经验熵或经验条件熵，由于真正的熵并不知道，是根据样本计算出来的），公式如下： img Gain(A) = Info(D)-Info_A(D)5. 信息增益比信息增益的一个大问题就是偏向选择特征值比较多的属性从而导致overfitting，那么我们能想到的解决办法自然就是对分支过多的情况进行惩罚(penalty)了。于是我们有了信息增益比。 6. Gini系数Gini系数是一种与信息熵类似的做特征选择的方式，可以用来数据的不纯度。在CART(Classification and Regression Tree)算法中利用基尼指数构造二叉决策树（选择基尼系数最小的特征及其对应的特征值）。Gini系数的计算方式如下： 其中，D表示数据集全体样本，表示每种类别出现的概率。取个极端情况，如果数据集中所有的样本都为同一类，那么有，，显然此时数据的不纯度最低。 Mutual Information(互信息)(MI)，Normalized Mutual Information (标准互信息)(NMI)互信息（Mutual Information）是用来衡量两个数据分布的吻合程度，是指两个事件集合之间的相关性。 特征选择：用互信息的方法，在某个类别C中的出现概率高，而在其它类别中的出现概率低的词条T，将获得较高的词条和类别互信息，也就可能被选取为类别C的特征。互信息是term的存在与否能给类别c的正确判断带来的信息量。词条和类别的互信息体现了词条和类别的相关程度，互信息越大，词条和类别的相关程度也越大。得到词条和类别之间的相关程度后，选取一定比例的，排名靠前的词条作为最能代表此种类别的特征。 例子：假设对于17个样本点(v1,v2,…,v17)进行聚类img 比如标准结果是图中的叉叉点点圈圈，我的聚类结果是图中标注的三个圈。 或者我的结果: A = [1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3]; 标准的结果 : B = [1 2 1 1 1 1 1 2 2 2 2 3 1 1 3 3 3]; 问题：衡量我的结果和标准结果有多大的区别，若我的结果和他的差不多，结果应该为1，若我做出来的结果很差，结果应趋近于0。 MI(X,Y)=\sum_{i=1}^{|X|}\sum_{j=1}^{|Y|}P(i,j)log(\frac{P(i,j)}{P(i)P^{'}(j)})首先计算上式分子中联合概率分布 \( P(i,j)=\frac{|X_i\cap Y_j|}{N} \) X=unique(A)=[1 2 3]，Y=unique(B)=[1 2 3]; 分子p(x,y)为x和y的联合分布概率， p(1,1)=5/17, p(1,2)=1/17, p(1,3)=0; p(2,1)=1/17, p(2,2)=4/17, p(2,3)=1/17; p(3,1)=2/17, p(3,2)=0, p(3,3)=3/17; 分母p(x)为x的概率函数，p(y)为y的概率函数，x和y分别来自于A和B中的分布，所以即使x=y时，p(x)和p(y)也可能是不一样的。 对p(x)： p(1)=6/17 p(2)=6/17 p(3)=5/17 对p(y)： p(1)=8/17 p(2)=5/17 P(3)=4/17 这样就可以算出MI值了。 标准化互信息 标准化互聚类信息都是用熵做分母将MI值调整到0与1之间。 NMI(X,Y)=\frac{2MI(X,Y)}{H(X)+H(Y)}H(X)和H(Y)分别为X和Y的熵，下面的公式中log的底b=2。 img 例如H(X) = -p(1)log2(p(1)) - -p(2)log2(p(2)) -p(3)*log2(p(3))。 相对熵—KL（Kullback-Leibler divergence）散度描述两个概率分布P和Q差异的一种方法,在信息论中，D(P||Q)表示当用概率分布Q来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。相对熵可以衡量两个随机分布之间的距离，当两个随机分布相同时，它们的相对熵为零，当两个随机分布的差别增大时，它们的相对熵也会增大。所以相对熵（KL散度）可以用于比较文本的相似度，先统计出词的频率，然后计算相对熵。 KL距离，是对同一个随机变量X的两个单独的概率分布的度量。记为\(D_{KL}(p||q)\)。 它度量当真实分布为p时，假设分布q的无效性。 D_{KL}(p||q)=Ep[logp(x)q(x)]=∑_{x∈χ}p(x)logp(x)q(x) \\ =∑_{x∈χ}[p(x)logp(x)−p(x)logq(x)] \\ =∑_{x∈χ}p(x)logp(x)−∑_{x∈χ}p(x)logq(x)\\ =−H(p)−∑_{x∈χ}p(x)logq(x) \\ =−H(p)+Ep[−logq(x)]\\ =Hp(q)−H(p)\\并且为了保证连续性，做如下约定：0log0/0=0，0log0/q=0，plogp/0=∞显然，当p=q时,两者之间的相对熵DKL(p||q)=0上式最后的Hp(q)表示在p分布下，使用q进行编码需要的bit数，而H(p)表示对真实分布p所需要的最小编码bit数。基于此，相对熵的意义就很明确了：DKL(p||q)表示在真实分布为p的前提下，使用q分布进行编码相对于使用真实分布p进行编码（即最优编码）所多出来的bit数。 重要性质 ： 1.它是非负的。 2.不是对称的:对于某些P和Q，DKL(p||q)DKL(p||q)不等于DKL(q||p)DKL(q||p),这样意味着选择DKL(p||q)DKL(p||q)还是DKL(q||p)DKL(q||p)影响很大 例子 假如一个字符发射器，随机发出0和1两种字符，真实发出概率分布为A，但实际不知道A的具体分布。通过观察，得到概率分布B与C，各个分布的具体情况如下： img img img 可以计算出得到如下： img img 也可以看出，按照概率分布B进行编码，要比按照C进行编码，平均每个符号增加的比特数目少。从分布上也可以看出，实际上B比C更接近实际分布（因为其与A分布的相对熵更小）. 交叉熵(Cross-Entropy)交叉熵容易跟相对熵搞混，二者联系紧密，但又有所区别。假设有两个分布p，q则它们在给定样本集上的交叉熵定义如下： CEH(p,q)=Ep[−logq]=−∑x∈χp(x)logq(x)=H(p)+DKL(p||q)可以看出，交叉熵与上一节定义的相对熵仅相差了H(p),当p已知时，可以把H(p)看做一个常数，此时交叉熵与KL距离在行为上是等价的，都反映了分布p、q的相似程度。最小化交叉熵等于最小化KL距离。它们都将在p=q时取得最小值H(p)（p=q时KL距离为0），因此有的工程文献中将最小化KL距离的方法称为Principle of Minimum Cross-Entropy (MCE)或Minxent方法。 由交叉熵到logistic regression特别的，在logistic regression中p:真实样本分布，服从参数为p的0-1分布，即X∼B(1,p)q:待估计的模型，服从参数为q的0-1分布，即X∼B(1,q)两者的交叉熵为： CEH(p,q) \\ =−∑x∈χp(x)logq(x) \\ =−[Pp(x=1)logPq(x=1)+Pp(x=0)logPq(x=0)] \\ =−[plogq+(1−p)log(1−q)] \\\ =−[yloghθ(x)+(1−y)log(1−hθ(x))]\\对所有训练样本取均值得： −(1/m)∑^m_{i=1}[y(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i)))]这个结果与通过极大似然估计方法求出来的结果是一致的；最小化交叉熵损失函数等价于求极大似然估计；从二者的公式来看，只是差了一个负号而已。 那么LR的损失函数为什么不用平方损失呢？因为平方损失函数不是凸函数，使用梯度下降法无法求得局部最小（全局最小），而交叉熵损失函数是凸函数，使用梯度下降法可以找到全局最优解。 Bhattacharyya distance巴氏距离 在统计理论中，Bhattacharyya距离用来度量两个离散或连续概率分布的相似性。 它与Bhattacharyya系数（Bhattacharyya coefficient）高度相关，后者是用来度量两个统计样本的重叠度的。所有这些命名都是为了纪念A. Bhattacharyya，一个在1930年工作于印度统计局的统计学家 该系数可以用来度量两个样本集的相似性。它通常在分类问题中被用来判断类别的可分性。 定义 对于定义在同一个定义域X上的两个离散概率分布p和q来说，它们之间的Bhattacharyya距离可定义如下： D_B(p,q) = -\ln \left( BC(p,q) \right) 这里 BC(p,q) = \sum_{x\in X} \sqrt{p(x) q(x)} 被称为Bhattacharyya系数。 对于连续概率分布，Bhattacharyya系数可以定义如下： BC(p,q) = \int \sqrt{p(x) q(x)}\, dx 在以上两种情况下，0&lt;=BC&lt;=1并且0&lt;=DB&lt;=∞。DB并不遵循三角不等式，但是Hellinger距离满足三角不等式。 对于一个多维高斯分布来说pi=N(mi,Pi)， D_B={1\over 8}(m_1-m_2)^T P^{-1}(m_1-m_2)+{1\over 2}\ln \,\left({\det P \over \sqrt{\det P_1 \, \det P_2} }\right) 这里mi和Pi分别代表该分布的均值和方差，并且 P={P_1+P_2 \over 2} 注意到，在这种情况下Bhattacharyya距离的第一项类似于Mahalanobis距离（马氏距离）。 Bhattacharyya系数Bhattacharyya系数用来度量两个统计样本的重叠度。该系数可以用来度量两个样本集的可分性。 计算Bhattacharyya系数包含了一个基本的关于两个样本集重合度的积分运算。两个样本集中的定义域被分成了事前定义的几份，这种划分可以体现在下面的定义中： \mathrm{Bhattacharyya} = \sum_{i=1}^{n}\sqrt{(\mathbf{\Sigma a}_i\cdot\mathbf{\Sigma b}_i)} 其中a，b代表样本，n代表划分的数目，∑ai和∑bi分别代表两个样本集中在第i个划分中的样本之和。 对于两个样本集来说，如果相同划分中的样本数越多，样本和越大，则该式的值越大。划分数的选择取决于每一个样本集中的样本数：太少的划分将因为过高估计了重叠区域而减小精度，而太多的划分将会因为在本该有重叠的区域没有恰好重叠而减小精度（最精细的划分将会使每一个相同的区间中都没有重叠）。 如果在每一个划分区间内的乘积都为零，则Bhattacharyya系数也为零。这就意味着如果A和B两个样本集都与样本集C完全可分，则BC（A，C）=B（B，C）=0，即Bhattacharyya系数对于A和B无法区分。]]></content>
  </entry>
  <entry>
    <title><![CDATA[聚类算法评价指标]]></title>
    <url>%2Fhexo%2F2018%2F06%2F04%2F2018-06-04_cluster_criteria%2F</url>
    <content type="text"><![CDATA[原文原文1 无监督聚类1. Compactness(紧密性)(CP) CP计算 每一个类各点到聚类中心的平均距离，CP越低意味着类内聚类距离越近 缺点：没有考虑类间效果 2. Separation(间隔性)(SP) SP计算 各聚类中心两两之间平均距离，SP越高意味类间聚类距离越远 缺点：没有考虑类内效果 3. Davies-Bouldin Index(戴维森堡丁指数)(分类适确性指标)(DB)(DBI) DB计算 任意两类别的类内距离平均距离(CP)之和除以两聚类中心距离 求最大值， DB越小意味着类内距离越小 同时类间距离越大 缺点：因使用欧式距离 所以对于环状分布 聚类评测很差 4. Dunn Validity Index (邓恩指数)(DVI) DVI计算 任意两个簇元素的最短距离(类间)除以任意簇中的最大距离(类内)，DVI越大意味着类间距离越大 同时类内距离越小 缺点：对离散点的聚类测评很高、对环状分布测评效果差 5. C-index for COX model6. Silhouette width轮廓系数（Silhouette coefficient）适用于实际类别信息未知的情况。对于单个样本，设a是与它同类别中其他样本的平均距离，b是与它距离最近不同类别中样本的平均距离。对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值。轮廓系数取值范围是[−1,1]，同类别样本越距离相近且不同类别样本距离越远，分数越高。 7. survival analysis有监督聚类1、Cluster Accuracy (准确性)(CA) CA计算 聚类正确的百分比 CA越大证明聚类效果越好 召回率（Recall）指的是所有正样本有多少被模型判为正样本 纯度purity 计算正确聚类的文档数占总文档数的比例） 其中Ω = {ω1,ω2, . . . ,ωK}是聚类的集合ωK表示第k个聚类的集合。C = {c1, c2, . . . , cJ}是文档集合，cJ表示第J个文档。N表示文档总数。 2. ROC曲线 （二分类）真正类率(true positive rate ,TPR)，刻画的是分类器所识别出的 正实例占所有正实例的比例（正样本预测结果数 / 正样本实际数）。负正类率(false positive rate, FPR)，计算的是分类器错认为正类的负实例占所有负实例的比例（被预测为正的负样本结果数 /负样本实际数）。 ( TPR=0,FPR=0 ) 把每个实例都预测为负类的模型( TPR=1,FPR=1 ) 把每个实例都预测为正类的模型( TPR=1,FPR=0 ) 理想模型 3. F值RI方法有个特点就是把准确率和召回率看得同等重要，事实上有时候我们可能需要某一特性更多一点，这时候就适合F值方法. 4. Rand index(兰德指数)(RI) 、Adjusted Rand index(调整兰德指数)(ARI) 其中C表示实际类别信息，K表示聚类结果，a表示在C与K中都是同类别的元素对数，b表示在C与K中都是不同类别的元素对数。分母表示数据集中可以组成的对数。 RI取值范围为[0,1]，值越大意味着聚类结果与真实情况越吻合。RI越大表示聚类效果准确性越高 同时每个类内的纯度越高 为了实现“在聚类结果随机产生的情况下，指标应该接近零”，调整兰德系数（Adjusted rand index）被提出，它具有更高的区分度： ARI取值范围为[−1,1]，值越大意味着聚类结果与真实情况越吻合。从广义的角度来讲，ARI衡量的是两个数据分布的吻合程度。 5、Normalized Mutual Information (标准互信息)(NMI)、Mutual Information(互信息)(MI)互信息（Mutual Information）是用来衡量两个数据分布的吻合程度。也是一有用的信息度量，它是指两个事件集合之间的相关性。用互信息的方法，在某个类别C中的出现概率高，而在其它类别中的出现概率低的词条T，将获得较高的词条和类别互信息，也就可能被选取为类别C的特征。 互信息是term的存在与否能给类别c的正确判断带来的信息量。词条和类别的互信息体现了词条和类别的相关程度，互信息越大，词条和类别的相关程度也越大。得到词条和类别之间的相关程度后，选取一定比例的，排名靠前的词条作为最能代表此种类别的特征。 标准化互聚类信息都是用熵做分母将MI值调整到0与1之间，一个比较多见的实现是下面所示： 例子讲解如图认为x代表一类文档，o代表一类文档，方框代表一类文档， 假设一个集合中有N篇文章一个集合中有N(N-1)/2个集合对TP：同一类的文章被分到同一个簇TN：不同类的文章被分到不同簇FP：不同类的文章被分到同一个簇FN：同一类的文章被分到不同簇 purity = ( 5+ 4 + 3) / 17 = 0.71 RI = （TP+TN）/（TP+FP+FN+TN）#Rand Index度量的正确的百分比TP＋FP ＝ C(2,6) + C(2,6) + C(2,5) = 15 + 15 + 10 = 40 其中C(n,m)是指在m中任选n个的组合数。TP = C(2,5) + C(2,4) + C(2,3) + C(2,2) = 20FP = 40 - 20 = 20相似的方法可以计算出TN = 72 FN = 24所以RI ＝ ( 20 + 72) / ( 20 + 20 + 72 +24) = 0.68 准确率Precision=TP/(TP+FP) 召回率Recall=TP/(TP+FN) F1=2×Recall×Precision/(Recall+Precision)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Gap Statistic 间隔统计量]]></title>
    <url>%2Fhexo%2F2018%2F06%2F04%2F2018-06-04_gap_statistic%2F</url>
    <content type="text"><![CDATA[原文 Gap statistic由Tibshirani等人提出，用以解决聚类问题确定所判定类的数目。 聚类的紧支测度 （measure of the compactness）最简单的方法是使用类内样本点之间的欧式距离来表示，记为Dk，DK越小聚类的紧支性越好。 D_k = \sum_{x_i \in C_k}\sum_{x_j\in C_k}||x_i - x_j ||^2 = 2n_k\sum_{x_i\in C_k}||x_i - \mu_k||^2标准化后： W_k = \sum_{k=1}^{K}\frac{1}{2n_k}D_kWk 是elbow method的基础。 Reference]]></content>
  </entry>
  <entry>
    <title><![CDATA[R-appveyor设置]]></title>
    <url>%2Fhexo%2F2018%2F05%2F18%2F2018-05-18_r-appveyor%2F</url>
    <content type="text"><![CDATA[https://github.com/krlmlr/r-appveyor Usage Sign up to AppVeyor. Enable testing for your project. Run devtools::use_appveyor() in your project. (Optional) Adapt appveyor.yml to your needs according to the documentation. (Optional) Add a badge as described by the output of devtools::use_appveyor(). Be sure to supply a .gitattributes file that takes care of fixing CRLF conversion settings that are relevant on Windows. The one in this repo can be used for starters. Push to your repo to start building. Enjoy! !!! note The cell type informat]]></content>
  </entry>
  <entry>
    <title><![CDATA[R包安装路径设置及从Bioconductor安装包,BiocCheck]]></title>
    <url>%2Fhexo%2F2018%2F05%2F15%2F2018-05-15_R_environment%2F</url>
    <content type="text"><![CDATA[设置临时安装路径 .libPaths().libPaths(.libPaths()[2]) 设置永久安装路径windows设置环境变量 新建系统环境变量R_LIBS ,值为自定义R包安装目录。 重启R,在R中运行.libPaths()即可查看设置是否生效。 R_LIBS=D:/R_Library Linux 设置 1$ R_LIBS="/usr/lib/R/Library" R -q -e 'print(.libPaths())' R 编译不通过1Sys.setenv("PKG_CXXFLAGS"="-std=c++11") BiocCheck12dir="D:/xxx_0.99.7.tar.gz"BiocCheck(dir) By default that the directory gets cleaned up as long as there aren’t any errors. If you want it to stick around (to check warnings, for instance) you can enable that under Tools &gt; Global Options &gt; Packages “Cleanup output after successful R CMD check” option. R从bioconductor安装包123456789101112source("http://bioconductor.org/biocLite.R")options(BioC_mirror="http://mirrors.ustc.edu.cn/bioc/")我的做法是把上述两句放到 ~/.Rprofile 的，但是有时候网速慢导致 source 这一步很慢，结果就是启动 R 需要几秒钟或者报错。最后我直接自定义函数：source.bio &lt;- function()&#123; source("http://bioconductor.org/biocLite.R") options(BioC_mirror="http://mirrors.ustc.edu.cn/bioc/")&#125;然后仍然放到 ~/.Rprofile ，然后需要装 Bioconductor 的包的时候先运行一下 source.bio() 就好了。最后，如果你用 Windows 系统的话，上述文件换成 C:\Program Files\R\R-3.4.2\etc\Rprofile.site 这个文件。链接：https://www.zhihu.com/question/63601255/answer/360879580 1234567891011121314151617181920212223242526272829303132333435363738394041424344local(&#123; options( repos = "https://mirrors.ustc.edu.cn/CRAN/" ) options( BioC_mirror = "https://mirrors.ustc.edu.cn/bioc/" )&#125;)bioPackages &lt;- c( "dplyr", "stringi", "purrr", ## ERROR "R.utils", "data.table", ## unzip and read table "GEOquery", ## download "FactoMineR", "factoextra", "ggfortify", ## PCA "pheatmap", ## heatmap "ggplot2", ## Volcano plot "limma", "DESeq2", "edgeR", ## DEG "clusterProfiler", "org.Hs.eg.db", ## annotation "pathview" ## kegg )# Step3 Install the packages ----------------------------------------------lapply( bioPackages, function(bioPackage) &#123; if ( !require( bioPackage, character.only = T ) ) &#123; CRANpackages &lt;- available.packages() ## install packages by CRAN if ( bioPackage %in% rownames( CRANpackages ) ) &#123; install.packages( bioPackage ) &#125;else&#123; ## install packages by bioconductor ## R version &gt;= 3.5 ===&gt; BiocManager if ( as.character( sessionInfo()$R.version$minor ) &gt;= 3.5 ) &#123; if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager") BiocManager::install(bioPackage, update = TRUE, ask = FALSE) &#125;else&#123; ## R version &lt; 3.5 ===&gt; BiocInstaller if (!requireNamespace("BiocInstaller", quietly = TRUE)) source( "https://bioconductor.org/biocLite.R" ) BiocInstaller::biocLite( bioPackage, ask = FALSE) &#125; &#125; &#125; &#125;)]]></content>
  </entry>
  <entry>
    <title><![CDATA[R语言数据库操作]]></title>
    <url>%2Fhexo%2F2018%2F04%2F11%2F2018-04-11_Rdatabase%2F</url>
    <content type="text"><![CDATA[RSQLite1234567891011121314151617181920212223242526272829303132333435363738394041424344table_name="howdo"colNames=c("weather","food","size","direction")### 1.创建sqlite数据库con&lt;-dbConnect(SQLite(), "test.db")### 2.判断一个表是否存在if(!dbExistsTable(con,table_name))&#123; ### 3.写入一个表 table_info=as.data.frame(matrix(character(0),ncol=4),stringsAsFactors = FALSE) colnames(table_info)= colNames dbWriteTable(con, table_name, table_info)&#125;### 4.读表DF=dbReadTable(con, table_name)### 5.写表DF=as.data.frame(t(c(1:4)))colnames(DF)=colNamesdbWriteTable(con,table_name,DF,append = TRUE)DF1=matrix(sample(1:100,12),ncol=4)colnames(DF1)=colNamesdbWriteTable(con,table_name,DF1,append = TRUE)DF2=dbReadTable(con, table_name)### 6.查询语句query1=paste("select * from",shQuote(table_name), "where weather=",shQuote(57), " and food=",shQuote(26))tmp=dbGetQuery(con,query1)### 7.写入指定单元格query2=paste("UPDATE",shQuote(table_name), "SET size=",shQuote(88888), "where weather=", shQuote(57),"and food=",shQuote(26))dbSendQuery(con,query2)### 8.删除recordsquery3=paste("DELETE FROM",shQuote(table_name), "where weather=", shQuote(57),"and food=",shQuote(26))dbSendQuery(con,query3)###10.最后断开连接dbDisconnect(con) # 断开连接]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python资料汇总]]></title>
    <url>%2Fhexo%2F2017%2F12%2F21%2F2017-12-21_python_all%2F</url>
    <content type="text"><![CDATA[原文 Python资料推荐 +经典练手项目今天给大家带来一批宝贝，大家可以在深夜里独自把玩，也可以在广场上一边遛狗一边和盆友们品鉴。 学习资料1、入门阶段 The Python TutorialPython官方文档，永远是最佳选择Google’s Python Class Google的文档，质量相当高 Python3 教程 | 菜鸟教程 如果英文不好，可以参考国内教程 Learn Python the Hard Way 最简单的学习 Python 的方法， HTML 在线版是完全免费的 零基础入门学习Python 网易云课堂，老师诙谐幽默，上手快 2、拔高阶段 最好自己动手写一些项目，一定要使用GitHub Python_精选项目课程 实验楼提供在线编程及在线实训学习平台Django 开发内容管理系统 全面的中文教程 Dataquest 提供了一系列和数据分析相关的Python教程 Python爬虫学习系列教程 | 静觅 静觅的爬虫课程，名气非常大 3、方向进阶 来到这个时候，最好确立学习的方向。 web开发（Django、Flask、Tornado） python web 入坑指南 数据科学（Numpy、Pandas、Matplotlib） 数据科学家的完整学习路径（Python版) 机器学习（scikit-learn）英文文档 中文文档 深度学习（TensorFlow）www.tensorflow.org/ 网络爬虫（Scrapy）Scrapy 1.4 documentation 4、实战演练 有哪些适合新手练手的Python项目 非常全面的项目，强烈推荐实战 廖雪峰老师的教程，非常经典，可以当做Python的百科全书来参考 Python开源软件 开源中国社区的项目合集 karan/Projects Python项目合集 Python项目_W3Cschool极客导航 不仅有web开发，还有爬虫相关 5、 博客列表廖雪峰的官方网站 研究互联网产品和技术，提供原创中文精品教程 虫师 关于Python自动化方面的一位大牛 宁哥的小站 Python网络爬虫和机器学习 静觅 崔庆才的个人网站，在爬虫方面造诣很深 Python, OpenStack 博客里面包含了很多python library的知识 知乎 - 与世界分享你的知识、经验和见解 隐藏着各种Python大神 经典入门及第三方库 vinta/awesome-python 精心设计的Python学习框架，书籍和软件 nvbn/thefuck 华丽的应用程序更正您以前的控制台命令 pallets/flask、django/django Python的web框架 requests/requests 强大的库，相信你一定学过 ipython/ipython 生产性交互式计算系统 python/cpython Python编程语言官方文档 好玩的项目 warner/magic-wormhole 把文件从一台电脑安全地复制到另一台 pyvideo/pyvideo 和Python相关的视频 7sDream/zhihu-oauth 知乎官方未开放的 OAuth2 接口 errbotio/errbot 最简单和最流行的聊天机器人 fogleman/Minecraft 用Python写的我的世界 mopidy/mopidy 一个可扩展的音乐服务器 Eloston/ungoogled-chromium 可以修改Google Chromium组件 livid/v2ex 在Google App Engine上运行的社区 overviewer/Minecraft-Overviewer 展示高分辨率地图 charlierguo/gmail Google Mail的Pythonic界面 egirault/googleplay-api Google Play非官方的Python API 网络爬虫 LiuXingMing/SinaSpider 新浪微博爬虫（Scrapy、Redis） binux/pyspider Python中强大的网络爬虫系统 bowenpay/wechat-spider 微信公众号爬虫 jhao104/proxy_pool Python爬虫代理IP池(proxy pool) smicallef/spiderfoot SpiderFoot，开源脚印和情报收集工具。 lining0806/PythonSpiderNotes Python入门网络爬虫之精华版 Germey/Zhihu 崔庆才博主写的知乎爬虫 gnemoug/distribute_crawler 分布式网络爬虫 Chyroc/基于搜狗微信搜索的微信公众号爬虫接口 ResolveWang/weibospider 分布式微博爬虫(PC端抓取) airingursb/bilibili-user Bilibili用户爬虫 yanzhou/CnkiSpider 中国知网爬虫 数据相关 donnemartin/data-science-ipython-notebooks 综合型的数据科学教程 sqlmapproject/sqlmap 自动SQL注入和数据库接管工具 mitmproxy/mitmproxy 针对渗透测试员和软件开发人员的交互式TLS功能拦截HTTP代理 rushter/data-science-blogs 很多数据科学的博客 ujjwalkarn/DataSciencePython 通用数据分析和机器学习任务 justmarkham/DAT3 华盛顿的大数据课程 billryan/algorithm-exercise leetcode/lintcode题解 bitly/data_hacks 使用命令行进行数据分析 机器学习 MorvanZhou/tutorials 机器学习相关教程 ahangchen/GDLnotes 谷歌深度学习笔记 tensorflow/models 使用TensorFlow构建模型 astorfi/TensorFlow-World TensorFlow的简单和现成的教程 fchollet/keras Python深度学习库，运行在TensorFlow，Theano或CNTK josephmisiti/awesome-machine-learning 精心设计的机器学习框架，书籍和软件 songrotek/Deep-Learning-Papers-Reading-Roadmap 深度学习论文阅读路线图 MLWave/Kaggle-Ensemble-Guide Kaggle组合指南的代码 eriklindernoren/ML-From-Scratch 机器学习模型和算法 humphd/have-fun-with-machine-learning 机器学习与神经网络图像分类的初学者指南 arielf/weight-loss 机器学习符合酮症：如何有效减肥 luispedro/BuildingMachineLearning 本书用Python构建机器学习系统的源代码 Comprehensive learning path – Data Science in PythonJourney from a Python noob to a Kaggler on PythonSo, you want to become a data scientist or may be you are already one and want to expand your tool repository. You have landed at the right place. The aim of this page is to provide a comprehensive learning path to people new to python for data analysis. This path provides a comprehensive overview of steps you need to learn to use Python for data analysis. If you already have some background, or don’t need all the components, feel free to adapt your own paths and let us know how you made changes in the path. You can also check the mini version of this learning path –&gt; Infographic: Quick Guide to learn Data Science in Python Step 0: Warming upBefore starting your journey, the first question to answer is: Why use Python? or How would Python be useful? Watch the first 30 minutes of this talk from Jeremy, Founder of DataRobot at PyCon 2014, Ukraine to get an idea of how useful Python could be. Step 1: Setting up your machineNow that you have made up your mind, it is time to set up your machine. The easiest way to proceed is to just download Anaconda from Continuum.io . It comes packaged with most of the things you will need ever. The major downside of taking this route is that you will need to wait for Continuum to update their packages, even when there might be an update available to the underlying libraries. If you are a starter, that should hardly matter. If you face any challenges in installing, you can find more detailed instructions for various OS here Step 2: Learn the basics of Python languageYou should start by understanding the basics of the language, libraries and data structure. The free interactive Python tutorial by DataCamp is one of the best places to start your journey. This 4 hour coding course focuses on how to get started with Python for data science and by the end you should be comfortable with the basic concepts of the language. Specifically learn: Lists, Tuples, Dictionaries, List comprehensions, Dictionary comprehensions Assignment: Take the interactive Python tutorial by DataCamp Alternate resources: If interactive coding is not your style of learning, you can also look at The Google Class for Python. It is a 2 day class series and also covers some of the parts discussed later. Step 3: Learn Regular Expressions in PythonYou will need to use them a lot for data cleansing, especially if you are working on text data. The best way to learn Regular expressions is to go through the Google class and keep this cheat sheet handy. Assignment: Do the baby names exercise If you still need more practice, follow this tutorial for text cleaning. It will challenge you on various steps involved in data wrangling. Step 4: Learn Scientific libraries in Python – NumPy, SciPy, Matplotlib and PandasThis is where fun begins! Here is a brief introduction to various libraries. Let’s start practicing some common operations. Practice the NumPy tutorial thoroughly, especially NumPy arrays. This will form a good foundation for things to come. Next, look at the SciPy tutorials. Go through the introduction and the basics and do the remaining ones basis your needs. If you guessed Matplotlib tutorials next, you are wrong! They are too comprehensive for our need here. Instead look at this ipython notebook till Line 68 (i.e. till animations) Finally, let us look at Pandas. Pandas provide DataFrame functionality (like R) for Python. This is also where you should spend good time practicing. Pandas would become the most effective tool for all mid-size data analysis. Start with a short introduction, 10 minutes to pandas. Then move on to a more detailed tutorial on pandas. Check out DataCamp’s course on Pandas Foundations You can also look at Exploratory Data Analysis with Pandas and Data munging with Pandas Additional Resources: If you need a book on Pandas and NumPy, “Python for Data Analysis by Wes McKinney” There are a lot of tutorials as part of Pandas documentation. You can have a look at them here Assignment: Solve this assignment from CS109 course from Harvard. Step 5: Effective Data VisualizationGo through this lecture form CS109. You can ignore the initial 2 minutes, but what follows after that is awesome! Follow this lecture up with this assignment Check out Bokeh Data Visualization Tutorial from DataCamp Step 6: Learn Scikit-learn and Machine LearningNow, we come to the meat of this entire process. Scikit-learn is the most useful library on python for machine learning. Here is a brief overview of the library. Go through lecture 10 to lecture 18 from CS109 course from Harvard. You will go through an overview of machine learning, Supervised learning algorithms like regressions, decision trees, ensemble modeling and non-supervised learning algorithms like clustering. Follow individual lectures with the assignments from those lectures. Additional Resources: If there is one book, you must read, it is Programming Collective Intelligence – a classic, but still one of the best books on the subject. Additionally, you can also follow one of the best courses on Machine Learning course from Yaser Abu-Mostafa. If you need more lucid explanation for the techniques, you can opt for the Machine learning course from Andrew Ngand follow the exercises on Python. Tutorials on Scikit learn Assignment: Try out this challenge on Kaggle Step 7: Practice, practice and PracticeCongratulations, you made it! You now have all what you need in technical skills. It is a matter of practice and what better place to practice than compete with fellow Data Scientists on Kaggle. Go, dive into one of the live competitions currently running on Kaggle and give all what you have learnt a try! Step 8: Deep LearningNow that you have learnt most of machine learning techniques, it is time to give Deep Learning a shot. There is a good chance that you already know what is Deep Learning, but if you still need a brief intro, here it is. I am myself new to deep learning, so please take these suggestions with a pinch of salt. The most comprehensive resource is deeplearning.net. You will find everything here – lectures, datasets, challenges, tutorials. You can also try the course from Geoff Hinton a try in a bid to understand the basics of Neural Networks. Get Started with Python: A Complete Tutorial To Learn Data Science with Python From Scratch P.S. In case you need to use Big Data libraries, give Pydoop and PyMongo a try. They are not included here as Big Data learning path is an entire topic in itself.]]></content>
  </entry>
  <entry>
    <title><![CDATA[python—sklearn学习]]></title>
    <url>%2Fhexo%2F2017%2F12%2F21%2F2017-12-21_python_sklearn%2F</url>
    <content type="text"><![CDATA[学习记录]]></content>
  </entry>
  <entry>
    <title><![CDATA[python基本知识]]></title>
    <url>%2Fhexo%2F2017%2F12%2F21%2F2017-12-21_python_study%2F</url>
    <content type="text"><![CDATA[入门教程 视频教程 python IDE环境配置环境配置 pycharm+ anaconda anaconda安装 1234567Install Anaconda: I installed in the root location of E directly choose both "Add anaconda to my PATH" and "Register Anaconda as my default...）# 或者在win10下添加环境变量F:\Anaconda3F:\Anaconda3\ScriptsF:\Anaconda3\Library\bin 123456# 安装anaconda之后 在 windows cmd 命令行创建python解释器conda create -n py_27 python=2.7 or conda create -n py_36 python=3.6# 在cmd命令行激活python解释器activeate py_27# 查看该解释器已经安装的包或者安装新的包conda list pycharm使用技巧(快捷键)1234Ctrl+Q (View | Quick Documentation) #快速查看函数说明文档Ctrl+P (View | Parameter Info) #Ctrl+B (Navigate | Declaration) #打开函数定义文件can be used not only in the editor but in the code completion popup list as well. Jupyter Notebooks Jupyter Notebooks提供了一个环境，你无需离开这个环境，就可以在其中编写你的代码、运行代码、查看输出、可视化数据并查看结果。因此，这是一款可执行端到端的数据科学工作流程的便捷工具，其中包括数据清理、统计建模、构建和训练机器学习模型、可视化数据等等. 123# 安装Anaconda之后自动安装Jupyter Notebooks# windows 命令行输入 就会在你的默认网络浏览器打开（http://localhost:8888/tree）jupyter notebook python 函数查询1. 查看模块下所有函数 123456dir(module_name)from sklearn import datasetsdir(datasets)['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_svmlight_format', 'base', 'california_housing', 'clear_data_home', 'covtype', 'dump_svmlight_file', 'fetch_20newsgroups', 'fetch_20newsgroups_vectorized', 'fetch_california_housing', ...'] 2. 查看模块下特定函数信息 123456789101112131415161718#方法一：help(datasets.load_iris)#方法二：print(datasets.load_iris.__doc__)Help on function load_iris in module sklearn.datasets.base:load_iris(return_X_y=False) Load and return the iris dataset (classification). The iris dataset is a classic and very easy multi-class classification dataset. ================= ============== Classes 3 Samples per class 50 Samples total 150 Dimensionality 4 Features real, positive ================= ============== 基本语法1. 认识python1234567#实例(Python 2.0+)#!/usr/bin/pythonprint "Hello, World!";#实例(Python 3.0+)#!/usr/bin/python3print("Hello, World!"); 2. python 脚本所有 Python 文件将以 .py 为扩展名。用 $ python *.py 执行脚本。 3. python语法Python 的代码块不使用大括号 {} 来控制类，函数以及其他逻辑判断。python 最具特色的就是用缩进来写模块。缩进的空白数量是可变的，但是所有代码块语句必须包含相同的缩进空白数量，这个必须严格执行。 1234if True: print "True"else: print "False" 3.0 变量定义 123&gt; ## 特殊用法&gt; 1. 矩阵A的转置 A.T&gt; __call__ 12345678变量:1. 前带_的变量: 标明是一个私有变量, 只用于标明, 外部类还是可以访问到这个变量2. 前带两个_ ,后带两个_ 的变量: 标明是内置变量,3. 大写加下划线的变量: 标明是不会发生改变的全局变量函数:1. 前带_的变量: 标明是一个私有函数, 只用于标明,2. 前带两个_ ,后带两个_ 的函数: 标明是特殊函数 3.1 python 类对象1__init__() 类对象的初始化函数 3.2 python函数语法1234def functionname( parameters ): "函数_文档字符串" function_suite return [expression] 123456789101112#示例#!/usr/bin/python# -*- coding: UTF-8 -*- # 定义函数def printme( str ): "打印任何传入的字符串" print str; return;# 调用函数printme("我要调用用户自定义函数!");printme("再次调用同一函数"); 3.2 Python 模块Python 模块(Module)，是一个 Python 文件，以 .py 结尾，包含了 Python 对象定义和Python语句。 1234# 定义support.py 模块：def print_func( par ): print "Hello : ", par return 模块的引入 1234567#!/usr/bin/python# -*- coding: UTF-8 -*-# 导入模块import support# 现在可以调用模块里包含的函数了# 调用方法：模块名.函数名support.print_func("Runoob") From…import 语句 Python 的 from 语句让你从模块中导入一个指定的部分到当前命名空间中。 1from fib import fibonacci 3.3 常用语句 读文件 123456try: f = open('/path/to/file', 'r') print(f.read())finally: if f: f.close() 123#with语句来自动帮我们调用close()方法with open('/path/to/file', 'r') as f: print(f.read()) 12for line in f.readlines(): print(line.strip()) # 把末尾的'\n'删掉 写文件 12with open('/Users/michael/test.txt', 'w') as f: f.write('Hello, world!') 3.4 常规语法import 1234567from...import如 from A import b,相当于import Ab=A.b在此过程中有一个隐含的赋值的过程import A as B,给予A库一个B的别称，帮助记忆 原始字符串在单引号或者双引号 前面加一个字符 r 运算符 1234567a=10a+=1 等价于 a=a+1=11/ 除法// 除法取整% 取余** 幂运算 3**2=9 三元操作符 small =x if x &lt;y else y 条件真为x, 假为y 断言 assert 3&gt;4 循环121. for for i in range(1,8): 注意冒号 break跳出当前循环 continue 终止本轮循环，开启下一轮循环（如果循环条件为真） range()函数 range([start,]stop[,step=1]) list(range(5)) 列表12345678910menber=["a","b","c"] 单一类型列表mix=[1,"a",3.14,[a,b,c]] 混合类型列表empty=[] 创建空列表empty.append("小甲鱼") 向列表添加单一元素empty.extend([a,b]) 向列表添加另外一个列表empty.insert(0,"牡丹")empty.remove("牡丹")del empty 删除pop()从列表中删除最后一个元素并弹出 操作符1列表比较大小 只比较第一个元素 Python特殊语法：filter、map、reduce、lambda1.lambda 123456789101112131415161718这是Python支持一种有趣的语法，它允许你快速定义单行的最小函数，类似与C语言中的宏，这些叫做lambda的函数，是从LISP借用来的，可以用在任何需要函数的地方： &gt;&gt;&gt; g = lambda x: x * 2 &gt;&gt;&gt; g(3) 6 &gt;&gt;&gt; (lambda x: x * 2)(3) 6---lambda表达式返回一个函数对象例子：func = lambda x,y:x+yfunc相当于下面这个函数def func(x,y): return x+y 注意def是语句而lambda是表达式,下面这种情况下就只能用lambda而不能用def[(lambda x:x*x)(x) for x in range(1,11)]返回值：[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 2. filter(function, sequence) 12345678910111213141516对sequence中的item依次执行function(item)，将执行结果为True的item组成一个List/String/Tuple（取决于sequence的类型）返回,function的返回值只能是True或False.&gt;&gt;&gt; def f(x): return x % 2 != 0 and x % 3 != 0 &gt;&gt;&gt; filter(f, range(2, 25)) [5, 7, 11, 13, 17, 19, 23]&gt;&gt;&gt; def f(x): return x != 'a' &gt;&gt;&gt; filter(f, "abcdef") 'bcdef' 找出1到10之间的奇数filter(lambda x:x%2!=0,range(1,11))返回值[1,3,5,7,9] 如果sequence是一个stringfilter(lambda x:len(x)!=0,'hello')返回'hello'filter(lambda x:len(x)==0,'hello')返回'' 3.map(function, sequence) 123456789101112对sequence中的item依次执行function(item)，见执行结果组成一个List返回：&gt;&gt;&gt; def cube(x): return x*x*x &gt;&gt;&gt; map(cube, range(1, 11)) [1, 8, 27, 64, 125, 216, 343, 512, 729, 1000]&gt;&gt;&gt; def cube(x) : return x + x ... &gt;&gt;&gt; map(cube , "abcde") ['aa', 'bb', 'cc', 'dd', 'ee']另外map也支持多个sequence，这就要求function也支持相应数量的参数输入：&gt;&gt;&gt; def add(x, y): return x+y &gt;&gt;&gt; map(add, range(8), range(8)) [0, 2, 4, 6, 8, 10, 12, 14] 4. reduce(function, sequence, starting_value) 123456789101112function接收的参数个数只能为2,先把sequence中第一个值和第二个值当参数传给function，再把function的返回值和第三个值当参数传给function (对sequence中的item顺序迭代调用function)，然后只返回一个结果，如果有starting_value，还可以作为初始值调用，例子：求1到10的累加reduce(lambda x,y:x+y,range(1,11))返回值是55。&gt;&gt;&gt; def add(x,y): return x + y &gt;&gt;&gt; reduce(add, range(1, 11)) 55 （注：1+2+3+4+5+6+7+8+9+10）&gt;&gt;&gt; reduce(add, range(1, 11), 20) 75 （注：1+2+3+4+5+6+7+8+9+10+20） 5. 可以把filter map reduce 和lambda结合起来用，函数就可以简单的写成一行。 12345例如kmpathes = filter(lambda kmpath: kmpath, map(lambda kmpath: string.strip(kmpath),string.split(L, ':'))) #对L中的所有元素以':'做分割，得出一个列表。对这个列表的每一个元素做字符串strip，形成一个列表。对这个列表的每一个元素做直接返回操作(这个地方可以加上过滤条件限制)，最终获得一个字符串被':'分割的列表，列表中的每一个字符串都做了strip，并可以对特殊字符串过滤。 6. zip([iterable, …]) 1234567891011zip()是Python的一个内建函数，它接受一系列可迭代的对象作为参数，将对象中对应的元素打包成一个个tuple（元组），然后返回由这些tuples组成的list（列表）。若传入参数的长度不等，则返回list的长度和参数中长度最短的对象相同。利用*号操作符，可以将list unzip（解压）.&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; b = [4,5,6]&gt;&gt;&gt; c = [4,5,6,7,8]&gt;&gt;&gt; zipped = zip(a,b)[(1, 4), (2, 5), (3, 6)]&gt;&gt;&gt; zip(a,c)[(1, 4), (2, 5), (3, 6)]&gt;&gt;&gt; zip(*zipped)[(1, 2, 3), (4, 5, 6)]]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习Tensorflow]]></title>
    <url>%2Fhexo%2F2017%2F12%2F20%2F2017-12-20_deeplearning%2F</url>
    <content type="text"><![CDATA[学习教程 斯坦福李飞飞 TensorFlow中文社区 numpy API 卷积神经网络1 卷积神经网络2 Roadmap AI圣经 零基础入门深度学习]]></content>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2Fhexo%2F2017%2F11%2F21%2F2017-11-13_decision_tree%2F</url>
    <content type="text"><![CDATA[特征选择问题选择信息增益和信息增益比大的特征 信息增益（互信息）表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。 信息熵熵越大，随机变量的不确定性就越大。 条件熵随机变量X给定条件下随机变量Y的条件熵 H(Y|X)定义为X给定条件下Y的条件概率分布的熵对X的数学期望。 信息熵和条件熵中概率由数据估计得到，所对应的熵与条件熵分别称为经验熵和经验条件熵 信息增益比信息增益与训练数据集经验熵的比值]]></content>
  </entry>
  <entry>
    <title><![CDATA[SAM文件与SAMtools]]></title>
    <url>%2Fhexo%2F2017%2F11%2F12%2F2017-11-11_sam_samtools%2F</url>
    <content type="text"><![CDATA[SAMSAM输出的结果中每一行都包括十二项通过Tab分隔1FCC0YG3ACXX:2:1103:1572:139769#GCTTAATG 99 chr10 60001 0 90M = 60390 479 GAATTCCTTGAGGCCTAAATGCATCGGGGTGCTCTGGTTTTGTTGTTGTTATTTCTGAATGACATTTACTTTGGTGCTCTTTATTTTGCG CCCFFFFFHHHHHJJJJJJJJIJJJJJJJ?HHGIJJJBFHIJIJIDHIHIEHJJIJJIJJJHHGHHHFFFFFFEDCEEECCDDDDEECDD XT:A:R NM:i:0 SM:i:0 AM:i:0 X0:i:2 X1:i:0 XM:i:0 XO:i:0 XG:i:0 MD:Z:90 XA:Z:chr18,+14415,90M,0; RG:Z:120618_I245_FCC0YG3ACXX_L2_SZAXPI010030-30 1234567891011121314151617181920212223242526(1)FCC0YG3ACXX:2:1103:1572:139769#GCTTAATG(2)99(3)chr10(4)60001(5)0(6)90M(7)=(8)60390(9)479(10)GAATTCCTTGAGGCCTAAATGCATCGGGGTGCTCTGGTTTTGTTGTTGTTATTTCTGAATGACATTTACTTTGGTGCTCTTTATTTTGCG(11)CCCFFFFFHHHHHJJJJJJJJIJJJJJJJ?HHGIJJJBFHIJIJIDHIHIEHJJIJJIJJJHHGHHHFFFFFFEDCEEECCDDDDEECDD(12)XT:A:R NM:i:0 SM:i:0 AM:i:0 X0:i:2 X1:i:0 XM:i:0 XO:i:0 XG:i:0 MD:Z:90 XA:Z:chr18,+14415,90M,0; RG:Z:120618_I245_FCC0YG3ACXX_L2_SZAXPI010030-30(1)read的名字(2)Flag 为各个标志的和(3)比对到的染色体号(4)第一个比对上的碱基所在位置(5)质量值 6. CIGAR 如果CIGAR是*，这个才是判断左右端是否匹配失败的标准7. mate比对上的染色体号，如果是“=”，则表示在同一条染色体上，*表示没有比对上8. mate第一个比对上的碱基所在位置 9. 该read和mate的距离(估计出的片段的长度，当mate 序列位于本序列上游时该值为负值)10. 序列 11. 序列对应的质量值 (ASCII码格式)12. 标记 (1)如果PE reads的左右两端均没有比对成功，那么第3,6,7列都是*，4，5，8，9都是0，第2列flag只有77,141这两种情况。 77代表PE,而且PE的两条reads都是unmanned的， 141跟77一样，只是它们分别指代unmanned的PE的reads的两端, (2) 如果是左右两端reads只有一个比对成功，另一个reads没有比对上. 不管是左端还是右端，第3列都是有染色体的，(第7列是=号[好像不对])，但这并不能说明左端跟右端有着同样的比对结果。而第6列CIGAR是*，这个才是判断左右端是否匹配失败的标准。An unmapped segment without coordinate has a * at this field. However, an unmapped segment may also have an ordinary coordinate such that it can be placed at a desired position after sorting. If RNAME is *, no assumptions can be made about POS and CIGAR. (https://samtools.github.io/hts-specs/SAMv1.pdf) 这也就是我为什么没有发现第7列有染色体，第3列是*号的reads。即使PE reads的右端匹配，左端未匹配，它只会把这个read比对的染色体写在第3列，而不是第7列！所以说要想探究它是左端还是右端未比对成功，得看flag。 Flag 标志Flag值解析 https://broadinstitute.github.io/picard/explain-flags.html 1 序列是一对序列中的一个 2 比对结果是一个pair-end比对的末端 4 没有找到位点 8 这个序列是pair中的一个但是没有找到位点 16 在这个比对上的位点，序列与参考序列反向互补 32 这个序列在pair-end中的mate序列与参考序列反向互补 64 序列是 mate 1 128 序列是 mate 2 质量值因工具而异。质量值越高这个比对越可信，如果质量值为0，可能是该序列在参考基因组有多种定位的可能性。 CIGAR如37M1D2M1I，这段字符的意思是37个匹配，1个参考序列上的删除，2个匹配，1个参考序列上的插入。M代表的是alignment match(可以是错配) SAMtoolsSAMtools使用手册 SAM（sequence Alignment/mapping)数据格式是目前高通量测序中存放比对数据的标准格式，当然他可以用于存放未比对的数据。 主要功能有： samtools view : BAM-SAM/SAM-BAM 转换和提取部分比对 samtools sort : 比对排序 samtools index: 索引排序比对 samtools merge: 聚合多个排序比对 samtools faidx: 建立FASTA索引，提取部分序列 samtools tview: 文本格式查看序列 samtools pileup: 产生基于位置的结果和 consensus/indel calling]]></content>
  </entry>
  <entry>
    <title><![CDATA[常用生物信息学数据格式]]></title>
    <url>%2Fhexo%2F2017%2F11%2F07%2F2017-11-07_Data_format%2F</url>
    <content type="text"><![CDATA[原文 PDF fasta fastq gff2 gtf(gff2.5) gff3 bed sam、bam vcf 另外，wig_bigWig和bedgraph文件详解，是为了追踪参考基因组的各个区域的覆盖度，测序深度！而且这些定义好的文件，可以无缝连接到UCSC的Genome Browser工具里面进行可视化！]]></content>
  </entry>
  <entry>
    <title><![CDATA[DNA-SEQ 直播我的基因组]]></title>
    <url>%2Fhexo%2F2017%2F11%2F07%2F2017-11-07_DNA_seq%2F</url>
    <content type="text"><![CDATA[教程 Jimmy直播我的基因组分析 - 目录 下载hg19基因组123456cd /data/referencemkdir -p genome/hg19 &amp;&amp; cd genome/hg19nohup wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz &amp;tar zvfx chromFa.tar.gzcat *.fa &gt; hg19.farm chr*.fa fastq-sam-bam1234567891011ls *gz |xargs ~/biosoft/fastqc/FastQC/fastqc -t 10for i in $(seq 1 6) ;do (nohup bwa mem -t 5 -M /data/reference/index/bwa/hg19 KPGP-00001_L$&#123;i&#125;_R1.fq.gz KPGP-00001_L$&#123;i&#125;_R2.fq.gz 1&gt;KPGP-00001_L$&#123;i&#125;.sam 2&gt;KPGP-00001_L$&#123;i&#125;.bwa.align.log &amp;);donefor i in $(seq 1 6) ;do (nohup samtools sort -@ 5 -o KPGP-00001_L$&#123;i&#125;.sorted.bam KPGP-00001_L$&#123;i&#125;.sam &amp;);donefor i in $(seq 1 6) ;do (nohup samtools index KPGP-00001_L$&#123;i&#125;.sorted.bam &amp;);donesamtools merge KPGP-00001.merge.bam *.sorted.bamsamtools sort -@ 10 -O bam -o KPGP-00001.sorted.merge.bam KPGP-00001.merge.bamsamtools index KPGP-00001.sorted.merge.bamfor i in $(seq 1 6) ;do ( samtools flagstat KPGP-00001_L$&#123;i&#125;.sorted.bam &gt;KPGP-00001_L$&#123;i&#125;.flagstat.txt );done 根据gtf格式的基因注释文件得到人所有基因的染色体坐标原文 123456789101112131415161718192021mkdir -p ~/reference/gtf/gencodecd ~/reference/gtf/gencode## https://www.gencodegenes.org/releases/current.htmlwget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.2wayconspseudos.gtf.gzwget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.long_noncoding_RNAs.gtf.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.polyAs.gtf.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.annotation.gtf.gz ## https://www.gencodegenes.org/releases/25lift37.html wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.annotation.gtf.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.metadata.HGNC.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.metadata.EntrezGene.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.metadata.RefSeq.gz zcat gencode.v25.long_noncoding_RNAs.gtf.gz |perl -alne '&#123;next unless $F[2] eq "gene" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;lncRNA.hg38.positionzcat gencode.v25.2wayconspseudos.gtf.gz |perl -alne '&#123;next unless $F[2] eq "transcript" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;pseudos.hg38.positionzcat gencode.v25.annotation.gtf.gz| grep protein_coding |perl -alne '&#123;next unless $F[2] eq "gene" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;protein_coding.hg38.positionzcat gencode.v25.annotation.gtf.gz|perl -alne '&#123;next unless $F[2] eq "gene" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;allGene.hg38.position zcat gencode.v25lift37.annotation.gtf.gz | grep protein_coding |perl -alne '&#123;next unless $F[2] eq "gene" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;protein_coding.hg19.positionzcat gencode.v25lift37.annotation.gtf.gz | perl -alne '&#123;next unless $F[2] eq "gene" ;/gene_name \"(.*?)\";/; print "$F[0]\t$F[3]\t$F[4]\t$1" &#125;' &gt;allGene.hg19.position 有个很严重的问题，gencode里面的数据有着HAVANA和ENSEMBL的区别，尤其是在hg38里面，需要区别对待！具体见：http://www.bio-info-trainee.com/1991.html 的解释. Call 感兴趣的基因 variation原文 12345678910grep H3F3A /data/reference/gtf/gencode/protein_coding.hg19.positionsamtools mpileup -r chr1:226249552-226259702 -ugf /data/reference/genome/hg19/hg19.fa *sorted.bam | bcftools call -vmO z -o H3F3A.vcf.gzgunzip H3F3A.vcf.gz/data/biosoft/annovar/convert2annovar.pl -format vcf4old H3F3A.vcf &gt;H3F3A.annovar/data/biosoft/annovar/annotate_variation.pl -buildver hg19 --geneanno --outfile H3F3A.anno H3F3A.annovar /data/biosoft/annovar/humandb/#/data/biosoft/annovar/annotate_variation.pl --downdb refGene /data/biosoft/annovar/humandb/data/biosoft/annovar/annotate_variation.pl -buildver hg19 --dbtype refGene --geneanno --outfile H3F3A.anno H3F3A.annovar /data/biosoft/annovar/humandb/ 获取感兴趣的基因坐标12345678grep H3F3A /data/reference/gtf/gencode/protein_coding.hg19.position &gt;&gt; key.list.txtgrep HLA-DQ /data/reference/gtf/gencode/protein_coding.hg19.position &gt;&gt; key.list.txtgrep AVPR1 /data/reference/gtf/gencode/protein_coding.hg19.position &gt;&gt; key.list.txtgrep IRX3 /data/reference/gtf/gencode/protein_coding.hg19.position &gt;&gt; key.list.txtgrep IRX5 /data/reference/gtf/gencode/protein_coding.hg19.position &gt;&gt; key.list.txtgrep GLI3 /data/reference/gtf/gencode/protein_coding.hg19.position &gt;&gt; key.list.txtgrep PAX1 /data/reference/gtf/gencode/protein_coding.hg19.position &gt;&gt; key.list.txtgrep RUNX2 /data/reference/gtf/gencode/protein_coding.hg19.position &gt;&gt; key.list.txt 批量提取我们感兴趣的基因的变异情况123456789cat key.list.txt |while read id;dochr=$(echo $id |cut -d" " -f 1|sed 's/chr//' )start=$(echo $id |cut -d" " -f 2 )end=$(echo $id |cut -d" " -f 3 )gene=$(echo $id |cut -d" " -f 4 )echo $chr:$start-$end $genesamtools mpileup -r $chr:$start-$end -ugf /data/reference/genome/hg19/hg19.fa KPGP-00001.sorted.merge.bam | bcftools call -vmO z -o $gene.vcf.gzdone 前面我们说到有研究表明STAT4上的rs7574865和HLA-DQ的rs9275319是国人群中乙型肝炎病毒（HBV）相关肝细胞癌（HCC）遗传易感基因，那么我们很容易去dbSNP数据库或者我最近强烈推荐 的Snpedia数据库吐血推荐snpedia数据库，非常丰富的snp信息记录里面找到它的坐标。126 32666295 :Rs9275319--HLA-DQ2 191964633 :Rs7574865--STAT4 然后我检查了我刚才call到的variation文件，12zcat STAT4.vcf.gz |grep -w 191964633 显示为空。zcat HLA-DQ* |grep 32666295 也是空。 哈哈，我完美的错过了这两个易感位点！！！！谢天谢地！！！ bam文件给按照染色体给分割按照染色体（chr1-chr22,chrX,chrY,chrMT） 1bamtools split -in KPGP-00001.sorted.merge.bam -reference 提取未比对的测序数据12345678910111213samtools view -f4 KPGP-00001.sorted.merge.bam &gt; KPGP-00001.sorted.merge.unmapped.sam #more fast, or# samtools view sample.bam |perl -alne '&#123;print if $F[2] eq "*" or $F[5] eq "*" &#125;' &gt; sample.unmapped.sam#提前未比对成功的测序数据(可以分成3类，仅reads1，仅reads2，和两端reads都没有比对成功)#小写的f是提取，大写的F是过滤samtools view -u -f 4 -F 264 alignments.bam &gt; tmps1.bam #仅reads1samtools view -u -f 8 -F 260 alignments.bam &gt; tmps2.bam #仅reads2samtools view -u -f 12 -F 256 alignments.bam &gt; tmps3.bam #两端reads都没有比对成功samtools merge -u - tmps[123].bam | samtools sort -n - unmappedbamToFastq -bam unmapped.bam -fq1 unmapped_reads1.fastq -fq2 unmapped_reads2.fastq#统计一下未比对成功的reads有多少cut -f 3,6 KPGP-00001.sorted.merge.unmapped.sam |sort |uniq -c &gt;KPGP-00001.sorted.merge.unmapped.counts 提取左右端测序数据比对到不同染色体的PE reads左右端测序数据比对到不同染色体的情况，比较有意义，可能是融合基因，也可能是基因之间本来就相似性很大。三种具有代表性的肿瘤融合基因BCR-ABL、SLC45A3-ELK4 和PAX3-FOXO1 融合基因（英语：Fusion gene）是指两个基因的全部或一部分的序列相互融合为一个新的基因的过程。其有可能是染色体易位、中间缺失或染色体倒置所致的结果. 12samtools view KPGP-00001.sorted.merge.bam | perl -alne '&#123;print if $F[6] ne "="&#125;' &gt;unpaired.sam cut -f 3,7 unpaired.sam |sort |uniq -c PCR duplicationhttp://blog.sina.com.cn/s/blog_69e75efd0102wu57.html 设一个基因组有A、B两个片段，PCR后得到无论多少条reads，比如nA+mB条，在数据分析的时候，都只保留1条A和1条B（unique reads）用于组装，而去掉(n-1)条A和(m-1)条B。共有(n-1)条A和(m-1)条B被当成duplicatedreads看待，尽管它们是正常PCR的正常产物。 那么为什么要去除这个duplication呢？主要是因为在call snp的时候，如果某个变异位点的变异碱基都是来自于PCR重复，而我们却认为它深度足够判断是真的变异位点，这个结论其实有很大可能是假阳性。 覆盖度详细探究1234567891011samtools flagstat KPGP-00001.sorted.merge.bam#全基因组samtools mpileup KPGP-00001.sorted.merge.bam |perl -alne '&#123;if($F[3]&gt;100)&#123;$depth&#123;"over100"&#125;++&#125;else&#123;$depth&#123;$F[3]&#125;++&#125;&#125;END&#123;print "$_\t$depth&#123;$_&#125;" foreach sort&#123;$a &lt;=&gt; $b&#125;keys %depth&#125;' #每一条染色体ls KPGP-00001.sorted.merge.REF*.bam |while read id do echo $idsamtools mpileup $id |perl -alne '&#123;if($F[3]&gt;100)&#123;$depth&#123;"over100"&#125;++&#125;else&#123;$depth&#123;$F[3]&#125;++&#125;&#125; END &#123;print "$_\t$depth&#123;$_&#125;" foreach sort&#123;$a &lt;=&gt; $b&#125;keys %depth&#125;' &gt;$id.depth.txtdone 对比对结果文件进行过滤这个地方有问题 123456789samtools view -h -F4 -q 5 KPGP-00001.sorted.merge.bam |samtools view -bS |samtools rmdup - KPGP-00001.filter.rmdup.bamsamtools index KPGP-00001.filter.rmdup.bambam文件是二进制文件，我们需要samtools view的命令进行格式转换。这个管道的意思分开来说就是运行第一步时过滤的时候已将bam文件转成我们能看的sam格式。其中h是在输出的结果中包含头header，F和q是过滤掉没有mapped上的reads（也就是multiple mapping的情况）和低质量的reads。第二步是将上一步得出的sam文件再转成bam，第三步就是用samtools rmdup过滤掉PCR duplication的情况了，最后得到了过滤了multiple mapping、PCR duplication及低质量比对的bam文件。最后利用samtools index对过滤后的bam文件建立索引。 用GATK对SAM格式的文件进行重排GATK使用注意事项 https://github.com/snewhouse/ngs_nextflow/wiki/GATK-Bundle https://github.com/ddcap/halvade/wiki 12345678910111213141516171819202122# ftp://ftp.broadinstitute.org/bundle/hg19axel ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.gzwget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.fai.gzgunzip ucsc.hg19.fasta.gzbwa index ucsc.hg19.fasta# ucsc.hg19.fasta.amb# ucsc.hg19.fasta.ann# ucsc.hg19.fasta.bwt# ucsc.hg19.fasta.pac# ucsc.hg19.fasta.saaxel ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/1000G_phase1.indels.hg19.sites.vcf.gzaxel ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/1000G_phase1.indels.hg19.sites.vcf.idx.gzaxel ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.gzaxel ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.idx.gzaxel ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.dict.gzaxel ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/dbsnp_138.hg19.vcf.gzls *.gz |while read id;dogunzip $iddone 按染色体下载hg19基因组1234567891011121314for i in $(seq 1 22) X Y M;do echo $i;wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/chr$&#123;i&#125;.fa.gz;donegunzip *.gzfor i in $(seq 1 22) X Y M;do cat chr$&#123;i&#125;.fa &gt;&gt; hg19.fasta;donerm -fr chr*.fastabwa index ucsc.hg19.fastabwa index -a bwtsw hg19.fastasamtools faidx hg19.fasta 首先用RealignerTargetCreator找到需要重新比对的区域，输出文件intervals，然后用输出的 tmp.intervals 做输入文件来进行重新比对，也就是用IndelRealigner在这些区域内进行重新比对. 12345678910nohup java -Xmx60g -jar /data/biosoft/GATK/GenomeAnalysisTK.jar -R \/data/reference/annotation/GATK/ucsc.hg19.fasta -T RealignerTargetCreator \-I KPGP-00001.filter.rmdup.bam -o KPGP-00001.filter.rmdup.intervals \-known /data/reference/annotation/GATK/1000G_phase1.indels.hg19.sites.vcf \1&gt; KPGP-00001.filter.rmdup.RealignerTargetCreator.lognohup java -Xmx60g -jar /data/biosoft/GATK/GenomeAnalysisTK.jar -R \/data/reference/annotation/GATK/ucsc.hg19.fasta -T IndelRealigner \-I KPGP-00001.filter.rmdup.bam -targetIntervals KPGP-00001.filter.rmdup.intervals \-o KPGP-00001.filter.rmdup.realgn.bam 1&gt;KPGP-00001.filter.rmdup.IndelRealigner.log]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WES分析]]></title>
    <url>%2Fhexo%2F2017%2F11%2F07%2F2017-11-07_WES_Seq%2F</url>
    <content type="text"><![CDATA[给学徒的WES数据分析流程 这个教程没有介绍缺少分析数据下载B站视频 1234conda install sra-toolsconda install samtools gatk4conda install -y bcftools vcftools snpeffconda install -y multiqc qualimap 1234567891011121314151617181920212223242526#下载GATK Bundle 依赖文件#https://software.broadinstitute.org/gatk/download/bundle#https://github.com/snewhouse/ngs_nextflow/wiki/GATK-Bundle231M Jul 2 05:14 1000G_phase1.indels.hg19.sites.vcf1.2M Jul 2 10:45 1000G_phase1.indels.hg19.sites.vcf.idx11G Jul 2 08:05 dbsnp_138.hg19.vcf2.5K Jul 1 04:31 hg19.dict3.0G Jun 30 21:29 hg19.fasta6.6K Jun 30 22:54 hg19.fasta.amb944 Jun 30 22:54 hg19.fasta.ann2.9G Jun 30 22:54 hg19.fasta.bwt788 Jul 2 01:53 hg19.fasta.fai739M Jun 30 22:54 hg19.fasta.pac1.5G Jun 30 23:23 hg19.fasta.sa87M Jul 2 05:37 Mills_and_1000G_gold_standard.indels.hg19.sites.vcf2.3M Jul 2 10:45 Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.idx#例如：wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/1000G_phase1.indels.hg19.sites.vcf.gzftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/1000G_phase1.indels.hg19.sites.vcf.idx.gzftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/dbsnp_138.hg19.vcf.gzftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.dict.gzftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.gz/media/ubuntu/Data2/Reference/index/GATK 学习提纲： （1）肿瘤全外显子测序数据分析流程 微信连接 (2) WES分析七步走 (3) WES 测序质量控制 (4) WES（二）snp-calling (5) WES（三）snp-filter (6) WES（四）不同个体的比较 (7) WES（五）不同软件比较 (8) WES（六）用annovar注释 (9) WES（七）看de novo变异情况 (10) GATK流程 (11) GATK使用注意事项 WES数据分析步骤：A Survey of Computational Tools to Analyze and Interpret Whole Exome Sequencing Data(2016) 数据下载Whole-exome sequencing identifies MST1R as a genetic susceptibility gene in nasopharyngeal carcinoma We sequenced the blood samples from 161 NPC cases, including 39 EAO cases, 63 FH+ cases from 52 independent families, and 59 sporadic cases by WES and achieved an average coverage of 49-fold on target (range of 32- to 76-fold). An additional 2,160 NPC cases and 2,433 healthy controls from Hong Kong were further examined for the selected candidate variants. GEO Accession ID SRA291701 挑选5个样本进行分析 npc.sra.txt12345678910SRX445405 MALE NPC15 SRR1139956 NPC15F NO SRS540548 NPC15F-TSRX445406 MALE NPC15 SRR1139958 NPC15F NO SRS540549 NPC15F-NSRX445407 MALE NPC29 SRR1139966 NPC29F YES SRS540550 NPC29F-TSRX445408 MALE NPC29 SRR1139973 NPC29F YES SRS540551 NPC29F-NSRX445409 FEMALE NPC10 SRR1139999 NPC10F NO SRS540552 NPC10F-TSRX445410 FEMALE NPC10 SRR1140007 NPC10F NO SRS540553 NPC10F-NSRX445411 FEMALE NPC34 SRR1140015 NPC34F NO SRS540554 NPC34F-TSRX445412 FEMALE NPC34 SRR1140023 NPC34F NO SRS540555 NPC34F-NSRX445413 MALE NPC37 SRR1140044 NPC37F YES SRS540556 NPC37F-TSRX445414 MALE NPC37 SRR1140045 NPC37F YES SRS540557 NPC37F-N 12345cat npc.sra.txt | cut -f 4| while read id# or cut -f 4 npc.sra.txt | while read iddo echo $idaxel ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP035/SRP035573/$id/$id.sra &amp;done 123456cat npc.sra.txt | while read iddoarray=($id)echo $&#123;array[3]&#125;.sra $&#123;array[7]&#125; fastq-dump --gzip --split-3 -A \ $&#123;array[7]&#125; $&#123;array[3]&#125;.sra done 刚刚开始没搞清楚，这个太难了，学了好像暂时也没啥好用的，数据、资料收集到这里，以后有空再学习吧，数据太大了，浪费空间和电费了。]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WGS,WES,RNA-seq组与ChIP-seq之间的异同]]></title>
    <url>%2Fhexo%2F2017%2F11%2F07%2F2017-11-07_WES_RNASEQ_CHIPSEQ%2F</url>
    <content type="text"><![CDATA[教程 测序的几个概念de-novo 测序没有参考基因组情况下对新物种测序构建参考基因组 DNA Re-sequencing相对于de-novo而言在有参考基因组的条件下进行较低覆盖度的测序，与参考基因组比对，寻找相似品种之间的差异。]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chip-Seq 分析]]></title>
    <url>%2Fhexo%2F2017%2F11%2F07%2F2017-11-07_ChIP_seq%2F</url>
    <content type="text"><![CDATA[数据获取 将蛋白交联到DNA上。 也就是保证蛋白和DNA能够结合，找到互作位点。 通过超声波剪切DNA链。 加上附上抗体的磁珠用于免疫沉淀靶蛋白。抗体很重要 接触蛋白交联；纯化DNA 测序。 分析流程： 质量控制, 用到的是FastQC 序列比对, Bowtie2或这BWA peak calling，此处用的MACS peak注释, 这里用的Y叔的ChIPseeker 得到一个物种所有基因的TSS(转录起始位点)区域的bed文件https://genome.ucsc.edu/cgi-bin/hgTables(参阅http://www.bio-info-trainee.com/2136.html）图片中有个错误，要选择output format 为bed 类型 123CHIPSeq TSS（转录起始位点）信号强度热图.- 横坐标：TSS 位点上下游（如10kb）坐标- 纵坐标：每个基因的在不同位点的信号强度，上面曲线是所有基因信号强度叠加 ==================================================== 给学徒ChIP-seq数据处理流程 对应的B站视频教程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188#视频代码---------------------------------------------##数据下载 或者根据ftp地址迅雷下载 国内下载很慢，不能按照教程的方法for i in ` seq 32 50`;donohup axel ftp://ftp.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR391/SRR3910$&#123;i&#125;/SRR3910$&#123;i&#125;.sra &amp; done---------------------------------------------wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-latest-Linux-x86_64.shsudo sh Miniconda3-latest-Linux-x86_64.shconda create -n chipseq python=2 bwaconda info --envssource activate chipseq# 可以用search先进行检索conda search trim-galore## 保证所有的软件都是安装在 chipseq 这个环境下面conda install -y sra-tools conda install -y trim-galore samtoolsconda install -y deeptools homer memeconda install -y macs2 bowtie bowtie2 conda install -c bioconda multiqc##制作config文件，或者写代码 或者直接用excelcut -f 4,7 sra.table |cut -d":" -f 2 |sed 's/ChIPSeq//g' | sed 's/MockIP//g'|sed 's/^ //' |tr ' ' '_' |perl -alne '&#123;$h&#123;$F[0]&#125;++ if exists $h&#123;$F[0]&#125;; $h&#123;$F[0]&#125;=1 unless exists $h&#123;$F[0]&#125;;print "$F[0]$h&#123;$F[0]&#125;\t$F[1]"&#125;' &gt; config ##fastq-dump从SRA文件中提取fastq文件 ##fastq-dump是sratoolkit软件中的一个功能，##fastq-dump -h （查看是否添加完成）dump=fastq-dumpanalysis_dir=raw## 下面用到的 config 文件，就是上面自行制作的。cat config|while read id;do echo $idarr=($id)srr=$&#123;arr[1]&#125;sample=$&#123;arr[0]&#125;# 单端测序数据的sra转fastqnohup $dump -A $sample -O $analysis_dir --gzip --split-3 sra/$srr.sra &amp; done #Trim Galore是对FastQC和Cutadapt的包装。适用于所有高通量测序，包括RRBS(Reduced Representation #Bisulfite-Seq ), Illumina、Nextera 和smallRNA测序平台的双端和单端数据。主要功能包括两步：#第一步首先去除低质量碱基，然后去除3' 末端的adapter, 如果没有指定具体的adapter，程序会自动检测前#1million的序列，然后对比前12-13bp的序列是否符合以下类型的adapter.##这个时候选择trim_galore软件进行过滤，单端测序数据的代码如下；ls ./raw/*gz | while read fq1;do nohup trim_galore -q 25 --phred33 --length 25 -e 0.1 --stringency 4 -o ./clean $fq1 &amp; done ##走一波QCcd ~/project/epi/qc## 相对目录需要理解ls ../raw/*gz|xargs fastqc -t 10 -o ./ls ../clean/*gz|xargs fastqc -t 10 -o ./# multiqc 对所有qc结果综合一下multiqc ./#构建索引（不建议）nohup time bowtie2-build /data/reference/mm10/mm10.fa /data/reference/index/bowtie/mm10 1&gt;mm10.bowtie_index.log 2&gt;&amp;1 &amp;## 直接下载索引文件，下载参考基因组索引大小为3.2GB$ mkdir referece &amp;&amp; cd reference## 断点续传，后台下载wget -c -b ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/mm10.zip##开始比对过程cd align# 这个地方搞了好几天都不知道为什么错。原来是存放bowtie2 index后面要带mm10参数才行，比如 ~/bowtie/mm10 是存放mm10索引的目录，在~/bowtie/mm10还要加/mm10才能保证正确的索引到bowtie2_index=/data/reference/index/bowtie/mm10/mm10ls ../clean/*gz |while read iddo file=$(basename $id )sample=$&#123;file%%.*&#125;echo $file $sample## 比对过程3分钟一个样本bowtie2 -p 5 -x $bowtie2_index -U $id | samtools sort -O bam -@ 5 -o -&gt; $&#123;sample&#125;.bam done #对bam文件进行QCcd alignls *.bam |xargs -i samtools index &#123;&#125; #构建indexls *.bam | while read id ;do (nohup samtools flagstat $id &gt; $(basename $id ".bam").stat &amp; );done #生成stat文件，可以在这个文件中查看质控#cat *stat|grep 'N/A'|grep '%'grep 'N/A' *.stat|grep '%'##合并bam文件，因为一个样品分成了多个lane进行测序，所以在进行peaks calling的时候，需要把bam进行合并。## 如果不用循环 samtools merge control.merge.bam Control_1_trimmed.bam Control_2_trimmed.bam## 通常我们用批处理。cd alignmkdir ../mergeBamls *.bam|sed 's/_[0-9]_trimmed.bam//g' |sort -u |while read id;do samtools merge ../mergeBam/$id.merge.bam $id*.bam ;done##去除PCR重复conda activate chipseqcd mergeBamls *merge.bam | while read id ;do (nohup samtools markdup -r $id $(basename $id ".bam").rmdup.bam &amp; );donels *.rmdup.bam |xargs -i samtools index &#123;&#125; ls *.rmdup.bam | while read id ;do (nohup samtools flagstat $id &gt; $(basename $id ".bam").stat &amp; );done##使用macs2进行找peaks## macs2参数参考上面网页教程cd mergeBam conda activate chipseqls *merge.bam |cut -d"." -f 1 |while read id;do if [ ! -s $&#123;id&#125;_summits.bed ]; then echo $id nohup macs2 callpeak -c Control.merge.bam -t $id.merge.bam -f BAM -B -g mm -n $id --outdir ../peaks 2&gt; $id.log &amp; fi done mkdir dupmv *rmdup* dup/cd dup/ls *.merge.rmdup.bam |cut -d"." -f 1 |while read id;do if [ ! -s $&#123;id&#125;_rmdup_summits.bed ]; then echo $id nohup macs2 callpeak -c Control.merge.rmdup.bam -t $id.merge.rmdup.bam -f BAM -B -g mm -n $&#123;id&#125;_rmdup --outdir ../peaks 2&gt; $id.log &amp; fi done ##使用deeptool是进行可视化#首先把bam文件转为bw文件cd mergeBam ls *.bam |xargs -i samtools index &#123;&#125; ls *.bam |while read id;donohup bamCoverage --normalizeUsing CPM -b $id -o $&#123;id%%.*&#125;.bw &amp; done cd dup ls *.bam |xargs -i samtools index &#123;&#125; ls *.bam |while read id;donohup bamCoverage --normalizeUsing CPM -b $id -o $&#123;id%%.*&#125;.rm.bw &amp; done #查看tss（转录起始位置）附近的信号强度mkdir tsscd tssbed=/data/reference/annotation/CHIPSeq/mm10/ucsc_mm10_refSeq.bedfor id in /data/DataSet_Experiment/ChIPSeq/chipseq_jimmy/mergeBam/*bw ;do echo $idfile=$(basename $id )sample=$&#123;file%%.*&#125; echo $sample computeMatrix reference-point --referencePoint TSS -p 15 \-b 10000 -a 10000 \-R $bed \-S $id \--skipZeros -o matrix1_$&#123;sample&#125;_TSS_10K.gz \--outFileSortedRegions regions1_$&#123;sample&#125;_TSS_10K.bed# 输出的gz为文件用于plotHeatmap, plotProfile## both plotHeatmap and plotProfile will use the output from computeMatrixplotHeatmap -m matrix1_$&#123;sample&#125;_TSS_10K.gz -out $&#123;sample&#125;_Heatmap_10K.pngplotHeatmap -m matrix1_$&#123;sample&#125;_TSS_10K.gz -out $&#123;sample&#125;_Heatmap_10K.pdf --plotFileFormat pdf --dpi 720 plotProfile -m matrix1_$&#123;sample&#125;_TSS_10K.gz -out $&#123;sample&#125;_Profile_10K.pngplotProfile -m matrix1_$&#123;sample&#125;_TSS_10K.gz -out $&#123;sample&#125;_Profile_10K.pdf --plotFileFormat pdf --perGroup --dpi 720 done #使用命令批量提交：nohup bash 10k.sh 1&gt;10k.log &amp;bed=/data/reference/annotation/CHIPSeq/mm10/ucsc_mm10_refSeq.bedfor id in /data/DataSet_Experiment/ChIPSeq/chipseq_jimmy/mergeBam/*bw ;do echo $idfile=$(basename $id )sample=$&#123;file%%.*&#125; echo $sample computeMatrix reference-point --referencePoint TSS -p 15 \-b 2000 -a 2000 \-R $bed \-S $id \--skipZeros -o matrix1_$&#123;sample&#125;_TSS_2K.gz \--outFileSortedRegions regions1_$&#123;sample&#125;_TSS_2K.bed## both plotHeatmap and plotProfile will use the output from computeMatrixplotHeatmap -m matrix1_$&#123;sample&#125;_TSS_2K.gz -out $&#123;sample&#125;_Heatmap_2K.pngplotHeatmap -m matrix1_$&#123;sample&#125;_TSS_2K.gz -out $&#123;sample&#125;_Heatmap_2K.pdf --plotFileFormat pdf --dpi 720 plotProfile -m matrix1_$&#123;sample&#125;_TSS_2K.gz -out $&#123;sample&#125;_Profile_2K.pngplotProfile -m matrix1_$&#123;sample&#125;_TSS_2K.gz -out $&#123;sample&#125;_Profile_2K.pdf --plotFileFormat pdf --perGroup --dpi 720 done #使用命令批量提交：nohup bash 2k.sh 1&gt;2k.log &amp; 广告： 如何使用deeptools处理BAM数据 使用R包对找到的peaks文件进行注释12345678910111213141516171819202122232425262728293031323334353637383940install.packages("devtools")library(devtools) #选择安装源local(&#123; options( repos = "https://mirrors.ustc.edu.cn/CRAN/" ) options( BioC_mirror = "https://mirrors.ustc.edu.cn/bioc/" )&#125;)# Step2 List of the used packages ----------------------------------------##bash sudo apt-get install libcurl4-gnutls-dev ##不然RCurl包安装不了sudo apt-get install libssl-dev##R 3.5install.packages("RCurl")bioPackages &lt;- c('airway','DESeq2','edgeR','limma','ChIPpeakAnno','ChIPseeker' 'TxDb.Hsapiens.UCSC.hg19.knownGene', 'TxDb.Hsapiens.UCSC.hg38.knownGene', 'TxDb.Mmusculus.UCSC.mm10.knownGene')install.packages("BiocManager")BiocManager::install(bioPackages, update = TRUE, ask = FALSE)library(TxDb.Hsapiens.UCSC.hg19.knownGene) library(TxDb.Mmusculus.UCSC.mm10.knownGene) library(TxDb.Hsapiens.UCSC.hg38.knownGene) library(ChIPpeakAnno) library(ChIPseeker) bedPeaksFile = 'H3K36me3_summits.bed'#以这一个bed文件为例bedPeaksFile## loading packagesrequire(ChIPseeker)require(TxDb.Mmusculus.UCSC.mm10.knownGene)txdb &lt;- TxDb.Mmusculus.UCSC.mm10.knownGenerequire(clusterProfiler) peak &lt;- readPeakFile( bedPeaksFile ) keepChr= !grepl('_',seqlevels(peak))seqlevels(peak, pruning.mode="coarse") &lt;- seqlevels(peak)[keepChr]peakAnno &lt;- annotatePeak(peak, tssRegion=c(-3000, 3000), TxDb=txdb, annoDb="org.Mm.eg.db") peakAnno_df &lt;- as.data.frame(peakAnno) peaks相关基因集的注释都是得到感兴趣基因集，然后注释，分析方法等同于GEO数据挖掘课程或者转录组下游分析： https://github.com/jmzeng1314/GEO （有配套视频） homer软件来寻找motifhomer软件找motif整合了两个方法，包括依赖于数据库的查询，和de novo的推断,都是读取ChIP-seq数据上游分析得到的bed格式的peaks文件。 使用起来很简单：http://homer.ucsd.edu/homer/ngs/peakMotifs.html 123456789101112131415161718192021222324252627##下载 mm10注释###!!!!Genome mm10 not found in /soft/miniconda3/envs/chipseq/share/homer-4.10-0/.//config.txt## To check if is available, run "perl /soft/miniconda3/envs/chipseq/share/homer-4.10-0/.//configureHomer.pl -list"## If so, add it by typing "perl /soft/miniconda3/envs/chipseq/share/homer-4.10-0/.//configureHomer.pl -install mm10"#首先用homer自带的perl配置程序安装小鼠基因组perl /soft/miniconda3/envs/chipseq/share/homer-4.10-0/configureHomer.pl -install mm10du -h /soft/miniconda3/envs/chipseq/share/homer-4.10-0/data/ #查看#homer软件找motif整合了两个方法，包括依赖于数据库的查询，和de novo的推断,都是读取ChIP-seq数据上游分析得到的bed格式的peaks文件。# http://homer.ucsd.edu/homer/ngs/peakMotifs.htmlcd motif for id in ../peaks/*.bed;doecho $idfile=$(basename $id )sample=$&#123;file%%.*&#125; echo $sample awk '&#123;print $4"\t"$1"\t"$2"\t"$3"\t+"&#125;' $id &gt;homer_peaks.tmp ##homer软件要求提供这样的文件格式findMotifsGenome.pl homer_peaks.tmp mm10 $&#123;sample&#125;_motifDir -len 8,10,12annotatePeaks.pl homer_peaks.tmp mm10 1&gt;$&#123;sample&#125;.peakAnn.xls 2&gt;$&#123;sample&#125;.annLog.txt done #把上面的代码保存为脚本runMotif.sh，然后运行：nohup bash runMotif.sh 1&gt;motif.log &amp;不仅仅找了motif，还顺便把peaks注释了一下。得到的后缀为peakAnn.xls 的文件就可以看到和使用R包注释的结果是差不多的。还可以使用在线工具 meme来找motif，需要通过bed格式的peaks的坐标来获取fasta序列。MEME，链接：http://meme-suite.org/ =================================================================== 另外一篇文章 学习提纲 笔记 一篇文章学会ChIP-seq分析1 一篇文章学会ChIP-seq分析2 ChIPseq从入门到放弃 1.基础知识 peak calling的统计学原理 http://www.plob.org/2014/05/08/7227.html 根据比对的bam文件来对peaks区域可视化 http://www.bio-info-trainee.com/1843.html wig、bigWig和bedgraph文件详解 http://www.bio-info-trainee.com/1815.html 【综述】ChIP-seq guidelines and practices of the ENCODE and modENCODE consortia. https://www.ncbi.nlm.nih.gov/pubmed/22955991 2.Chip-Seq测序的统计学原理TF在基因组上的结合其实是一个随机过程，基因组的每个位置其实都有机会结合某个TF，只是概率不一样，说白了，peak出现的位置，是TF结合的热点，而peak-calling就是为了找到这些热点。 如何定义热点呢？通俗地讲，热点是这样一些位置，这些位置多次被测得的read所覆盖（我们测的是一个细胞群体，read出现次数多，说明该位置被TF结合的几率大）。那么，read数达到多少才叫多？这就要用到统计检验喽。假设TF在基因组上的分布是没有任何规律的，那么，测序得到的read在基因组上的分布也必然是随机的，某个碱基上覆盖的read的数目应该服从二项分布。这其实和高中大学课本上抽小球的过程是类似的。当n很大，p很小时，二项分布可以近似用泊松分布替代。 \lambda=n \ast p, p=\frac{l}{s}\(\lambda\)是泊松分布唯一的参数，n是测序得到的read总数目，l是单个read的长度，s是基因组的大小。有了分布，我们可以算出在某个置信概率（如0.00001）下，随机情况下，某个碱基上可以覆盖的read的数目的最小值，当实际观察到的read数目超过这个值（单侧检验）时，我们认为该碱基是TF的一个结合热点。反过来，针对每一个read数目，我们也可以算出对应的置信概率P。但是，这只是一个简化的模型，实际情况要复杂好多。比如，由于测序、mapping过程内在的偏好性，以及不同染色质间的差异性，相比全基因组，某些碱基可能内在地会被更多的read所覆盖，这种情况得到的很多peak可能都是假的。MACS考虑到了这一点，当对某个碱基进行假设检验时，MACS只考虑该碱基附近的染色质区段（如10k），此时，上述公式中n表示附近10k区间内的read数目，s被置为10k。当有对照组实验（Control，相比实验组，没有用抗体捕获TF，或用了一个通用抗体）存在时，利用Control组的数据构建泊松分布，当没有Control时，利用实验组，稍大一点的局部区间（比如50k）的数据构建泊松分布。这儿还有一个问题，read只是跟随着TF一起沉淀下来的DNA fragment的末端，read的位置并不是真实的TF结合的位置。所以在peak-calling之前，延伸read是必须的。不同TF大小不一样，对read延伸的长度也理应不同。我们知道，测得的read最终其实会近似地平均分配到正负链上，这样，对于一个TF结合热点而言，read在附近正负链上会近似地形成“双峰”。MACS会以某个window size扫描基因组，统计每个window里面read的富集程度，然后抽取（比如1000个）合适的（read富集程度适中，过少，无法建立模型，过大，可能反映的只是某种偏好性）window作样本，建立“双峰模型”。最后，两个峰之间的距离就被认为是TF的长度D，每个read将延伸D/2的长度。 数据下载RYBP and Cbx7 define specific biological functions of polycomb complexes in mouse embryonic stem cells https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42466 123456for ((i=204;i&lt;=209;i++))doaxel ftp://ftp.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR620/SRR620$i/SRR620$i.sra &amp;donels *sra |while read id; do fastq-dump –split-3 $id &amp; done 注意文件： 这里要注意是双端测序还是单端测序。 fastq-dump 转换sra文件成fastq/fasta 文件。 pair-end: fastq-dump —split-3 *.sra single-end: fastq-dump .sra 或者 fastq-dump —fasta sra 下载参考基因组,建立bowtie2索引索引可以自己build，也可以下载 ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/mm10.zip12345678910mkdir -p /data/reference/genome/mm10cd /data/reference/genome/mm10axel http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/chromFa.tar.gztar zvxf chromFa.tar.gz cat *.fa &gt; mm10.farm chr*.fa mkdir -p /data/reference/index/bowtiecd /data/reference/index/bowtienohup time bowtie2-build /data/reference/genome/mm10/mm10.fa /data/reference/index/bowtie/mm10 1&gt;mm10.bowtie_index.log 2&gt;&amp;1 &amp; 数据处理与分析质控1ls *fastq |xargs fastqc -t 10 发现3端质量有点问题，我就用了-3 5—local参数， 序列拼接bowtie 短序列比对工具，用bowtie2软件把测序得到的fastq文件比对到mm10参考基因组上面1234567cd /data/ChIPSeqbowtie2 -p 6 -3 5 --local -x /data/reference/index/bowtie/mm10 -U SRR620204.fastq | samtools sort -O bam -o ring1B.bambowtie2 -p 6 -3 5 --local -x /data/reference/index/bowtie/mm10 -U SRR620205.fastq | samtools sort -O bam -o cbx7.bambowtie2 -p 6 -3 5 --local -x /data/reference/index/bowtie/mm10 -U SRR620206.fastq | samtools sort -O bam -o suz12.bambowtie2 -p 6 -3 5 --local -x /data/reference/index/bowtie/mm10 -U SRR620207.fastq | samtools sort -O bam -o RYBP.bambowtie2 -p 6 -3 5 --local -x /data/reference/index/bowtie/mm10 -U SRR620208.fastq | samtools sort -O bam -o IgGold.bambowtie2 -p 6 -3 5 --local -x /data/reference/index/bowtie/mm10 -U SRR620209.fastq | samtools sort -O bam -o IgG.bam call peaks1234567##参数解释##-m 建立“双峰模型”用到，默认就算10 30，-p p-value 大于 1e-5，##-f 文件来源是bam格式，-g 基因组大小是小鼠的（代号mm），-n 起名字的话叫 cbx7 nohup macs2 callpeak -c IgGold.bam -t suz12.bam -m 10 30 -p 1e-5 -f BAM -g mm -n suz12 2&gt;suz12.masc2.log &amp;nohup macs2 callpeak -c IgGold.bam -t ring1B.bam -m 10 30 -p 1e-5 -f BAM -g mm -n ring1B 2&gt;ring1B.masc2.log &amp;nohup macs2 callpeak -c IgG.bam -t cbx7.bam -m 10 30 -p 1e-5 -f BAM -g mm -n cbx7 2&gt;cbx7.masc2.log &amp;nohup macs2 callpeak -c IgG.bam -t RYBP.bam -m 10 30 -p 1e-5 -f BAM -g mm -n RYBP 2&gt;RYBP.masc2.log &amp; bam转换成bw文件12345678910ls *.bam |while read id; do samtools index $id $id.bai; donels *bam |while read iddo file=$(basename $id ) sample=$&#123;file%%.*&#125; echo $sample bamCoverage -b $id -o $sample.bw ## 这里有个参数，-p 10 --normalizeUsingRPKM computeMatrix reference-point --referencePoint TSS -b 10000 -a 10000 -R /data/reference/annotation/ChIPSeq/mm10/ucsc.refseq.bed -S $sample.bw --skipZeros -o matrix1_$&#123;sample&#125;_TSS.gz --outFileSortedRegions regions1_$&#123;sample&#125;_genes.bed plotHeatmap -m matrix1_$&#123;sample&#125;_TSS.gz -out $&#123;sample&#125;.pngdone 结果文件不只是上述提到的一类，还有如下格式： NAMEpeaks.xls: 以表格形式存放peak信息，虽然后缀是xls，但其实能用文本编辑器打开，和bed格式类似，但是以1为基，而bed文件是以0为基.也就是说xls的坐标都要减一才是bed文件的坐标 NAMEpeaks.narrowPeak NAMEpeaks.broadPeak 类似。后面4列表示为， integer score for display， fold-change，-log10pvalue，-log10qvalue，relative summit position to peak start。内容和NAMEpeaks.xls基本一致，适合用于导入R进行分析。 NAMEsummits.bed：记录每个peak的peak summits，话句话说就是记录极值点的位置。MACS建议用该文件寻找结合位点的motif。 NAMEmodel.r，能通过$ Rscript NAME_model.r作图，得到是基于你提供数据的peak模型 计算peak数1wc -l *summits.bed 下载原作者的peak数据,可以用于分析结果比较123wget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE42nnn/GSE42466/suppl/GSE42466_RYBP_peaks_5.txt.gzgzip -d GSE42466_RYBP_peaks_5.txt.gzmv GSE42466_RYBP_peaks_5.txt RYBP2_summits.bed 整合所有的chipseq的bam文件，画基因的TSS附近的profile和heatmap图1234computeMatrix reference-point -p 10 --referencePoint TSS -b 2000 -a 2000 -S *bw -R /data/reference/annotation/ChIPSeq/mm10/ucsc.refseq.bed --skipZeros -o tmp4.mat.gzplotHeatmap -m tmp4.mat.gz -out tmp4.merge.pngplotProfile --dpi 720 -m tmp4.mat.gz -out tmp4.profile.pdf --plotFileFormat pdf --perGroupplotHeatmap --dpi 720 -m tmp4.mat.gz -out tmp4.merge.pdf --plotFileFormat pdf 整合所有的chipseq的bam文件，画基因的genebody附近的profile和heatmap图1234computeMatrix scale-regions -p 10 -S *bw -R /data/reference/annotation/ChIPSeq/mm10/ucsc.refseq.bed -b 3000 -a 3000 -m 5000 --skipZeros -o tmp5.mat.gzplotHeatmap -m tmp5.mat.gz -out tmp5.merge.pngplotProfile --dpi 720 -m tmp5.mat.gz -out tmp5.profile.pdf --plotFileFormat pdf --perGroupplotHeatmap --dpi 720 -m tmp5.mat.gz -out tmp5.merge.pdf --plotFileFormat pdf 结果注释与可视化ChIPseeker R package的功能分为三类: 注释：提取peak附近最近的基因， 注释peak所在区域 比较：估计ChIP peak数据集中重叠部分的显著性；整合GEO数据集，以便于将当前结果和已知结果比较 可视化： peak的覆盖情况；TSS区域结合的peak的平均表达谱和热图；基因组注释；TSS距离；peak和基因的重叠。 1234567891011121314151617181920212223source ("https://bioconductor.org/biocLite.R")biocLite("ChIPseeker")biocLite("org.Mm.eg.db")biocLite("TxDb.Mmusculus.UCSC.mm10.knownGene")biocLite("clusterProfiler")biocLite("ReactomePA")biocLite("DOSE")library("ChIPseeker")library("org.Mm.eg.db")library("TxDb.Mmusculus.UCSC.mm10.knownGene")txdb &lt;- TxDb.Mmusculus.UCSC.mm10.knownGenelibrary("clusterProfiler")#读入bed文件ring1B &lt;- readPeakFile("F:/Chip-seq_exercise/ring1B_peaks.narrowPeak")#查看peak在全基因组的位置covplot(ring1B)##全基因组covplot(ring1B,chrs=c("chr17", "chr18")) #指定染色体#Average Profile of ChIP peaks binding to TSS region#(Confidence interval estimated by bootstrap method)plotAvgProf(tagMatrix, xlim=c(-3000, 3000), conf = 0.95, resample = 1000) peak的注释 peak的注释用annotatePeak()函数， TSS (transcription start site) region 可以自己设定，默认是（-3000，3000）， TxDb 是指某个物种的基因组，例如TxDb.Hsapiens.UCSC.hg38.knownGene, TxDb.Hsapiens.UCSC.hg19.knownGene for human genome hg38 and hg19, TxDb.Mmusculus.UCSC.mm10.knownGene and TxDb.Mmusculus.UCSC.mm9.knownGene for mouse mm10 and mm9. 12345678910peakAnno &lt;- annotatePeak(ring1B, tssRegion=c(-3000, 3000),TxDb=txdb, annoDb="org.Mm.eg.db")#可视化 Pie and Bar plotplotAnnoBar(peakAnno)vennpie(peakAnno)upsetplot(peakAnno)#可视化TSS区域的TF binding lociplotDistToTSS(peakAnno,title="Distribution of transcription factor-binding loci\nrelative to TSS") 多个peak的比较 多个peak set注释时，先构建list,然后用lapply.list(name1=bed_file1,name2=bed_file2) RYBP的数据有问题，这里加上去，会一直报错。 12345678910111213141516peaks &lt;- list(cbx7=cbx7,ring1B=ring1B,suz12=suz12)promoter &lt;- getPromoters(TxDb=txdb, upstream=3000, downstream=3000)tagMatrixList &lt;- lapply(peaks, getTagMatrix, windows=promoter)plotAvgProf(tagMatrixList, xlim=c(-3000, 3000))plotAvgProf(tagMatrixList, xlim=c(-3000, 3000), conf=0.95,resample=500, facet="row")tagHeatmap(tagMatrixList, xlim=c(-3000, 3000), color=NULL)#ChIP peak annotation comparisionpeakAnnoList &lt;- lapply(peaks, annotatePeak, TxDb=txdb,tssRegion=c(-3000, 3000), verbose=FALSE)plotAnnoBar(peakAnnoList)plotDistToTSS(peakAnnoList)#Overlap of peaks and annotated genesgenes= lapply(peakAnnoList, function(i) as.data.frame(i)$geneId)vennplot(genes)]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNA-Seq分析 笔记]]></title>
    <url>%2Fhexo%2F2017%2F11%2F07%2F2017-11-07_RNA_Seq_1%2F</url>
    <content type="text"><![CDATA[这是一个生信技能树的优秀作业，一字未改。 原文 转录组差异表达分析小实战（一）读文献获取数据文献名称：AKAP95 regulates splicing through scaffoldingRNAs and RNA processing factors 查找数据：Data availabilityThe RIP-seq an RNA-seq data have been deposited in the GeneExpression Omnibus database, with accession code GSE81916. All other data isavailable from the author upon reasonable request. 获得GSE号：GSE81916 下载测序数据 https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE81916获取数据信息，并点击网址下方的ftp，下载测序数据 从https://trace.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA323422可知我们需要的mRNA测序编号为SRR3589956到SRR3589962 通过Apera下载SRR数据，这里以SRR3589956为例： ascp -T -i /home/anlan/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp-private.ncbi.nlm.nih.gov:sra/sra-instant/reads/ByRun/sra/SRR/SRR358/SRR3589956/SRR3589956.sra ./ 转化fastq测序数据 通过sratoolkit工具将SRR文件转化为fastq格式的测序数据（写了个shell循环） for i in $(seq 56 62);do nohup fastq-dump --split-3 SRR35899${i} &amp;;done 通过fastqc对每个fastq文件进行质检，用multiqc查看整体质检报告（对当前目录下的fastq测序结果进行质检，生成每个fq文件的质检报告总multiqc整合后统计查看） fastqc *.fastq multiqc ./ 点击这个url可以查看我这个multiqc报告：http://www.bioinfo-scrounger.com/data/multiqc_report.html 如果有接头或者质量值不达标的需要进行过滤，这次的数据质量都不错，因此直接进行比对即可 序列比对 安装hisat2软件，下载人类的hiast2索引文件 hisat2下载并安装： ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.1.0-Linux_x86_64.zip unzip hisat2-2.1.0-Linux_x86_64.zip 下载hisat2的human索引 ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/hg19.tar.gz tar zxvf hg19.tar.gz 用hisat2进行比对，测序数据放在data目录下，索引文件放在reference/index/hisat2/hg19目录下，SRR3589956-SRR3589958为人的测序数据 for i in $(seq 56 58);do hisat2 -p 4 \ -x ~/reference/index/hisat2/hg19/genome \ -1 ./data/SRR35899${i}_1.fastq -2 ./data/SRR35899${i}_2.fastq \ -S SRR35899$i.sam &gt;SRR35899${i}.log;done 用samtools将sam文件转化为bam文件，并使用默认排序 for i in $(seq 56 58);do samtools sort -@ 5 -o SRR35899${i}.bam SRR35899${i}.sam;done reads计数 用htseq对比对产生的bam进行count计数 htseq安装，使用miniconda，省事！唯一的问题是htseq版本不是最新的，是0.7.2。想要最新版还是要正常安装，可参考http://www.biotrainee.com/thread-1847-1-2.html conda install -c bioconda htseq 用htseq将对比后的结果进行计数 for i in $(seq 56 58);do htseq-count -f bam -r pos -s no \ SRR35899${i}.bam ~/reference/genome/hg19/gencode.v26lift37.annotation.gtf \ 1&gt;SRR35899${i}.count 2&gt;SRR35899${i}_htseq.log;done 将3个count文件（SRR3589956.count，SRR3589957.count，SRR3589958.count）合并成一个count矩阵，这是就需要脚本来解决这个问题，不然其他方法会稍微麻烦点 #!/usr/bin/perl -w use strict; my $path = shift @ARGV; opendir DIR, $path or die; my @dir = readdir DIR; my $header; my @sample; my %hash; foreach my $file (@dir) { if ($file =~ /^\w+.*\.count/) { push @sample, $file; $header .= &quot;\t$file&quot;; open my $fh, $file or die; while (&lt;$fh&gt;) { chomp; next if ($_ =~ /^\W+/); my @array = split /\t/, $_; $hash{$array[0]} -&gt; {$file} = $array[1]; } close $fh; } } print &quot;$header\n&quot;; map{ my $gene = $_; print &quot;$gene&quot;; foreach my $file (@sample) { print &quot;\t&quot;.$hash{$gene} -&gt; {$file}; } print &quot;\n&quot;; }keys %hash; 按照接下来的剧本，应该讲count_matrix文件导入DESeq进行差异表达分析。但是从这篇文章的Bioinformatic analyses部分可以发现，作者的control组的2组数据是来自2个不同的批次（一个是SRR3589956，另外一个来源GSM1095127 in GSE44976），treat组倒是同一个批次（SRR3589957和SRR3589958）。但是对于Mouse cells来说，倒是满足2个control和2个treat都正常来自同个批次，因此打算重新用SRR3589959-SRR3589962重新做个一个count_matrix进行后续差异分析]]></content>
  </entry>
  <entry>
    <title><![CDATA[RNA-Seq分析]]></title>
    <url>%2Fhexo%2F2017%2F11%2F07%2F2017-11-07_RNA_Seq%2F</url>
    <content type="text"><![CDATA[这是一个学习笔记，跟随生信技能树的学习笔记重复,把几个优秀笔记的内容重复摘录在此。 学习提纲：RNA-seq基础入门传送门 文章链接：https://www.nature.com/articles/ncomms13347 非常棒的学习笔记1：PANDA姐的转录组入门（0-6）合辑 非常棒的学习笔记2:浙大植物学小白的转录组笔记 Link2 非常棒的学习笔记3:下一篇 分析软件安装最方便的安装方式就是 Anaconda 1234567wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh conda install -c bioconda samtools=1.5 conda install -c bioconda htseq=0.7.2 conda install -c bioconda hisat2=2.1.0 conda install -c bioconda fastqc=0.11.5 conda install -c jfear sratoolkit=2.8.1 数据下载From NCBI GEO ftp The RIP-seq an RNA-seq data have been deposited in the Gene Expression Omnibus database, with accession code GSE81916 数据分析质控12345678910111213for id in `seq 56 62`do fastq-dump --gzip --split-3 -O /data/RNASeq -A SRR35899$&#123;id&#125;done ##很慢，建议后台多线程##查看fastq文件zcat SRR3589956_1.fastq.gz | head -n 4##安装集成分析工具conda install -c bioconda multiqc# 先获取QC结果ls *gz | while read id; do fastqc -t 4 $id; done# multiqcmultiqc *fastqc.zip --pdf Python质控脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445import reimport zipfile# read the zip filedef zipReader(file): qcfile = zipfile.ZipFile(file) data_txt = [file for file in qcfile.namelist() if re.match(".*?_data\.txt", file)][0] data = [bytes.decode(line) for line in qcfile.open(data_txt)] return data def fastqc_summary(data): module_num = 0 bases = 0 Q20 = 0 Q30 = 0 for line in data: if re.match('Filename', line): filename = line.split(sep="\t")[1].strip() if re.match('Total Sequence', line): read = line.split(sep="\t")[1].strip() if re.match('%GC', line): GC = line.split(sep="\t")[1].strip() if re.match("[^#](.*?\t)&#123;6&#125;",line): bases = bases + 1 if float(line.split("\t")[1]) &gt; 30: Q20 = Q20 + 1 Q30 = Q30 + 1 elif float(line.split("\t")[1]) &gt; 20: Q20 = Q20 + 1 if re.match("&gt;&gt;END", line) : module_num = module_num + 1 if module_num &gt;= 2: break Q20 = Q20 / bases Q30 = Q30 / bases summary = [filename, read, GC, str(Q20), str(Q30)] return summary if __name__ == '__main__': import sys for arg in range(1, len(sys.argv)): data = zipReader(sys.argv[arg]) summary = fastqc_summary(data) with open('summary.txt', 'a') as f: f.write('\t'.join(summary) + '\n') 12345grep -w 'gene' gencode.v26lift37.annotation.gtf |grep -w 'TP53'|cut -f 1,4,5 &gt;&gt;gene.bedgrep -w 'gene' gencode.v26lift37.annotation.gtf |grep -w 'KRAS'|cut -f 1,4,5 &gt;&gt;gene.bedgrep -w 'gene' gencode.v26lift37.annotation.gtf |grep -w 'EGFR'|cut -f 1,4,5 &gt;&gt;gene.bedbedtools igv -i gene.bed &gt;Bach_sanpshot.txtperl -alne '&#123;print "goto $F[0]:$F[1]-$F[2]\nsnapshot $F[3].png"&#125; ' Hisat2比对HISAT2是TopHat2/Bowti2的继任者，使用改进的BWT算法，实现了更快的速度和更少的资源占用，作者推荐TopHat2/Bowti2和HISAT的用户转换到HISAT2。官网：https://ccb.jhu.edu/software/hisat2/index.shtml 1.建立基因组索引or index 下载12345678910#建立基因组索引#hisat2-build –p 4 genome.fa genome#下载索引cd ~/referencemkdir -p index/hisat &amp;&amp; cd index/hisatwget -c ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/hg19.tar.gzwget -c ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/mm10.tar.gztar zxvf hg19.tar.gztar xvzf mm10.tar.gz 2.比对1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283for i in `seq 56 58`do hisat2 -t -p 24 -x /data/Reference/index/hisat2/hg19/genome \ -1 /data/RNASeq/fastq/SRR35899$&#123;i&#125;_1.fastq.gz \ -2 SRR35899$&#123;i&#125;_2.fastq.gz \ -S /data/RNASeq/fastq/SRR35899$&#123;i&#125;.sam &gt; SRR35899$&#123;i&#125;.log &amp;done##比对结果xts@R710:/data/RNASeq/fastq$ for i in `seq 56 58`&gt; do&gt; hisat2 -t -p 24 -x /data/Reference/index/hisat2/hg19/genome \&gt; -1 /data/RNASeq/fastq/SRR35899$&#123;i&#125;_1.fastq.gz \&gt; -2 SRR35899$&#123;i&#125;_2.fastq.gz \&gt; -S /data/RNASeq/fastq/SRR35899$&#123;i&#125;.sam &gt; SRR35899$&#123;i&#125;.log &amp;&gt; done[1] 11177[2] 11178[3] 11179xts@R710:/data/RNASeq/fastq$ tipTime loading forward index: 00:00:24Time loading forward index: 00:00:24Time loading forward index: 00:00:24Time loading reference: 00:00:04Time loading reference: 00:00:04Time loading reference: 00:00:04Multiseed full-index search: 00:13:2228856780 reads; of these: 28856780 (100.00%) were paired; of these: 1838981 (6.37%) aligned concordantly 0 times 24732654 (85.71%) aligned concordantly exactly 1 time 2285145 (7.92%) aligned concordantly &gt;1 times ---- 1838981 pairs aligned concordantly 0 times; of these: 90927 (4.94%) aligned discordantly 1 time ---- 1748054 pairs aligned 0 times concordantly or discordantly; of these: 3496108 mates make up the pairs; of these: 2034939 (58.21%) aligned 0 times 1221462 (34.94%) aligned exactly 1 time 239707 (6.86%) aligned &gt;1 times96.47% overall alignment rateTime searching: 00:13:26Overall time: 00:13:50Multiseed full-index search: 00:14:4225914821 reads; of these: 25914821 (100.00%) were paired; of these: 1785160 (6.89%) aligned concordantly 0 times 21786672 (84.07%) aligned concordantly exactly 1 time 2342989 (9.04%) aligned concordantly &gt;1 times ---- 1785160 pairs aligned concordantly 0 times; of these: 53455 (2.99%) aligned discordantly 1 time ---- 1731705 pairs aligned 0 times concordantly or discordantly; of these: 3463410 mates make up the pairs; of these: 2187330 (63.16%) aligned 0 times 1050929 (30.34%) aligned exactly 1 time 225151 (6.50%) aligned &gt;1 times95.78% overall alignment rateTime searching: 00:14:46Overall time: 00:15:10[1] 已完成 hisat2 -t -p 24 -x /data/Reference/index/hisat2/hg19/genome -1 /data/RNASeq/fastq/SRR35899$&#123;i&#125;_1.fastq.gz -2 SRR35899$&#123;i&#125;_2.fastq.gz -S /data/RNASeq/fastq/SRR35899$&#123;i&#125;.sam &gt; SRR35899$&#123;i&#125;.log[3]+ 已完成 hisat2 -t -p 24 -x /data/Reference/index/hisat2/hg19/genome -1 /data/RNASeq/fastq/SRR35899$&#123;i&#125;_1.fastq.gz -2 SRR35899$&#123;i&#125;_2.fastq.gz -S /data/RNASeq/fastq/SRR35899$&#123;i&#125;.sam &gt; SRR35899$&#123;i&#125;.logxts@R710:/data/RNASeq/fastq$ Multiseed full-index search: 00:16:0829720636 reads; of these: 29720636 (100.00%) were paired; of these: 1920019 (6.46%) aligned concordantly 0 times 25503958 (85.81%) aligned concordantly exactly 1 time 2296659 (7.73%) aligned concordantly &gt;1 times ---- 1920019 pairs aligned concordantly 0 times; of these: 61683 (3.21%) aligned discordantly 1 time ---- 1858336 pairs aligned 0 times concordantly or discordantly; of these: 3716672 mates make up the pairs; of these: 2292272 (61.68%) aligned 0 times 1196099 (32.18%) aligned exactly 1 time 228301 (6.14%) aligned &gt;1 times96.14% overall alignment rateTime searching: 00:16:12Overall time: 00:16:36 数据转换sam-bam-sorted bam123456for i in `seq 56 58`do samtools view -S SRR35899$&#123;i&#125;.sam -b &gt; SRR35899$&#123;i&#125;.bam samtools sort SRR35899$&#123;i&#125;.bam -o SRR35899$&#123;i&#125;_sorted.bam samtools index SRR35899$&#123;i&#125;_sorted.bamdone SAMtools其他操作 1234567891011121314151617181920212223head -1000 SRR3589957.sam &gt; test.samsamtools view -b test.sam &gt; test.bamsamtools view test.bam | headsamtools sort test.bam -o default.bamsamtools view default.bam | head # Sort alignments by leftmost coordinates, or by read name when -n is usedsamtools sort test.bam defaultsamtools view default.bam | head#提取1号染色体1234-123456区域的比对readsamtools view SRR3589957_sorted.bam chr1:1234-123456 | head#在比如搭配flag(0.1.19版本没有）和flagstat，使用-f或-F参数提取不同匹配情况的read。# 可以先用flagstat看下总体情况samtools flagstat SRR3589957_sorted.bam#筛选恰好配对的read,就需要用0x10samtools view -b -f 0x10 SRR3589957_sorted.bam chr1:1234-123456 &gt; flag.bamsamtools flagstat flag.bam 比对质控 RSeQC——http://rseqc.sourceforge.net/ Qualimap——http://qualimap.bioinfo.cipf.es/ Picard——http://broadinstitute.github.io/picard/ 使用RSeQC来对我们的比对结果进行质控,RSeQC包括了十多个Python脚本，实现很多功能，具体每个脚本的参数用法，都可以在官网学习. 1234567# RSeQC的安装，需要先安装gcc；numpy；R；Python2.7$ pip install RSeQC# 对bam文件进行质控，其余都同样的进行$ bam_stat.py -i SRR3589956_sorted.bam基因组覆盖率的QC需要提供bed文件，可以直接RSeQC的网站下载，或者可以用gtf转换read_distribution.py -i RNA-Seq/aligned/SRR3589956_sorted.bam -r reference/hg19_RefSeq.bed Reads 计数 (From hoptop)定量分为三个水平 基因水平(gene-level) 转录本水平(transcript-level) 外显子使用水平(exon-usage-level)。 1.在基因水平上，常用的软件为HTSeq-count，featureCounts，BEDTools, Qualimap, Rsubread, GenomicRanges等。以常用的HTSeq-count为例，这些工具要解决的问题就是根据read和基因位置的overlap判断这个read到底是谁家的孩子。值得注意的是不同工具对multimapping reads处理方式也是不同的，例如HTSeq-count就直接当它们不存在。而Qualimpa则是一人一份，平均分配。 对每个基因计数之后得到的count matrix再后续的分析中，要注意标准化的问题。如果你要比较同一个样本(within-sample)不同基因之间的表达情况，你就需要考虑到转录本长度，因为转录本越长，那么检测的片段也会更多，直接比较等于让小孩和大人进行赛跑。如果你是比较不同样本（across sample）同一个基因的表达情况，虽然不必在意转录本长度，但是你要考虑到测序深度（sequence depth)，毕竟测序深度越高，检测到的概率越大。除了这两个因素外，你还需要考虑GC%所导致的偏差，以及测序仪器的系统偏差。目前对read count标准化的算法有RPKM（SE）, FPKM（PE），TPM, TMM等，不同算法之间的差异与换算方法已经有文章进行整理和吐槽了。但是，有一些下游分析的软件会要求是输入的count matrix是原始数据，未经标准化，比如说DESeq2，这个时候你需要注意你上一步所用软件会不会进行标准化。 2.在转录本水平上，一般常用工具为Cufflinks和它的继任者StringTie， eXpress。这些软件要处理的难题就时转录本亚型（isoforms）之间通常是有重叠的，当二代测序读长低于转录本长度时，如何进行区分？这些工具大多采用的都是expectation maximization（EM）。好在我们有三代测序。上述软件都是alignment-based，目前许多alignment-free软件，如kallisto, silfish, salmon，能够省去比对这一步，直接得到read count，在运行效率上更高。不过最近一篇文献[1]指出这类方法在估计丰度时存在样本特异性和读长偏差。 3.在外显子使用水平上，其实和基因水平的统计类似。但是值得注意的是为了更好的计数，我们需要提供无重叠的外显子区域的gtf文件。用于分析差异外显子使用的DEXSeq提供了一个Python脚本（dexseq_prepare_annotation.py）执行这个任务。 12345mkdir -p /data/RNASeq/fastq/matrix/for i in $(seq 56 58); do htseq-count -f bam -r pos -s no \/data/RNASeq/fastq/SRR35899$&#123;i&#125;_sorted.bam /data/Reference/gtf/gencode/gencode.v26lift37.annotation.sorted.gtf \1&gt;matrix/SRR35899$&#123;i&#125;.count 2&gt;matrix/SRR35899$&#123;i&#125;_htseq.log &amp;; done 表达矩阵在RNA-Seq分析中，每一个基因就是一个feature（特征？），而基因被认为是它的所有外显子的和集。在可变剪切分析中，可以单独把每个外显子当作一个feature。而在ChIP-Seq分析中，feature则是预先定义的结合域。但是确定一个read到底属于哪一个feature有时会非常棘手。 这段描述很有意思，信息量也很多(From hoptop)我们这次分析是人类mRNA-Seq测序的结果，但是我们其实只下载了3个sra文件。一般而言RNA-Seq数据分析都至少要有2个重复，所以必须要有4个sra文件才行。我在仔细读完文章的方法这一段以后，发现他们有一批数据用的是其他课题组的： For 293 cells, the mRNA-seq results of the control samples include (1) those from the doxycycline-treated parental Flp-In T-REx 293 cells by us and (2) those from the doxycycline-treated control Flp-In T-REx 293 cells performed by another group unrelated to us (sample GSM1095127 in GSE44976)。 然后和Jimmy交流之后，他也承认自己只分析了小鼠的数据，而没有分析人类的数据。所以我们需要根据文章提供的线索下载另外一份数据，才能进行下一步的分析。这个时候就有一个经常被问到的问题：不同来源的RNA-Seq数据能够直接比较吗？甚至说如果不同来源的RNA-seq数据的构建文库都不一样该如何比较?不同来源的RNA-Seq结果之间比较需要考虑 批次效应（batch effect) 的影响。处理批次效应，根据我搜索的结果，是不能使用FPKM/RPKM，关于这个标准化的吐槽，我在biostars上找到了如下观点：FPKM/RPKM 不是标准化的方法，它会引入文库特异的协变量FPKM/RPKM has never been peer-reviewed, it has been introduced as an ad-hoc measure in a supplementary 没有同行评审One of the authors of this paper states, that it should not be used because of faulty arithmetic 作者说算法有问题All reviews so far have shown it to be an inferior scale for DE analysis of genes Length normalization is mostly dispensable imo in DE analysis because gene length is constant有人建议使用一个Bioconductor包http://www.bioconductor.org/packages/devel/bioc/html/sva.html 我没有具体了解，有生之年去了解补充。还有人引用了一篇文献 IVT-seq reveals extreme bias in RNA-sequencing 证明不同文库的RNA-Seq结果会存在很大差异。结论： 可以问下原作者他们是如何处理数据的，居然有一个居然没有重复的分析也能过审。改用小鼠数据进行分析。或者使用无重复的分析方法，或者模拟一份数据出来，先把流程走完。 PANDA实践完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112##1.SRA to fastqperl -F'\t' -alne 'if($F[7]=~/SRR/)&#123;$F[6]=~s/\s/_/g;$F[13]=~ s/\s|#/_/g;$F[13]=~s/\(|\)//g;print "$F[7]\t$F[6]_$F[13]"&#125;' SraRunTable.txt &gt; Rename.txtperl -F'\t' -alne 'print "fastq-dump --split-3 --gzip -A $F[1] $F[0].sra &amp;" ' Rename.txt &gt;sratofq.shsh sratofq.sh md5sum *.fastq.gz &gt;md5sum.txt##2.QCcd MYShenmkdir 1_FastQC_Raw_Datals *.gz|while read id;do(fastqc $id -o 1_FastQC_Raw_Data -t 8);done# 质控统计cd 1_FastQC_Raw_Datafor i in *.zip; do unzip $i; doneperl /data/RNASeq/MYShen/fastqc_table.plcsvtk tab2csv fastqc_table.txt | csvtk csv2md------------------------------------------------------------------------------------------------ |total_reads|GC |Q20 |Q30:-------------------------------------------------|:----------|:--|:----------------|:----------------Homo_sapiens_AKAP95_KD_miR_12_293_cell_1_fastqc |25914821 |50 |0.999810031487387|0.972201583024633Homo_sapiens_AKAP95_KD_miR_12_293_cell_2_fastqc |25914821 |50 |0.977900484051192|0.934933990090072Homo_sapiens_AKAP95_KD_miR_8_293_cell_1_fastqc |29720636 |50 |0.992602101445318|0.966831930304341Homo_sapiens_AKAP95_KD_miR_8_293_cell_2_fastqc |29720636 |50 |0.978714654693123|0.93782410982053Homo_sapiens_Control_293_cell_1_fastqc |28856780 |50 |0.989872946867992|0.966726176148248Homo_sapiens_Control_293_cell_2_fastqc |28856780 |50 |0.977797280223227|0.940547004897982Mus_musculus_E14_cells_Akap95_shRNA_rep1_1_fastqc |52972617 |50 |0.99966391028307 |0.987561698545491Mus_musculus_E14_cells_Akap95_shRNA_rep1_2_fastqc |52972617 |50 |0.991854774481748|0.968082091276275Mus_musculus_E14_cells_Akap95_shRNA_rep2_1_fastqc |43802631 |49 |0.999679725428068|0.972184686419785Mus_musculus_E14_cells_Akap95_shRNA_rep2_2_fastqc |43802631 |49 |0.986887120253892|0.937616464383233Mus_musculus_E14_cells_control_shRNA_rep1_1_fastqc|30468155 |50 |0.99960200376177 |0.987110860654561Mus_musculus_E14_cells_control_shRNA_rep1_2_fastqc|30468155 |50 |0.991083449869222|0.966506018600785Mus_musculus_E14_cells_control_shRNA_rep2_1_fastqc|36763726 |50 |0.999694316126568|0.979622471169646Mus_musculus_E14_cells_control_shRNA_rep2_2_fastqc|36763726 |50 |0.990428680495556|0.951578715079022-------------------------------------------------------------------------------------------------------#截图几个基因的 IGV 可视化结构grep -w 'gene' gencode.v26lift37.annotation.gtf |grep -w 'TP53'|cut -f 1,4,5 &gt;&gt;gene.bedgrep -w 'gene' gencode.v26lift37.annotation.gtf |grep -w 'KRAS'|cut -f 1,4,5 &gt;&gt;gene.bedgrep -w 'gene' gencode.v26lift37.annotation.gtf |grep -w 'EGFR'|cut -f 1,4,5 &gt;&gt;gene.bedbedtools igv -i gene.bed &gt;Bach_sanpshot.txt #perl -alne '&#123;print "goto $F[0]:$F[1]-$F[2]\nsnapshot $F[3].png"&#125; '##3. HISAT2比对---------------------------------------------------------------#map.sh--------------------------------------------------------------------------------------------#! usr/bin/bashset -uset -eset -o pipefailhg19_ref=/data/Reference/index/hisat2/hg19/genomemm10_ref=/data/Reference/index/hisat2/mm10/genomedata_path=/data/RNASeq/MYShenNUM_THREADS=5ls Homo*1.fastq.gz|while read id; \do((hisat2 -t -p $NUM_THREADS -x $hg19_ref -1 $data_path/$&#123;id%_*&#125;_1.fastq.gz -2 \$data_path/$&#123;id%_*&#125;_2.fastq.gz 2&gt;$&#123;id%_*&#125;_map.log | samtools view -b - &gt;$&#123;id%_*&#125;.bam) &amp;);done ls Mus*1.fastq.gz|while read id; \do((hisat2 -t -p $NUM_THREADS -x $mm10_ref -1 $data_path/$&#123;id%_*&#125;_1.fastq.gz -2 \$data_path/$&#123;id%_*&#125;_2.fastq.gz 2&gt;$&#123;id%_*&#125;_map.log | samtools view -b - &gt;$&#123;id%_*&#125;.bam) &amp;);done--------------------------------------------------------------------------------------------- bash map.sh##因为前面开了并行，等前面执行完成，后面再单独执行ls *.bam | while read id;do samtools sort --threads 25 $id -o $&#123;id%.*&#125;_sorted.bam; done ls *_sorted.bam | while read id;do samtools index $id; done## 4.Count reads# 人类mkdir matrixHomo_GTF=/data/Reference/gtf/gencode/gencode.v26lift37.annotation.gtf###count 结果显示基因名称，如果用基因的id号，将 -i gene_name 参数删除即可ls Homo_sapiens*sorted.bam | while read id; do (htseq-count -f bam -r pos -i gene_name -s no \$id $Homo_GTF &gt; matrix/$&#123;id%_*&#125;.count 2&gt; matrix/$&#123;id%_*&#125;.log &amp;); done# 老鼠# 下载 gtf：http://www.gencodegenes.org/mouse_stats/archive.htmlaxel ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_mouse/release_M10/gencode.vM10.annotation.gtf.gzgzip -d gencode.vM10.annotation.gtf.gzMus_GTF=/data/Reference/gtf/gencode/gencode.vM10.annotation.gtfls Mus_musculus*sorted.bam|while read id;do (htseq-count -f bam -r pos -i gene_name -s no \$id $Mus_GTF &gt; matrix/$&#123;id%_*&#125;.count 2&gt; matrix/$&#123;id%_*&#125;.log &amp;);done##另外一些相关的代码## ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_mouse/release_M1/## http://hgdownload-test.cse.ucsc.edu/goldenPath/mm10/liftOver/## GRCm38/mm10 (Dec, 2011) ## ls *bam |while read id;do ( ~/.local/bin/htseq-count -f bam $id genecode/mm9/gencode.vM1.annotation.gtf.gz 1&gt;$&#123;id%%.*&#125;.gene.counts ) ;done ## ls *bam |while read id;do ( ~/.local/bin/htseq-count -f bam -i exon_id $id genecode/mm9/gencode.vM1.annotation.gtf.gz 1&gt;$&#123;id%%.*&#125;.exon.counts ) ;donecd matrix#wc命令的功能为统计指定文件中的字节数-c,字符数-m,字数-w,行数-lwc -l Homo_sapiens*.counthead -n 4 Homo_sapiens*.countperl -lne 'if($ARGV=~/Homo_sapiens_(.*)count/)&#123;print "$1\t$_"&#125;' *|grep -v Homo_sapiens&gt;hg1.count# 先把所有文件进行合并setwd("~/rna_seq/work/matrix")hg &lt;- read.csv(file = "hg.count",header = F,sep = "\t")colnames(hg) &lt;- c('sample','gene','count')library(reshape2)reads &lt;- dcast(hg,formula = gene ~ sample)write.table(reads,file = "hg_join.count",sep = "\t",quote = FALSE,row.names = FALSE)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Markdown+Rmarkdown+Shiny手册]]></title>
    <url>%2Fhexo%2F2017%2F11%2F06%2F2017-11-06_Markdown_Rmarkdown_Shiny%2F</url>
    <content type="text"><![CDATA[Markdown+Rmarkdown+Shiny手册]]></content>
  </entry>
  <entry>
    <title><![CDATA[ENCODE计划数据,千人基因组计划数据,Roadmap计划数据]]></title>
    <url>%2Fhexo%2F2017%2F11%2F06%2F2017-11-06_ENCODE_Data%2F</url>
    <content type="text"><![CDATA[From Jimmy 原文1 From Jimmy 原文2 From Jimmy 原文3 DNA元件百科全书(Encyclopedia of DNA Elements, ENCODE)ENCODE计划的重要性我就不多说了，如果大家还不是很了解，可以直接跳到本文末尾去下载一下ENCODE教程，好好学习。该计划采用以下几种高通量测序技术来刻画了超过100种不同的细胞系或者组织内的全基因组范围内的基因调控元件信息。本来只是针对人类的，后来对mouse以及fly等模式生物也开始测这些数据并进行分析了， 叫做 modENCODE. chromatin structure (5C) open chromatin (DNase-seq and FAIRE-seq) histone modifications and DNA-binding of over 100 transcription factors (ChIP-seq) RNA transcription (RNAseq and CAGE) 目前所有数据均全部公开(http://genome.ucsc.edu/ENCODE/ )，ENCODE results from 2007 and later are available from the ENCODE Project Portal, encodeproject.org. 并以30篇论文在Nature、Science、Cell、JBC、Genome Biol、Genome Research同时发表(http://www.nature.com/encode )。所有数据从raw data形式的原始测序数据到比对后的信号文件以及分析好的有意的peaks文件都可以下载。 我这里根据自己的学习情况，简单介绍一些ENCODE计划数据下载方式，包括ENCODE官网下载,UCSC下载，ENSEMBL下载，broad研究所数据，IHEC存放的数据，还有GEO下载这6种形式！！！ UCSC直接浏览文件，根据文件夹分类及文件名就可以任意方式下载自己感兴趣的数据 http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/ 大家可能会比较习惯用UCSC提供的Genome Browser工具来可视化CHIP-seq的结果，而且Genome Browser里面非常多的选项可以控制各种在线资料是否跟你的数据一起显示来做对比，所以它必然有ftp服务器存放这些数据，其中比较出名的就是ENCODE计划的相关数据啦. ENCODE计划的官网下载https://www.encodeproject.org/pipelines/ 官网的数据下载，做得像是一个购物网站，大家可以根据自己的需求把数据添加到购物篮，然后统一下载。 This document describes what data are available at the ENCODE Portal, ways to get started searching and downloading data, and an overview to how the metadata describing the assays and reagents are organized. ENCODE data can be visualized and accessed from other resources, including the UCSC Genome Browser and ENSEMBL.进入 https://www.encodeproject.org/matrix/?type=Experiment 可以看到里面列出了173种细胞系，148种组织，还有一堆癌症样本的，包括CHIP-seq，DNase-seq等在内的十几种高通量测序数据。 GEOhttp://www.ncbi.nlm.nih.gov/geo/info/ENCODE.html Broad 研究所托管的ENCODE计划数据 http://www.broadinstitute.org/~anshul/projects/encode 原始数据在：http://www.broadinstitute.org/~anshul/projects/encode/rawdata/ iHEChttp://epigenomesportal.ca/ihec/download.html 以文件夹文件的形式直接浏览，根据自己的需求下载即可,除了ENCODE计划的数据，还有Blueprint计划和roadmap计划的数据都可以下载 ENSEMBLhttp://asia.ensembl.org/info/website/tutorials/encode.html 结尾如果你对ENCODE计划不是很了解，可以先看看一些教程：NIH提供的ENCODE计划相关教程： https://www.genome.gov/27553900/encode-tutorials/ https://www.genome.gov/27562350/encode-workshop-april-2015-keystone-symposia/ https://www.genome.gov/27561253/encode-workshop-tutorial-october-2014-ashg/ https://www.genome.gov/27553901/encode-tutorial-may-2013-biology-of-genomes-cshl/ https://www.genome.gov/27563006/encoderoadmap-epigenomics-tutorial-october-2015-ashg/ https://www.genome.gov/27555330/encoderoadmap-epigenomics-tutorial-october-2013-ashg/ https://www.genome.gov/27551933/encoderoadmap-epigenomics-tutorial-nov-2012-ashg/ http://useast.ensembl.org/info/website/tutorials/encode.html https://www.encodeproject.org/tutorials/ https://www.encodeproject.org/tutorials/encode-meeting-2016/ https://www.encodeproject.org/tutorials/encode-users-meeting-2015/ DNA元件百科全书(Encyclopedia of DNA Elements, ENCODE)项目旨在描述人类基因组中所编码的全部功能性序列元件。ENCODE计划于2003年9月正式启动，吸引了来自美国、英国、西班牙、日本和新加坡五国32个研究机构的440多名研究人员的参与，经过了9年的努力，研究了147个组织类型，进行了1478次实验，获得并分析了超过15万亿字节的原始数据，确定了400万个基因开关，明确了哪些DNA片段能打开或关闭特定的基因，以及不同类型细胞之间的“开关”存在的差异。证明所谓“垃圾DNA”都是十分有用的基因成分，担任着基因调控重任。证明人体内没有一个DNA片段是无用的。 千人基因组计划由于时间跨度比较长，最终的数据不只是一千人，最新版共有NA编号开头的1182个人，HG开头的1768个人！它的官方网站是：有一个ppt讲得很清楚如何通过官网做的data portal来下载数据：https://www.genome.gov/pages/research/der/ichg-1000genomestutorial/how_to_access_the_data.pdf 我不喜欢可视化的界面，我比较喜欢直接进入ftp自己翻需要的数据，千人基因组计划不仅仅有自己的ftp站点，而且在NCBI，EBI和sanger研究所里面也有数据源可以下载， 是非常丰富的生信入门资源！ ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/ftp://ftp.sanger.ac.uk/pub/1000genomes/ftp://ftp.ebi.ac.uk/pub/databases/1000genomes/ ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp 千人基因组计划测了5个大的人种，25个亚人种，具体介绍如下：09/08/2014 12:00AM 1,663 20131219.populations.tsv09/09/2014 12:00AM 97 20131219.superpopulations.tsv其实对大部分人来说，除非你想下载千人基因组计划的原始数据来学习生物信息学分析流程，不然用不着这个ftp站点的，它自己在EBI里面的有一个非常好用的可视化界面来浏览千人基因组计划的variation结果 千人基因组计划 — 基因组浏览器： http://www.ncbi.nlm.nih.gov/variation/tools/1000genomes/http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=rs35761398 chr1:24201919:24201920http://www.ncbi.nlm.nih.gov/SNP/snp_ref.cgi?rs=2501432 chr1:24201920http://www.ncbi.nlm.nih.gov/SNP/snp_ref.cgi?rs=2502992 chr1:24201919在千人基因组计划里面看一个rs就能看到各种人群信息：http://browser.1000genomes.org/Homo_sapiens/Variation/Population?r=1:24201420-24202420;v=rs2501432;vdb=variation;vf=1849472这些人群信息，可以画一个网路图！ 只需要变化rs ID号即可，当然并不是所有的rs ID号都在千人基因组计划里面有显示的。还有一个java软件-可视化检测千人基因组数据 http://bioinformatics.oxfordjournals.org/content/early/2016/03/17/bioinformatics.btw147.short?rss=1http://limousophie35.github.io/Ferret/但是好像不是很好用！ 在千人基因组计划的ftp主站点里面可以下载所有数据。ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/直接看最新版的数据，共有NA编号开头的1182个人，HG开头的1768个人！ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/phase3/data/也可以按照人种来查看这些数据：ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/每个人的目录下面都有 四个数据文件夹Oct 01 2014 00:00 Directory alignmentOct 01 2014 00:00 Directory exome_alignmentOct 01 2014 00:00 Directory high_coverage_alignmentOct 01 2014 00:00 Directory sequence_read这些数据实在是太丰富了！也可以直接看最新版的vcf文件，记录了这两千多人的所有变异位点信息！可以直接看到所有的位点，具体到每个人在该位点是否变异！ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/不过它的基因型信息是通过MVNcall+SHAPEIT这个程序call出来的，具体原理见：http://www.ncbi.nlm.nih.gov/pubmed/23093610而且网站还提供一些教程：ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/working/我们肯定可以在千人基因计划的官网下载测序数据，主要是vcf格式的突变！Coriell Catalog website: 1000 Genomes Project1000 Genomes website: browser.1000genomes.org/index.html (by SNP ID)1000 Genomes website: www.1000genomes.org/data (bulk data)但是关于它的表达数据，就不是那么简单了！The most important available existing expression datasets involving 1000g individuals are probably the following: RNAseq (mRNA &amp; miRNA) on 465 individuals (CEU, TSI, GBR, FIN, YRI) Pre-publication RNA-sequencing data from the Geuvadis project is available through http://www.geuvadis.org http://www.ebi.ac.uk/arrayexpress/experiments/E-GEUV-1/samples.htmlhttp://www.ebi.ac.uk/arrayexpress/experiments/E-GEUV-2/samples.html RNAseq on 60 CEU individual [1] http://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-197 Expression arrays on about 800 HapMap 3 individuals with a lot of overlap with 1000g data [1,2] http://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-198http://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-264 RNAseq for 69 YRI individuals [3] http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-19480 居然可以下载千人基因组计划的所有数据bam，vcf数据它有两个ftp站点存储所有的数据！ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/直接看最新版的数据，共有NA编号开头的1182个人，HG开头的1768个人！ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/phase3/data/也可以按照人种来查看这些数据：ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/每个人的目录下面都有 四个数据文件夹1234Oct 01 2014 00:00 Directory alignmentOct 01 2014 00:00 Directory exome_alignmentOct 01 2014 00:00 Directory high_coverage_alignmentOct 01 2014 00:00 Directory sequence_read 这些数据实在是太丰富了！也可以直接看最新版的vcf文件，记录了这两千多人的所有变异位点信息！可以直接看到所有的位点，具体到每个人在该位点是否变异！ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/不过它的基因型信息是通过MVNcall+SHAPEIT这个程序call出来的，具体原理见：http://www.ncbi.nlm.nih.gov/pubmed/23093610而且网站还提供一些教程：ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/working/ 还有Illumina的450K甲基化芯片数据：ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/hgsv_sv_discovery/working/20151214_450k_methylation/还有一个小程序，ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/browser/vcf_to_ped_converter/version_1.1/还有Illumina的450K甲基化芯片数据：ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/hgsv_sv_discovery/working/20151214_450k_methylation/还有一个小程序，ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/browser/vcf_to_ped_converter/version_1.1/ roadmaphttp://www.roadmapepigenomics.org/ 精选的129个细胞系，细胞系的介绍如下：http://www.broadinstitute.org/~anshul/projects/roadmap/metadata/EID_metadata.tab 对每个细胞系，都至少处理了5个核心组蛋白修饰数据，还有其它若干转录因子数据。官网介绍的很详细，我就不翻译了： The NIH Roadmap Epigenomics Mapping Consortium was launched with the goal of producing a public resource of human epigenomic data to catalyze basic biology and disease-oriented research. The Consortium leverages experimental pipelines built around next-generation sequencing technologies to map DNA methylation, histone modifications, chromatin accessibility and small RNA transcripts in stem cells and primary ex vivo tissues selected to represent the normal counterparts of tissues and organ systems frequently involved in human disease. The Consortium expects to deliver a collection of normal epigenomes that will provide a framework or reference for comparison and integration within a broad array of future studies. The Consortium also aims to close the gap between data generation and its public dissemination by rapid release of raw sequence data, profiles of epigenomics features and higher-level integrated maps to the scientific community. The Consortium is also committed to the development, standardization and dissemination of protocols, reagents and analytical tools to enable the research community to utilize, integrate and expand upon this body of data. 首先是这个网站：http://www.encode-roadmap.org/矩阵很容易看懂roadmap处理了哪些细胞系，进行了什么样的处理，数据可以直接下载。 然后我比较首先推崇broad研究所的下载方式 里面还列出了他们用过的peaks caller 工具：http://www.broadinstitute.org/~anshul/projects/encode/preprocessing/peakcalling/ 可以看到，主要有MACS，peakranger，quest，sicer，peakseq，hotspot等等直接进入broad分析好的peaks结果：123456[DIR] Parent Directory - [DIR] broadPeak/ 08-Feb-2015 21:00 - [DIR] gappedPeak/ 08-Feb-2015 21:00 - [DIR] lowq/ 31-Aug-2014 20:42 - [DIR] narrowPeak/ 08-Feb-2015 20:59 - 这里面有3种peaks，我现在还没有搞懂是什么意思。 接着是 iHEC存放的数据： http://epigenomesportal.ca/ihec/download.html我还是第一次看到这个数据接口，也是以文件夹文件的形式直接浏览，根据自己的需求下载即可：除了ENCODE计划的数据，还有Blueprint计划和roadmap计划的数据都可以下载。NIH Roadmap 2014-05-29 Click here for policies最后可以从圣路易斯华盛顿大学里面下载 圣路易斯华盛顿大学Washington University in St. Louis，简称（Wash U，WU）以美国国父乔治·华盛顿命名，始建于1853年2月22日，位于美国密苏里州圣路易斯市，是美国历史上建校最早也是最负盛名的“华盛顿大学”，该校在美国新闻和世界报道（US News &amp; World Report）2014大学综合排名中名列14位。里面有一个非常详细的页面来介绍roadmap的各种数据:http://egg2.wustl.edu/roadmap/web_portal/processed_data.html如果你已经了解了roadmap计划，就很容易找到自己的数据，从而直接浏览器或者wget下载即可。首先是序列比对结果下载。onsolidated Epigenomes:36 bp mappability filtered, pooled and subsampled read alignment files:http://egg2.wustl.edu/roadmap/data/byFileType/alignments/consolidated/Unconsolidated Epigenomes (Uniform mappability): 36 bp mappability filtered primary alignment files:http://egg2.wustl.edu/roadmap/data/byFileType/alignments/unconsolidated/包括各种peaks记录文件下载Narrow contiguous regions of enrichment (peaks) for histone ChIP-seq and DNase-seqData format: NarrowPeakhttp://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/narrowPeak/Broad domains on enrichment for histone ChIP-seq and DNase-seq)Data format: BroadPeakhttp://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/broadPeak/ Data format: GappedPeak (subset of domains containing at least one narrow peaks)http://egg2.wustl.edu/roadmap/data/byFileType/peaks/consolidated/gappedPeak/]]></content>
  </entry>
  <entry>
    <title><![CDATA[nat123]]></title>
    <url>%2Fhexo%2F2017%2F11%2F05%2F2017-11-05_nat123%2F</url>
    <content type="text"><![CDATA[启动教程 12345678cd /soft/nat123/ --进入自己本地实际安装目录sudo apt-get install mono-completemono nat123linux.sh --根据提示手动输入帐号密码 ##或者 mono nat123linux.sh service &amp; --自动读取上一次成功登录帐号以后台服务启动用ctrl+z转动后台执行 开机自动登录12345678910111213141516（1）本地必须先手动输入帐号密码成功登录一次；（2）执行“chmod +x /etc/rc.local”命令确保有权限；（3）把启动程序的命令添加到/etc/rc.local文件中，此文件内容如下，#!/bin/sh -e# rc.local# This script is executed at the end of each multiuser runlevel.# Make sure that the script will "exit 0" on success or any other# value on error.# In order to enable or disable this script just change the execution# bits.# By default this script does nothing.cd /soft/nat123 --本地实际安装目录mono nat123linux.sh service &amp; ---自动读取上次成功登录帐号并以后台服务启动exit 0 LINUX环境开机自动启动防掉线脚本教程 花生壳盒子（花生棒Pro）设置（1）不需要安装任何客户端， （2）先设置联上网（有线或者无线） 花生壳盒子（花生棒Pro）使用教程——有线上网 http://service.oray.com/question/5098.html （3）访问“ b.oray.com ”，然后用sn码和对应密码登录。 登录后，在花生壳管理页面，鼠标移动到右上角偏中间的SN码，点击“切换账号”再用Oray账号登录]]></content>
  </entry>
  <entry>
    <title><![CDATA[GEO数据下载]]></title>
    <url>%2Fhexo%2F2017%2F11%2F04%2F2017-11-04_geo%2F</url>
    <content type="text"><![CDATA[GEO 基础 GEO Platform (GPL) 芯片平台 GEO Sample (GSM) 样本ID号 GEO Series (GSE) study的ID号 GEO Dataset (GDS) 数据集的ID号 ## 用法 数据搜索方法一 在https://www.ncbi.nlm.nih.gov/ 中搜索 GSE81916 选择 BioProject查询 Accession：PRJNA323422; GEO: GSE81916 https://trace.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA323422 可以查询数据具体信息 方法二 在 Gene Expression Omnibus (https://www.ncbi.nlm.nih.gov/geo/)搜素GSE81916进入 https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE81916 数据地址 数据下载原始GSE数据，以GSE114727为例ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE114nnn/GSE114727ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE114nnn/GSE114727/suppl/GSE114727_RAW.tarftp地址用迅雷下载即可 ftp地址ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP075/SRP075747 可以分为以下几个部分 所有SRA数据的共同部分： ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant reads表示存放reads数据，在FTP可以看到另一个选项是analysis，表示分析结果 ByStudy表示根据Study进行分类，其他还可以根据实验ByExp,根据Run,ByRun. sra/SRP/SRP075/SRP075747: 后面部分都是为了便于检索。 12345678910#/bin/bash# @author: xt# @date: 2017-11-04for i in ` seq 56 62`;doaxel ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP075/SRP075747/SRR35899$&#123;i&#125;/SRR35899$&#123;i&#125;.sra#wget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP075/SRP075747/SRR35899$&#123;i&#125;/SRR35899$&#123;i&#125;.sra#echo $i done 1234567891011# https://www.ncbi.nlm.nih.gov/gquery/?term=GSE81916# esearch -db sra -query PRJNA299273 | efetch -format runinfo &gt; runinfo.txt # 这个命令是把所有的结果放到一个文件里，也可以通过 https://trace.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA323422下载SRR的编号# cat runinfo.txt | cut -f 1 -d ',' | grep SRR &gt; sra.ids# ~/biosoft/sratoolkit.2.8.2-1-centos_linux64/bin/prefetch --option-file sra.ids # 数据存在/home/shenmy/ncbi/public/sra这个文件下面，找了半天mkdir /mnt/d/rna_seq/data &amp;&amp; cd /mnt/d/rna_seq/dataperl -lne '$id=substr($_,0,6);print "axel ftp://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/$id/$_/$_.sra"' SRR_Acc_List.txt &gt;sra_down.shbash sra_down.sh# 改成用axel下是因为prefetch下载总是不成功ls *.sra|while read id;do(/mnt/d/Software/Biosoft/sratoolkit/sratoolkit.2.8.2-1-ubuntu64/bin/fastq-dump --split-3 $id);donerm *.srachmod u-w *]]></content>
  </entry>
  <entry>
    <title><![CDATA[生物信息学常见1000个软件的安装代码]]></title>
    <url>%2Fhexo%2F2017%2F11%2F03%2F2017-11-03_1000software%2F</url>
    <content type="text"><![CDATA[引自Jiangming Zeng原文 分析软件1234567891011121314151617181920212223## annovar and GATK ## Download and install sratoolkit## http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software## http://www.ncbi.nlm.nih.gov/books/NBK158900/cd ~/biosoftmkdir sratoolkit &amp;&amp; cd sratoolkitwget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.6.3/sratoolkit.2.6.3-centos_linux64.tar.gz#### Length: 63453761 (61M) [application/x-gzip]## Saving to: "sratoolkit.2.6.3-centos_linux64.tar.gz"tar zxvf sratoolkit.2.6.3-centos_linux64.tar.gz~/biosoft/sratoolkit/sratoolkit.2.6.3-centos_linux64/bin/fastdump -hmkdir -p ~/biosoft/myBinecho 'export PATH=/home/jianmingzeng/biosoft/myBin/bin:$PATH' &gt;&gt;~/.bashrc source ~/.bashrccd ~/biosoftmkdir cmake &amp;&amp; cd cmakewget http://cmake.org/files/v3.3/cmake-3.3.2.tar.gztar xvfz cmake-3.3.2.tar.gzcd cmake-3.3.2 ./configure --prefix=/home/jianmingzeng/biosoft/myBinmakemake install 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231## Download and install samtools## http://samtools.sourceforge.net/## http://www.htslib.org/doc/samtools.html##存放高通量测序比对结果的标准格式##功能： Reading/writing/editing/indexing/viewing SAM/BAM/CRAM formatcd ~/biosoftmkdir samtools &amp;&amp; cd samtoolswget https://github.com/samtools/samtools/releases/download/1.3.1/samtools-1.3.1.tar.bz2 tar xvfj samtools-1.3.1.tar.bz2 cd samtools-1.3.1 ./configure --prefix=/home/jianmingzeng/biosoft/myBinmakemake install~/biosoft/myBin/bin/samtools --help~/biosoft/myBin/bin/plot-bamstats --helpcd htslib-1.3.1./configure --prefix=/home/jianmingzeng/biosoft/myBinmakemake install~/biosoft/myBin/bin/tabix# ## Download and install tabix # cd ~/biosoft# mkdir tabix &amp;&amp; cd tabix# # http://genometoolbox.blogspot.com/2013/11/installing-tabix-on-unix.html# tar xvfj tabix-0.2.6.tar.bz2 # cd tabix-0.2.6# make# cd ~/biosoft# ## http://samtools.github.io/bcftools/# mkdir htslib &amp;&amp; cd htslib # git clone git://github.com/samtools/htslib.git # cd htslib# make ## Download and install bcftools## http://www.htslib.org/download/## http://www.htslib.org/doc/bcftools-1.0.htmlcd ~/biosoftmkdir bcftools &amp;&amp; cd bcftoolswget https://github.com/samtools/bcftools/releases/download/1.3.1/bcftools-1.3.1.tar.bz2tar xvfj bcftools-1.3.1.tar.bz2cd bcftools-1.3.1 makecp bcftools /home/jianmingzeng/biosoft/myBin~/biosoft/myBin/bin/bcftools --help## Download and install vcftools## https://vcftools.github.io/index.html ## http://vcftools.sourceforge.net/specs.htmlcd ~/biosoftmkdir vcftools &amp;&amp; cd vcftools# wget https://codeload.github.com/vcftools/vcftools/legacy.zip/master -O vcftools-vcftools-v0.1.14-24-gac1bfd5.zip # unzip vcftools-vcftools-v0.1.14-24-gac1bfd5.zip # mv vcftools-vcftools-ac1bfd5 vcftools-v0.1.14-24# cd vcftools-v0.1.14-24# export PERL5LIB=/home/jianmingzeng/biosoft/vcftools/vcftools-v0.1.14-24/src/perl/# ./autogen.sh # ./configure --prefix=/home/jianmingzeng/biosoft/myBin# make # make install # ~/biosoft/myBin/bin/vcftools --helpwget https://sourceforge.net/projects/vcftools/files/vcftools_0.1.13.tar.gz tar zxvf vcftools_0.1.13.tar.gzcd vcftools_0.1.13make## Download and install ANNOVAR cd ~/biosoft# The latest version of ANNOVAR (2016Feb01) can be downloaded here (registration required)# http://www.openbioinformatics.org/annovar/annovar_download_form.php mkdir ANNOVAR &amp;&amp; cd ANNOVAR ## Download and install samstat## http://samstat.sourceforge.net/## http://www.htslib.org/doc/samtools.htmlcd ~/biosoftmkdir samstat &amp;&amp; cd samstatwget http://liquidtelecom.dl.sourceforge.net/project/samstat/samstat-1.5.tar.gztar zxvf samstat-1.5.tar.gz cd samstat-1.5 ./configure --prefix=/home/jianmingzeng/biosoft/myBinmakemake install~/biosoft/myBin/bin/samstat --help## Download and install picardtools## https://sourceforge.net/projects/picard/## https://github.com/broadinstitute/picardcd ~/biosoftmkdir picardtools &amp;&amp; cd picardtoolswget http://ncu.dl.sourceforge.net/project/picard/picard-tools/1.119/picard-tools-1.119.zipunzip picard-tools-1.119.zip ## Download and install freebayes## https://github.com/ekg/freebayes## http://clavius.bc.edu/~erik/CSHL-advanced-sequencing/freebayes-tutorial.htmlcd ~/biosoftmkdir freebayes &amp;&amp; cd freebayes## wget -O freebayes-master.zip https://codeload.github.com/ekg/freebayes/zip/master## unzip freebayes-master.zipwget http://clavius.bc.edu/~erik/freebayes/freebayes-5d5b8ac0.tar.gztar xzvf freebayes-5d5b8ac0.tar.gzcd freebayesmake ~/biosoft/freebayes/freebayes/bin/freebayescd ~/biosoft## https://sourceforge.net/projects/varscan/files/## http://varscan.sourceforge.net/index.htmlmkdir VarScan &amp;&amp; cd VarScan wget https://sourceforge.net/projects/varscan/files/VarScan.v2.3.9.jar cd ~/biosoftmkdir SnpEff &amp;&amp; cd SnpEff## http://snpeff.sourceforge.net/## http://snpeff.sourceforge.net/SnpSift.html ## http://snpeff.sourceforge.net/SnpEff_manual.htmlwget http://sourceforge.net/projects/snpeff/files/snpEff_latest_core.zip ## java -jar snpEff.jar download GRCh37.75## java -Xmx4G -jar snpEff.jar -i vcf -o vcf GRCh37.75 example.vcf &gt; example_snpeff.vcfunzip snpEff_latest_core.zip## https://github.com/najoshi/sicklecd ~/biosoftmkdir sickle &amp;&amp; cd sicklewget https://codeload.github.com/najoshi/sickle/zip/master -O sickle.zipunzip sickle.zipcd sickle-mastermake~/biosoft/sickle/sickle-master/sickle -hcd ~/biosoft## http://www.usadellab.org/cms/?page=trimmomatic## http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdfmkdir Trimmomatic &amp;&amp; cd Trimmomaticwget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip unzip Trimmomatic-0.36.zip java -jar ~/biosoft/Trimmomatic/Trimmomatic-0.36/trimmomatic-0.36.jar -h## Download and install bedtoolscd ~/biosoftmkdir bedtools &amp;&amp; cd bedtoolswget https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz## Length: 19581105 (19M) [application/octet-stream] tar -zxvf bedtools-2.25.0.tar.gzcd bedtools2make#~/biosoft/bedtools/bedtools2/bin/## Download and install PeakRangercd ~/biosoftmkdir PeakRanger &amp;&amp; cd PeakRangerwget https://sourceforge.net/projects/ranger/files/PeakRanger-1.18-Linux-x86_64.zip ## Length: 1517587 (1.4M) [application/octet-stream]unzip PeakRanger-1.18-Linux-x86_64.zip~/biosoft/PeakRanger/bin/peakranger -h## Download and install bowtiecd ~/biosoftmkdir bowtie &amp;&amp; cd bowtiewget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.2.9/bowtie2-2.2.9-linux-x86_64.zip #Length: 27073243 (26M) [application/octet-stream]#Saving to: "download" ## I made a mistake here for downloading the bowtie2 mv download bowtie2-2.2.9-linux-x86_64.zipunzip bowtie2-2.2.9-linux-x86_64.zip## Download and install BWAcd ~/biosoftmkdir bwa &amp;&amp; cd bwa#http://sourceforge.net/projects/bio-bwa/files/wget https://sourceforge.net/projects/bio-bwa/files/bwa-0.7.15.tar.bz2 tar xvfj bwa-0.7.15.tar.bz2 # x extracts, v is verbose (details of what it is doing), f skips prompting for each individual file, and j tells it to unzip .bz2 filescd bwa-0.7.15make#export PATH=$PATH:/path/to/bwa-0.7.15 # Add bwa to your PATH by editing ~/.bashrc file (or .bash_profile or .profile file)# /path/to/ is an placeholder. Replace with real path to BWA on your machine#source ~/.bashrc## Download and install macs2 ## // https://pypi.python.org/pypi/MACS2/cd ~/biosoftmkdir macs2 &amp;&amp; cd macs2## just stick to PyPI release: https://pypi.python.org/pypi/MACS2wget https://pypi.python.org/packages/9f/99/a8ac96b357f6b0a6f559fe0f5a81bcae12b98579551620ce07c5183aee2c/MACS2-2.1.1.20160309.tar.gz -O MACS2-2.1.1.tar.gz tar zxvf MACS2-2.1.1.tar.gz cd MACS2-2.1.1.20160309/python setup.py install --user ## https://docs.python.org/2/install/~/.local/bin/macs2 --help#wget https://codeload.github.com/taoliu/MACS/zip/master -O MACS-master.zip#unzip MACS-master.zip#cd MACS-master ## So you can't just pull github snapshot then run setup.py to install MACS2. Instead# ImageMagickcd ~/biosoftmkdir ImageMagick &amp;&amp; cd ImageMagick## http://www.imagemagick.org/ cd ~/biosoftmkdir weblogo &amp;&amp; cd weblogo## http://weblogo.berkeley.edu/wget http://weblogo.berkeley.edu/release/weblogo.2.8.2.tar.gztar zxvf weblogo.2.8.2.tar.gzcd weblogoexport PATH=$PATH:/home/jianmingzeng/biosoft/weblogo/weblogosource ~/.bashrccd ~/biosoftmkdir Ghostscript &amp;&amp; cd Ghostscript# http://www.ghostscript.com/download/gsdnld.html# http://www.ghostscript.com/doc/9.20/Readme.htmwget https://github.com/ArtifexSoftware/ghostpdl-downloads/releases/download/gs920/ghostscript-9.20-linux-x86_64.tgz tar zxvf ghostscript-9.20-linux-x86_64.tgzcp ghostscript-9.20-linux-x86_64/gs-920-linux_x86_64 ~/biosoft/myBin/bin/gs## make sure the "gs" program is executable ## Download and install homer (Hypergeometric Optimization of Motif EnRichment)## // http://homer.salk.edu/homer/## // http://blog.qiubio.com:8080/archives/3024 ## The commands gs, seqlogo, blat, and samtools should now work from the command linecd ~/biosoftmkdir homer &amp;&amp; cd homerwget http://homer.salk.edu/homer/configureHomer.pl perl configureHomer.pl -installperl configureHomer.pl -install hg19## Download and install SWEMBLcd ~/biosoftmkdir SWEMBL &amp;&amp; cd SWEMBL#### readme: http://www.ebi.ac.uk/~swilder/SWEMBL/beginners.htmlwget http://www.ebi.ac.uk/~swilder/SWEMBL/SWEMBL.3.3.1.tar.bz2 tar xvfj SWEMBL.3.3.1.tar.bz2cd SWEMBL.3.3.1/make## error ## Download and install SISSRscd ~/biosoftmkdir SISSRs &amp;&amp; cd SISSRs#### readme: https://dir.nhlbi.nih.gov/papers/lmi/epigenomes/sissrs/SISSRs-Manual.pdfwget http://dir.nhlbi.nih.gov/papers/lmi/epigenomes/sissrs/sissrs_v1.4.tar.gztar xzvf sissrs_v1.4.tar.gz~/biosoft/SISSRs/sissrs.pl## Download and install SISSRscd ~/biosoftmkdir QuEST &amp;&amp; cd QuEST#### http://mendel.stanford.edu/SidowLab/downloads/quest/wget http://mendel.stanford.edu/SidowLab/downloads/quest/QuEST_2.4.tar.gztar xzvf QuEST_2.4.tar.gzcd QuEST_2.4 12345678## Download and install fastqc##可视化展示二代测序数据质量##http://www.bioinformatics.babraham.ac.uk/projects/fastqc/cd ~/biosoftmkdir fastqc &amp;&amp; cd fastqcwget http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.5.zipunzip fastqc_v0.11.5.zip 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291## Download and install CEAS ## http://liulab.dfci.harvard.edu/CEAS/download.htmlcd ~/biosoftmkdir CEAS &amp;&amp; cd CEAS wget http://liulab.dfci.harvard.edu/CEAS/src/CEAS-Package-1.0.2.tar.gztar zxvf CEAS-Package-1.0.2.tar.gzcd CEAS-Package-1.0.2python setup.py install --user ## http://liulab.dfci.harvard.edu/CEAS/usermanual.html ~/.local/bin/ceas --help mkdir annotation &amp;&amp; cd annotation wget http://liulab.dfci.harvard.edu/CEAS/src/hg19.refGene.gz ; gunzip hg19.refGene.gz # http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refGene.txt.gz ## gunzip refGene.txt.gz ; mv refGene.txt hg19refGene.txt## Download and install CEAS ## http://liulab.dfci.harvard.edu/CEAS/download.htmlcd ~/biosoftmkdir crossmap &amp;&amp; cd crossmap pip install CrossMap --user## http://crossmap.sourceforge.net/#use-pip-to-install-crossmapmkdir chain_files &amp;&amp; cd chain_files wget http://hgdownload.soe.ucsc.edu/goldenPath/mm10/liftOver/mm10ToMm9.over.chain.gzwget http://hgdownload.soe.ucsc.edu/goldenPath/mm9/liftOver/mm9ToMm10.over.chain.gz wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/liftOver/hg38ToHg19.over.chain.gz wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/liftOver/hg38ToHg19.over.chain.gz # http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refGene.txt.gz ## gunzip refGene.txt.gz ; mv refGene.txt hg19refGene.txt# Usage: CrossMap.py bed ~/biosoft/crossmap/chain_files/mm9ToMm10.over.chain.gz test.mm9.bed3cd ~/biosoft# http://www.broadinstitute.org/cancer/cga/rnaseqc_run# http://www.broadinstitute.org/cancer/cga/rnaseqc_downloadmkdir RNA-SeQC &amp;&amp; cd RNA-SeQC #### readme: http://www.broadinstitute.org/cancer/cga/sites/default/files/data/tools/rnaseqc/RNA-SeQC_Help_v1.1.2.pdfwget http://www.broadinstitute.org/cancer/cga/tools/rnaseqc/RNA-SeQC_v1.1.8.jar #TopHat+Cufflinks+ pipeline## Download and install TopHat # https://ccb.jhu.edu/software/tophat/index.shtmlcd ~/biosoftmkdir TopHat &amp;&amp; cd TopHat #### readme: https://ccb.jhu.edu/software/tophat/manual.shtmlwget https://ccb.jhu.edu/software/tophat/downloads/tophat-2.1.1.Linux_x86_64.tar.gztar xzvf tophat-2.1.1.Linux_x86_64.tar.gz ln -s tophat-2.1.1.Linux_x86_64 current # ~/biosoft/TopHat/current/tophat2## Download and install Cufflinks # http://cole-trapnell-lab.github.io/cufflinks/cd ~/biosoftmkdir Cufflinks &amp;&amp; cd Cufflinks #### readme: http://cole-trapnell-lab.github.io/cufflinks/manual/#### install:http://cole-trapnell-lab.github.io/cufflinks/install/wget http://cole-trapnell-lab.github.io/cufflinks/assets/downloads/cufflinks-2.2.1.Linux_x86_64.tar.gztar xzvf cufflinks-2.2.1.Linux_x86_64.tar.gz ln -s cufflinks-2.2.1.Linux_x86_64 current~/biosoft/Cufflinks/current/cufflinks#HISAT-Stringtie2-Ballgown pipeline## Download and install HISAT ##https://ccb.jhu.edu/software/hisat2/index.shtml##功能： 将测序结果比对到参考基因组上cd ~/biosoftmkdir HISAT &amp;&amp; cd HISAT #### readme: https://ccb.jhu.edu/software/hisat2/manual.shtmlwget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.0.4-Linux_x86_64.zipunzip hisat2-2.0.4-Linux_x86_64.zipln -s hisat2-2.0.4 current ## ~/biosoft/HISAT/current/hisat2-build## ~/biosoft/HISAT/current/hisat2 ## Download and install StringTie## https://ccb.jhu.edu/software/stringtie/ ## https://ccb.jhu.edu/software/stringtie/index.shtml?t=manualcd ~/biosoftmkdir StringTie &amp;&amp; cd StringTie wget http://ccb.jhu.edu/software/stringtie/dl/stringtie-1.2.3.Linux_x86_64.tar.gz tar zxvf stringtie-1.2.3.Linux_x86_64.tar.gzln -s stringtie-1.2.3.Linux_x86_64 current# ~/biosoft/StringTie/current/stringtiecd ~/biosoftmkdir RSEM &amp;&amp; cd RSEM wget https://codeload.github.com/deweylab/RSEM/tar.gz/v1.2.31mv v1.2.31 RSEM.v1.2.31.tar.gz tar zxvf RSEM.v1.2.31.tar.gz ## Download and install HTSeq ## http://www-huber.embl.de/users/anders/HTSeq/doc/overview.html## https://pypi.python.org/pypi/HTSeqcd ~/biosoftmkdir HTSeq &amp;&amp; cd HTSeqwget https://pypi.python.org/packages/72/0f/566afae6c149762af301a19686cd5fd1876deb2b48d09546dbd5caebbb78/HTSeq-0.6.1.tar.gz tar zxvf HTSeq-0.6.1.tar.gzcd HTSeq-0.6.1python setup.py install --user ~/.local/bin/htseq-count --help## ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_mouse/release_M1/## http://hgdownload-test.cse.ucsc.edu/goldenPath/mm10/liftOver/## GRCm38/mm10 (Dec, 2011) ## ls *bam |while read id;do ( ~/.local/bin/htseq-count -f bam $id genecode/mm9/gencode.vM1.annotation.gtf.gz 1&gt;$&#123;id%%.*&#125;.gene.counts ) ;done ## ls *bam |while read id;do ( ~/.local/bin/htseq-count -f bam -i exon_id $id genecode/mm9/gencode.vM1.annotation.gtf.gz 1&gt;$&#123;id%%.*&#125;.exon.counts ) ;done## Download and install kallisto## https://pachterlab.github.io/kallisto/startingcd ~/biosoftmkdir kallisto &amp;&amp; cd kallisto wget https://github.com/pachterlab/kallisto/releases/download/v0.43.0/kallisto_linux-v0.43.0.tar.gz#tar zxvf ## Download and install Sailfish## http://www.cs.cmu.edu/~ckingsf/software/sailfish/ ## cd ~/biosoftmkdir Sailfish &amp;&amp; cd Sailfish wget https://github.com/kingsfordgroup/sailfish/releases/download/v0.9.2/SailfishBeta-0.9.2_DebianSqueeze.tar.gz #tar zxvf ## Download and install salmon## http://salmon.readthedocs.io/en/latest/salmon.html ## cd ~/biosoftmkdir salmon &amp;&amp; cd salmon ## https://github.com/COMBINE-lab/salmon#tar zxvf cd ~/biosoftmkdir GDC &amp;&amp; cd GDC # https://gdc.cancer.gov/access-data/gdc-data-transfer-tool# http://gdc-docs.nci.nih.gov/Data_Transfer_Tool/Users_Guide/Getting_Started/wget https://gdc.nci.nih.gov/files/public/file/gdc-client_v1.2.0_Ubuntu14.04_x64.zip unzip gdc-client_v1.2.0_Ubuntu14.04_x64.zipcd ~/biosoft/myBin/bin## http://hgdownload.cse.ucsc.edu/admin/exe/wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bedToBigBedwget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bedSortwget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bedGraphToBigWigwget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/fetchChromSizeswget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/wigToBigWigwget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/liftOverwget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bigWigToBedGraphwget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bigBedToBedwget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/blat/blatwget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/blat/gfClientwget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/blat/gfServer## Download and install variationtoolkit## https://code.google.com/archive/p/variationtoolkit/downloads cd ~/biosoftmkdir variationtoolkit &amp;&amp; cd variationtoolkit wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/variationtoolkit/archive.tar.gztar zxvf archive.tar.gz cd variationtoolkitmake## Download and install transvar# http://bioinformatics.mdanderson.org/main/Transvarcd ~/biosoft# https://bitbucket.org/wanding/transvarmkdir transvar &amp;&amp; cd transvar wget https://bitbucket.org/wanding/transvar/get/v2.1.19.20160221.zip unzip v2.1.19.20160221.zip cd wanding-transvar-5dd8a7366999 python setup.py install --user cd ~/biosoft# http://kobas.cbi.pku.edu.cn/download.php mkdir kobas &amp;&amp; cd kobas # wget http://kobas.cbi.pku.edu.cn/download_file.php?type=seq_pep&amp;filename=hsa.pep.fasta.gz # wget http://kobas.cbi.pku.edu.cn/download_file.php?type=sqlite3&amp;filename=hsa.db.gz wget http://kobas.cbi.pku.edu.cn/kobas-2.1.1/kobas-2.1.1.tar.gz tar zxvf kobas-2.1.1.tar.gz cd kobas-2.1.1_20160822# * Download the KOBAS organism data package (organism.db.gz) from KOBAS Backend databases download website# * Download the KOBAS specific species data package from KOBAS Backend databases download website (for example, hsa.db.gz)# * Download the specific sequence file from KOBAS sequence files download website (for example, hsa.pep.fasta.gz)# * `gunzip organism.db.gz`# * `gunzip hsa.db.gz`# * Move all databases into $&#123;kobas_home&#125;/sqlite3/ (for example, organism.db, hsa.db)# * `gunzip hsa.pep.fasta.gz`# * Move the fasta sequence file into $&#123;kobas_home&#125;/seq_pep/# * `makeblastdb -in hsa.pep.fasta -dbtype prot`pip install RPy2 --user pip install Numpy --user pip install Pandas --user pip install BioPython --user pip install matplotlib --user pip install PySQLite --user source("http://bioconductor.org/biocLite.R")biocLite("qvalue")## Download and install bamtools## https://github.com/pezmaster31/bamtools/wiki/Building-and-installingcd ~/biosoftmkdir bamtools &amp;&amp; cd bamtools git clone git://github.com/pezmaster31/bamtools.git cd bamtoolscmake --version ## BamTools requires CMake (version &gt;= 2.6.4).mkdir build &amp;&amp; cd build cmake ../ make~/biosoft/bamtools/bamtools/bin/bamtools## Download and install BAMStats## http://bamstats.sourceforge.net/## https://sourceforge.net/projects/bamstats/files/cd ~/biosoftmkdir BAMStats &amp;&amp; cd BAMStats wget https://nchc.dl.sourceforge.net/project/bamstats/BAMStats-1.25.zip unzip BAMStats-1.25.zip#java -jar ~/biosoft/BAMStats/BAMStats-1.25/BAMStats-1.25.jar --help## Download and install Qualimap ## http://qualimap.bioinfo.cipf.es/cd ~/biosoftmkdir Qualimap &amp;&amp; cd Qualimap wget https://bitbucket.org/kokonech/qualimap/downloads/qualimap_v2.2.1.zip ## readme http://qualimap.bioinfo.cipf.es/doc_html/index.html## example results :http://kokonech.github.io/qualimap/HG00096.chr20_bamqc/qualimapReport.html unzip qualimap_v2.2.1.zip ~/biosoft/bamtools/bamtools/bin/bamtools~/biosoft/Qualimap/qualimap_v2.2.1/qualimap --help ## --java-mem-size=4G## modify ~/.bashrc by adding PATH=$PATH:~/.local/bin/## Download and install deepTools## https://github.com/fidelram/deepTools## http://deeptools.readthedocs.io/en/latest/content/example_usage.htmlpip install pyBigWig --user cd ~/biosoftmkdir deepTools &amp;&amp; cd deepTools git clone https://github.com/fidelram/deepTools ## 130M,cd deepToolspython setup.py install --user## 17 tools in ~/.local/bin/~/.local/bin/deeptoolscd ~/biosoftmkdir ngsplot &amp;&amp; cd ngsplot ## download by yourself :https://drive.google.com/drive/folders/0B1PVLadG_dCKN1liNFY0MVM1Ulk tar -zxvf ngsplot-2.61.tar.gztar zxvf ngsplot.eg.bam.tar.gzecho 'export PATH=/home/jianmingzeng/biosoft/ngsplot/ngsplot/bin:$PATH' &gt;&gt;~/.bashrc echo 'export NGSPLOT=/home/jianmingzeng/biosoft/ngsplot/ngsplot' &gt;&gt;~/.bashrc source ~/.bashrcinstall.packages("doMC", dep=T)install.packages("caTools", dep=T)install.packages("utils", dep=T)source("http://bioconductor.org/biocLite.R")biocLite( "BSgenome" )biocLite( "Rsamtools" )biocLite( "ShortRead" )cd ~/biosoftmkdir breakdancer &amp;&amp; cd breakdancer # http://breakdancer.sourceforge.net/# you need to install 2 perl module by yourself : http://breakdancer.sourceforge.net/moreperl.htmlwget https://sourceforge.net/projects/breakdancer/files/breakdancer-1.1.2_2013_03_08.zip unzip breakdancer-1.1.2_2013_03_08.zip cd breakdancer-1.1.2/cppmake ##something wrong !## usage: http://breakdancer.sourceforge.net/pipeline.htmlcd ~/biosoft# http://boevalab.com/FREEC/mkdir Control-FREEC &amp;&amp; cd Control-FREEC# https://github.com/BoevaLab/FREEC/releaseswget https://github.com/BoevaLab/FREEC/archive/v10.3.zip unzip v10.3.zip # https://www.ncbi.nlm.nih.gov/pubmed/22155870# http://boevalab.com/FREEC/tutorial.html# http://samtools.sourceforge.net/pileup.shtmlcd ~/biosoft# https://github.com/dellytools/dellymkdir delly &amp;&amp; cd delly # git clone --recursive https://github.com/dellytools/delly.git# cd delly # make all# make PARALLEL=1 -B src/dellywget https://github.com/dellytools/delly/releases/download/v0.7.6/delly_v0.7.6_linux_x86_64bit chmod 777 delly_v0.7.6_linux_x86_64bit ~/biosoft/delly/delly_v0.7.6_linux_x86_64bit --help ## delly call -t DEL -g hg19.fa -o s1.bcf -x hg19.excl sample1.bam## ./delly/src/bcftools/bcftools view delly.bcf &gt; delly.vcf## The SV type can be DEL, DUP, INV, TRA, or INS for deletions, tandem duplications, inversions, translocations and small insertions, respectively.## In addition, you can filter input reads more stringently using -q 20 and -s 15.cd ~/biosoft# https://www.cog-genomics.org/plink2/data#merge_listmkdir PLINK &amp;&amp; cd PLINK wget https://www.cog-genomics.org/static/bin/plink170113/plink_linux_x86_64.zip unzip plink_linux_x86_64.zip~/biosoft/PLINK/plink## Download and install Scalpelcd ~/biosoftmkdir Scalpel &amp;&amp; cd Scalpelwget https://downloads.sourceforge.net/project/scalpel/scalpel-0.5.3.tar.gz tar zxvf scalpel-0.5.3.tar.gzcd scalpel-0.5.3make~/biosoft/Scalpel/scalpel-0.5.3/scalpel-discovery --help~/biosoft/Scalpel/scalpel-0.5.3/scalpel-export --helpcd ~/biosoft# https://www.cog-genomics.org/plink2/data#merge_listmkdir firehose &amp;&amp; cd firehose wget http://gdac.broadinstitute.org/runs/code/firehose_get_latest.zipunzip firehose_get_latest.zip ~/biosoft/firehose/firehose_get~/biosoft/firehose/firehose_get -tasks clinical analyses latest brca cd ~/biosoft# https://www.cog-genomics.org/plink2/data#merge_listmkdir fastpop &amp;&amp; cd fastpop wget https://sourceforge.net/projects/fastpop/files/FastPop.tar.gz wget https://jaist.dl.sourceforge.net/project/fastpop/FastPop_Instruction.pdftar zxvf FastPop.tar.gz pip install cnvkit --user 转录组 de novo分析流程的软件全代码Trinotate/Trinity/TransDecoder/sqlite/NCBI BLAST+/HMMER/PFAM signalP v4 /tmhmm v2 /RNAMMER 有些软件需要教育邮箱注册才行哦~~ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768## Trinotate/Trinity/TransDecoder/sqlite/NCBI BLAST+/HMMER/PFAM ## signalP v4 /tmhmm v2 /RNAMMERcd ~/biosoftmkdir hmmer &amp;&amp; cd hmmerwget http://eddylab.org/software/hmmer/2.3/hmmer-2.3.tar.gz tar zxvf hmmer-2.3.tar.gzcd hmmer-2.3./configure --prefix=/home/jmzeng/my-bin#./configure --prefix=/home/jianmingzeng/biosoft/myBinmakemake install#for file in hmmalign hmmbuild hmmcalibrate hmmconvert hmmemit hmmfetch hmmindex hmmpfam hmmsearch ; do\# cp src/$file /home/jmzeng/my-bin/bin/;\# done#for file in hmmer hmmalign hmmbuild hmmcalibrate hmmconvert hmmemit hmmfetch hmmindex hmmpfam hmmsearch; do\# cp documentation/man/$file.man /home/jmzeng/my-bin/man/man1/$file.1;\# donecp /home/jmzeng/my-bin/bin/hmmsearch /home/jmzeng/my-bin/bin/hmmsearch2cd ~/biosoftmkdir CBS &amp;&amp; cd CBS# signalP v4 (free academic download) http://www.cbs.dtu.dk/cgi-bin/nph-sw_request?signalp# tmhmm v2 (free academic download) http://www.cbs.dtu.dk/cgi-bin/nph-sw_request?tmhmm# RNAMMER (free academic download) http://www.cbs.dtu.dk/cgi-bin/sw_request?rnammermkdir signalp-4.1mkdir rnammer-1.2## be sure to untar it in a new directory## it&apos;s a perl script, we need to modify it according to readme http://trinotate.github.io/#SoftwareRequired## vi ~/biosoft/CBS/signalp-4.1/signalptar zxvf signalp-4.1e.Linux.tar.gz tar zxvf rnammer-1.2.src.tar.Z tar zxvf tmhmm-2.0c.Linux.tar.gz ## it&apos;s a perl script, we need to modify it according to readme http://trinotate.github.io/#SoftwareRequired## vi ~/biosoft/CBS/tmhmm-2.0c/bin/tmhmm ## vi ~/biosoft/CBS/tmhmm-2.0c/bin/tmhmmformat.plwhich perl ## /usr/bin/perlcd ~/biosoftmkdir blastPlus &amp;&amp; cd blastPlus# ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATESTwget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.5.0+-x64-linux.tar.gzblastBinFolder=~/biosoft/blastPlus/ncbi-blast-2.5.0+/bin$blastBinFolder/makeblastdb -help# http://www.cbs.dtu.dk/services/doc/signalp-4.1.readmecd ~/biosoftmkdir TransDecoder &amp;&amp; cd TransDecoder# https://transdecoder.github.io/# https://github.com/TransDecoder/TransDecoder/releaseswget https://github.com/TransDecoder/TransDecoder/archive/v3.0.0.tar.gz -O TransDecoder.v3.0.0.tar.gz tar zxvf TransDecoder.v3.0.0.tar.gz cd TransDecoder-3.0.0 make~/biosoft/TransDecoder/TransDecoder-3.0.0/TransDecoder.LongOrfs -h~/biosoft/TransDecoder/TransDecoder-3.0.0/TransDecoder.Predict -h## sqlite3 --helpcd ~/biosoftmkdir Trinotate &amp;&amp; cd Trinotate# http://trinotate.github.io/# https://github.com/Trinotate/Trinotate/releaseswget https://github.com/Trinotate/Trinotate/archive/v3.0.1.tar.gz -O Trinotate.v3.0.1.tar.gz tar zxvf Trinotate.v3.0.1.tar.gz~/biosoft/Trinotate/Trinotate-3.0.1/Trinotate -hwget https://data.broadinstitute.org/Trinity/Trinotate_v3_RESOURCES/Pfam-A.hmm.gzwget https://data.broadinstitute.org/Trinity/Trinotate_v3_RESOURCES/uniprot_sprot.pep.gzwget https://data.broadinstitute.org/Trinity/Trinotate_v3_RESOURCES/Trinotate_v3.sqlite.gz -O Trinotate.sqlite.gzgunzip Trinotate.sqlite.gzgunzip uniprot_sprot.pep.gzmakeblastdb -in uniprot_sprot.pep -dbtype protgunzip Pfam-A.hmm.gzhmmpress Pfam-A.hmm 基因组，gtf，bed，注释前面既然把我多年累积的软件安装代码共享了，就顺便把我最近做直播我的基因组的一些数据下载代码共享吧 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198cd ~/referencemkdir -p genome/hg19 &amp;&amp; cd genome/hg19 nohup wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz &amp;tar zvfx chromFa.tar.gzcat *.fa &gt; hg19.farm chr*.fa cd ~/referencemkdir -p genome/hg38 &amp;&amp; cd genome/hg38 nohup wget http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz &amp; cd ~/referencemkdir -p genome/mm10 &amp;&amp; cd genome/mm10 nohup wget http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/chromFa.tar.gz &amp;tar zvfx chromFa.tar.gzcat *.fa &gt; mm10.farm chr*.fa cd ~/biosoft/RNA-SeQCwget http://www.broadinstitute.org/cancer/cga/sites/default/files/data/tools/rnaseqc/ThousandReads.bamwget http://www.broadinstitute.org/cancer/cga/sites/default/files/data/tools/rnaseqc/gencode.v7.annotation_goodContig.gtf.gzwget http://www.broadinstitute.org/cancer/cga/sites/default/files/data/tools/rnaseqc/Homo_sapiens_assembly19.fasta.gzwget http://www.broadinstitute.org/cancer/cga/sites/default/files/data/tools/rnaseqc/Homo_sapiens_assembly19.other.tar.gzwget http://www.broadinstitute.org/cancer/cga/sites/default/files/data/tools/rnaseqc/gencode.v7.gc.txtwget http://www.broadinstitute.org/cancer/cga/sites/default/files/data/tools/rnaseqc/rRNA.tar.gz cd ~/referencemkdir -p index/bowtie &amp;&amp; cd index/bowtie nohup time ~/biosoft/bowtie/bowtie2-2.2.9/bowtie2-build ~/reference/genome/hg19/hg19.fa ~/reference/index/bowtie/hg19 1&gt;hg19.bowtie_index.log 2&gt;&amp;1 &amp;nohup time ~/biosoft/bowtie/bowtie2-2.2.9/bowtie2-build ~/reference/genome/hg38/hg38.fa ~/reference/index/bowtie/hg38 1&gt;hg38.bowtie_index.log 2&gt;&amp;1 &amp;nohup time ~/biosoft/bowtie/bowtie2-2.2.9/bowtie2-build ~/reference/genome/mm10/mm10.fa ~/reference/index/bowtie/mm10 1&gt;mm10.bowtie_index.log 2&gt;&amp;1 &amp; cd ~/referencemkdir -p index/bwa &amp;&amp; cd index/bwa nohup time ~/biosoft/bwa/bwa-0.7.15/bwa index -a bwtsw -p ~/reference/index/bwa/hg19 ~/reference/genome/hg19/hg19.fa 1&gt;hg19.bwa_index.log 2&gt;&amp;1 &amp;nohup time ~/biosoft/bwa/bwa-0.7.15/bwa index -a bwtsw -p ~/reference/index/bwa/hg38 ~/reference/genome/hg38/hg38.fa 1&gt;hg38.bwa_index.log 2&gt;&amp;1 &amp;nohup time ~/biosoft/bwa/bwa-0.7.15/bwa index -a bwtsw -p ~/reference/index/bwa/mm10 ~/reference/genome/mm10/mm10.fa 1&gt;mm10.bwa_index.log 2&gt;&amp;1 &amp; cd ~/referencemkdir -p index/hisat &amp;&amp; cd index/hisat nohup wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/hg19.tar.gz &amp;nohup wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/hg38.tar.gz &amp;nohup wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grcm38.tar.gz &amp;nohup wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/mm10.tar.gz &amp;tar zxvf hg19.tar.gztar zxvf grcm38.tar.gztar zxvf hg38.tar.gztar zxvf mm10.tar.gz mkdir -p ~/annotation/variation/human/ExACcd ~/annotation/variation/human/ExAC## http://exac.broadinstitute.org/## ftp://ftp.broadinstitute.org/pub/ExAC_release/currentwget ftp://ftp.broadinstitute.org/pub/ExAC_release/current/ExAC.r0.3.1.sites.vep.vcf.gz.tbi nohup wget ftp://ftp.broadinstitute.org/pub/ExAC_release/current/ExAC.r0.3.1.sites.vep.vcf.gz &amp;wget ftp://ftp.broadinstitute.org/pub/ExAC_release/current/cnv/exac-final-cnv.gene.scores071316 wget ftp://ftp.broadinstitute.org/pub/ExAC_release/current/cnv/exac-final.autosome-1pct-sq60-qc-prot-coding.cnv.bed mkdir -p ~/annotation/variation/human/dbSNPcd ~/annotation/variation/human/dbSNP## https://www.ncbi.nlm.nih.gov/projects/SNP/## ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b147_GRCh38p2/## ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b147_GRCh37p13/nohup wget ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b147_GRCh37p13/VCF/All_20160601.vcf.gz &amp;wget ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b147_GRCh37p13/VCF/All_20160601.vcf.gz.tbi mkdir -p ~/annotation/variation/human/1000genomescd ~/annotation/variation/human/1000genomes ## ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ nohup wget -c -r -nd -np -k -L -p ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502 &amp; mkdir -p ~/annotation/variation/human/cosmiccd ~/annotation/variation/human/cosmic## we need to register before we can download this file. mkdir -p ~/annotation/variation/human/ESP6500cd ~/annotation/variation/human/ESP6500# http://evs.gs.washington.edu/EVS/nohup wget http://evs.gs.washington.edu/evs_bulk_data/ESP6500SI-V2-SSA137.GRCh38-liftover.snps_indels.vcf.tar.gz &amp; mkdir -p ~/annotation/variation/human/UK10Kcd ~/annotation/variation/human/UK10K# http://www.uk10k.org/nohup wget ftp://ngs.sanger.ac.uk/production/uk10k/UK10K_COHORT/REL-2012-06-02/UK10K_COHORT.20160215.sites.vcf.gz &amp; mkdir -p ~/annotation/variation/human/gonlcd ~/annotation/variation/human/gonl## http://www.nlgenome.nl/search/## https://molgenis26.target.rug.nl/downloads/gonl_public/variants/release5/nohup wget -c -r -nd -np -k -L -p https://molgenis26.target.rug.nl/downloads/gonl_public/variants/release5 &amp; mkdir -p ~/annotation/variation/human/omincd ~/annotation/variation/human/omin mkdir -p ~/annotation/variation/human/GWAScd ~/annotation/variation/human/GWAS mkdir -p ~/annotation/variation/human/hapmapcd ~/annotation/variation/human/hapmap# ftp://ftp.ncbi.nlm.nih.gov/hapmap/wget ftp://ftp.ncbi.nlm.nih.gov/hapmap/phase_3/relationships_w_pops_051208.txt nohup wget -c -r -np -k -L -p -nd -A.gz ftp://ftp.ncbi.nlm.nih.gov/hapmap/phase_3/hapmap3_reformatted &amp;# ftp://ftp.hgsc.bcm.tmc.edu/pub/data/HapMap3-ENCODE/ENCODE3/ENCODE3v1/wget ftp://ftp.hgsc.bcm.tmc.edu/pub/data/HapMap3-ENCODE/ENCODE3/ENCODE3v1/bcm-encode3-QC.txt wget ftp://ftp.hgsc.bcm.tmc.edu/pub/data/HapMap3-ENCODE/ENCODE3/ENCODE3v1/bcm-encode3-submission.txt.gz ## 1 million single nucleotide polymorphisms (SNPs) for DNA samples from each of the three ethnic groups in Singapore – Chinese, Malays and Indians.## The Affymetrix Genome-Wide Human SNP Array 6.0 &amp;&amp; The Illumina Human1M single BeadChip ## http://www.statgen.nus.edu.sg/~SGVP/## http://www.statgen.nus.edu.sg/~SGVP/singhap/files-website/samples-information.txt# http://www.statgen.nus.edu.sg/~SGVP/singhap/files-website/genotypes/2009-01-30/QC/ ## Singapore Sequencing Malay Project (SSMP) mkdir -p ~/annotation/variation/human/SSMPcd ~/annotation/variation/human/SSMP## http://www.statgen.nus.edu.sg/~SSMP/## http://www.statgen.nus.edu.sg/~SSMP/download/vcf/2012_05 ## Singapore Sequencing Indian Project (SSIP) mkdir -p ~/annotation/variation/human/SSIPcd ~/annotation/variation/human/SSIP# http://www.statgen.nus.edu.sg/~SSIP/## http://www.statgen.nus.edu.sg/~SSIP/download/vcf/dataFreeze_Feb2013 wget ftp://ftp.ensembl.org/pub/release-75/gtf/homo_sapiens/Homo_sapiens.GRCh37.75.gtf.gz wget ftp://ftp.ensembl.org/pub/release-86/gtf/homo_sapiens/Homo_sapiens.GRCh38.86.chr.gtf.gz mkdir -p ~/reference/gtf/gencodecd ~/reference/gtf/gencode## https://www.gencodegenes.org/releases/current.htmlwget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.2wayconspseudos.gtf.gzwget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.long_noncoding_RNAs.gtf.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.polyAs.gtf.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.annotation.gtf.gz ## https://www.gencodegenes.org/releases/25lift37.html wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.annotation.gtf.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.metadata.HGNC.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.metadata.EntrezGene.gz wget ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.metadata.RefSeq.gz mkdir -p ~/reference/gtf/ensembl/homo_sapiens_86cd ~/reference/gtf/ensembl/homo_sapiens_86## http://asia.ensembl.org/info/data/ftp/index.html cd ~/referencemkdir -p genome/human_g1k_v37 &amp;&amp; cd genome/human_g1k_v37# http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/ nohup wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz &amp;gunzip human_g1k_v37.fasta.gzwget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.faiwget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/README.human_g1k_v37.fasta.txtjava -jar ~/biosoft/picardtools/picard-tools-1.119/CreateSequenceDictionary.jar R=human_g1k_v37.fasta O=human_g1k_v37.dict ## ftp://ftp.broadinstitute.org/bundle/b37/mkdir -p ~/annotation/GATKcd ~/annotation/variation/GATKwget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_phase1.snps.high_confidence.b37.vcf.gz wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/dbsnp_138.b37.vcf.gzwget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37.fasta.gz wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/NA12878.HiSeq.WGS.bwa.cleaned.raw.subset.b37.sites.vcf.gzwget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/Mills_and_1000G_gold_standard.indels.b37.vcf.gz wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/hapmap_3.3.b37.vcf.gzwget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_phase1.indels.b37.vcf.gz wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_phase1.indels.b37.vcf.idx.gzgunzip 1000G_phase1.indels.b37.vcf.idx.gzgunzip 1000G_phase1.indels.b37.vcf.gz mkdir -p ~/institute/ENSEMBL/gtfcd ~/institute/ENSEMBL/gtfwget ftp://ftp.ensembl.org/pub/release-87/gtf/homo_sapiens/Homo_sapiens.GRCh38.87.chr.gtf.gz wget ftp://ftp.ensembl.org/pub/release-87/gtf/mus_musculus/Mus_musculus.GRCm38.87.chr.gtf.gzwget ftp://ftp.ensembl.org/pub/release-87/gtf/danio_rerio/Danio_rerio.GRCz10.87.chr.gtf.gz cd ~/institute/TCGA/firehose## https://gdac.broadinstitute.org/wget http://gdac.broadinstitute.org/runs/stddata__2016_01_28/data/ACC/20160128/gdac.broadinstitute.org_ACC.Merge_snp__genome_wide_snp_6__broad_mit_edu__Level_3__segmented_scna_minus_germline_cnv_hg19__seg.Level_3.2016012800.0.0.tar.gz -O ACC.gistic.seg.tar.gzwget http://gdac.broadinstitute.org/runs/stddata__2016_01_28/data/ACC/20160128/gdac.broadinstitute.org_ACC.Merge_snp__genome_wide_snp_6__broad_mit_edu__Level_3__segmented_scna_hg19__seg.Level_3.2016012800.0.0.tar.gz -O ACC.raw.seg.tar.gz wget http://gdac.broadinstitute.org/runs/stddata__2016_01_28/data/ACC/20160128/gdac.broadinstitute.org_ACC.Mutation_Packager_Calls.Level_3.2016012800.0.0.tar.gz -O ACC.maf.tar.gzwget http://gdac.broadinstitute.org/runs/stddata__2016_01_28/data/ACC/20160128/gdac.broadinstitute.org_ACC.Mutation_Packager_Oncotated_Calls.Level_3.2016012800.0.0.tar.gz -O ACC.maf.anno.tar.gz]]></content>
  </entry>
  <entry>
    <title><![CDATA[一致性指数Concordance index(CI, C-index)]]></title>
    <url>%2Fhexo%2F2017%2F10%2F15%2F2017-10-15_Concordanceindex%2F</url>
    <content type="text"><![CDATA[原文 所谓C-index，英文名全称concordance index，最早是由范德堡大学（Vanderbilt University）生物统计教教授Frank E Harrell Jr 1996年提出，主要用于计算生存分析中的COX模型预测值与真实之间的区分度（discrimination）；现阶段用的最多的是肿瘤患者预后模型的预测精度。 一般评价模型的好坏主要有两个方面， 一是模型的拟合优度（Goodness of Fit),常见的评价指标主要有R方，-2logL,AIC,BIC等等； 另外一个是模型的预测精度，主要就是模型的真实值与预测值之间的差的大小，均方误差，相对误差等。 从临床应用的角度来说，我们更注重后者，即统计建模主要是用于预测，而从C-index的概念大家看出它属于模型评价指标的后者，这一指标比前面提到的几个指标看起来更高大上，一般文献中用的也比较多。 C-index本质上是估计了预测结果与实际观察到的结果相一致的概率，即资料所有病人对子中预测结果与实际结果一致的对子所占的比例。有点类似于ROC曲线下面积。 **C-index的计算方法是:把所研究的资料中的所有研究对象随机地两两组成对子。以生存分析为例,对于一对病人,如果生存时间较长的一位,其预测生存时间长于生存时间较短的一位,或预测的生存概率高的一位的生存时间长于生存概率低的另一位,则称之为预测结果与实际结果一致。** C-index的计算步骤为: (1)产生所有的病例配对。若有n个观察个体,则所有的对子数应为Cn2(组合数). (2)排除下面两种对子:对子中具有较小观察时间的个体没有达到观察终点及对子中两个个体都没达到观察终点。剩余的为有用对子。 (3)计算有用对子中,预测结果和实际相一致的对子数,即具有较坏预测结果个体的实际观察时间较短。 (4)计算。C= 一致对子数/有用对子数。 C-index在0.5-1之间。0.5为完全不一致,说明该模型没有预测作用,1为完全一致,说明该模型预测结果与实际完全一致。 在实际应用中,很难找到完全一致的预测模型,既往研究认为,C-index 在0.50-0.70为较低准确度, 在0.71-0.90之间为中等准确度; 而高于0.90则为高准确度。 当C-index检验由同一样本建成的模型时易造成偏倚,因此再采用重抽样技术(Bootstrap)可以几乎无偏倚的检验预测模型的准确度。Bootstrap是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法,是现代统计应用较为广泛的一种统计方法。 Bootstrap方法核心思想和基本步骤如下: (1)采用重抽样技术从原始样本中抽取一定数量的样本,此过程允许重复抽样。 (2)根据抽出的样本计算给定的统计量T。 (3)重复上述N次(一般大于1000),得到N个统计量T。 (4)计算上述N个统计量T的样木方差,得到统计量的方差。 Bootstarap方法只是对单一样本且样本量较小的资料，如果数据集很大可以按照不同的比例将数据集拆分，一部分用于建模一部分用于验证。关于交叉验证（Cross-validation），由于篇幅有限，留作下次探讨。 R软件实现：Note: 在生存分析中一般直接有c-index输出，直接采用就ok. C-index的R软件计算实现有两种实现方法，一种是用到Harrell本人的的R包Hmisc package；另一种是Le Kang, Weijie Chen 2014年12月18日发布的R compareC Package 123456789101112131415161718192021222324252627282930313233343536################################ Method 1.Hmisc code ###############################data &lt;- read.csv("survivaldta.csv")library(Hmisc) library(survival) ###加载survival包，主要用于建立模型###f &lt;- cph(Surv(time,death)~x1+x2+x3，data=survivldata) ###拟合cox模型fp &lt;- predict(f)###模型的预测值cindex.orig=1-rcorr.cens(fp,Surv(time,death)) [[1]]###计算出的C-index################################### Method 2.compareC code ##################################data &lt;- read.csv("survivaldta.csv") library(compareC) library(survival) cindex &lt;- cindex(Surv(time,death) ~ x1+x2+x3,data=survivldata)###计算出的C-index################################### Bootstrap code ##################################bootit=200for(i in 1:bootit)&#123;case=noNA[group=="long",] control=noNA[group=="&lt;24",]bootindex.case=sample(1:nrow(case),replace=T)boot.case.data=case[bootindex.case,]bootindex.control=sample(1:nrow(control),replace=T)boot.control.data=control[bootindex.control,]boot.data=rbind(boot.case.data,boot.control.data)dstr.boot=svydesign(id=~1, prob=~inv_weight, fpc=~ssize, data=boot.data)boot.fit=svycoxph(Surv(survival,surv_cens) ~x1+x2+x3,data=boot.data,x=TRUE,design=dstr.boot)cindex.train=1-rcorr.cens(lp.boot,Surv(boot.data$survival, boot.data$surv_cens))[[1]]cindex.test=1-rcorr.cens(lp_=.test,Surv(noNA$survival,noNA$surv_cens))[[1]]bias=rep(1,bootit)bias[i]=abs(cindex.train-cindex.test) &#125; 参考文献Harrell FE, Califf RM, Pryor DB, Lee KL, and Rosati RA. (1982) Evaluating the yield of medical tests. The Journal of the American Medical Association, 247(18), 2543–2546 Pencina MJ and D’Agostino RB. (2004) Overall C as a measure of discrimination in survival analysis: model specific population value and confidence interval estimation. Statistics in Medicine, 23(13), 2109–2123 Kang L, Chen W, Petrick NA, and Gallas BD. (2014) Comparing two correlated C indices with right-censored survival outcome: a one-shot nonparametric approach. Statistics in Medicine, 34(4), 685–703, doi: 10.1002/sim.6370 Hmisc Reference manual：http://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf compareC Reference manual: http://cran.r-project.org/web/packages/compareC/compareC.pdf Frank.Harrell :http://biostat.mc.vanderbilt.edu/wiki/Main/FrankHarrell 5 Ways to Estimate Concordance Index for Cox Models in RWhy Results Aren’t Identical? Harrell’s concordance index (c-index) can be used to evaluate the discriminatory power and the predictive accuracy of Cox models. An easy way out as a surrogate for ROC analysis. Approach 1Use function “rcorrcens” in package “Hmisc” Limitations: Can only handle un-censored data Roughly handle categorical predictor with more than 2 categories 123456Sample code:library(survival)library(Hmisc)attach(sample.data)surv &lt;- Surv(survival, censor)rcorrcens(surv ~ group) Approach 2Direct output from coxphRequire higher version of R, say R 2.15, didn’t test with older versions 123456Sample code:library(survival)attach(sample.data)surv &lt;- Surv(survival, censor)sum.surv &lt;- summary(coxph(surv ~ group))c_index &lt;- sum.surv$concordance Approach 3Use function “survConcordance”Result is the same as in Approach 2123456Sample code:library(survival)attach(sample.data)surv &lt;- Surv(survival, censor)fit &lt;- coxph( surv ~ group)survConcordance(surv ~ predict(fit)) Approach 4Use package “survcomp” in bioconductor Different result from Approach 2/3 The disparity is due to two different estimation approaches that are used to handle tied risks (i.e. cases when two observations have the same survival with the same x). The method that approaches 4/5 use ignores the tied risks, the other method (approaches 2/3) takes into consideration of the tied risks. In terms of formulation/symbol (for illustration only): Approaches 4/5 used: Concordance = #all concordant pairs/#total pairs ignoring ties. Approaches 2/3 used: Concordance = (#all concordant pairs + #tied pairs/2)/(#total pairs including ties).Those #s can be obtained in the output of Approach 3. 12345678Sample code:source("http://bioconductor.org/biocLite.R")biocLite("survcomp")library(survcomp)surv &lt;- Surv(survival, censor) fit &lt;- coxph(surv ~ group, data= sample.data)coxPredict &lt;- predict(fit, data=sample.data, type="risk") concordance.index(x=coxPredict, surv.time=survival, surv.event=censor, method="noether") Approach 5Use function “cph” in package “rms”Provide both un-adjusted and bias adjusted c-indexUn-adjusted c-index is the same as the one from Approach 412345678910111213Sample code: library(rms)surv &lt;- Surv(survival, censor) set.seed(1)fit.cph &lt;- cph(surv ~ group, data= sample.data, x=TRUE, y=TRUE, surv=TRUE) # Get the Dxyv &lt;- validate.cph(fit.cph, dxy=TRUE, B=1000)Dxy = v[rownames(v)=="Dxy", colnames(v)=="index.corrected"]orig_Dxy = v[rownames(v)=="Dxy", colnames(v)=="index.orig"]# The c-statistic according to Dxy=2(c-0.5)bias_corrected_c_index &lt;- abs(Dxy)/2+0.5orig_c_index &lt;- abs(Orig.Dxy)/2+0.5 Combining Approaches 2/3 &amp; 4/5 and Calculate p-value for Testing Two C-indices for Both Estimation MethodsDid not find it elsewhere online. Hope someone could find this helpful.123456789101112131415161718192021222324252627282930313233343536373839Sample code:surv &lt;- Surv(survival, censor)c_index &lt;- function(group, ties=TRUE)&#123; fit &lt;- coxph(surv ~ group, data=sample.data) coxPredict &lt;- predict(fit, data=sample.data, type="risk") # Approaches 4/5 if (ties==F) &#123; concordance.index(x=coxPredict, surv.time=survival, surv.event=censor, method="noether") &#125; # Approaches 2/3 else if (ties==T) &#123; survConcordance(surv ~ coxPredict, data=sample.data) &#125;&#125;c_index_ties1 &lt;- c_index(group=group1, ties=TRUE)c_index_ties2 &lt;- c_index(group=group2, ties=TRUE)c_index_no_ties1 &lt;- c_index_ties(group=group1, ties=F)c_index_no_ties2 &lt;- c_index_ties(group=group2, ties=F)# p-value of testing two c-indices ignoring tiesround(cindex.comp(c_index_no_ties1, c_index_no_ties2)$p.value,3)# Function for p-value of testing two c-indices accounting for ties# t-test for dependent variables is used for significance # Input variables are objects obtained from the first functioncindex.p.ties &lt;- function(c_index_ties1, c_index_ties2, c_index_no_ties1, c_index_no_ties2) &#123; eps &lt;- 1E-15 n &lt;- c_index_no_ties1$n r &lt;- cor(c_index_no_ties1$data$x, c_index_no_ties2$data$x, use="complete.obs", method="spearman") if ((1 - abs(r)) &gt; eps) &#123; t.stat &lt;- (c_index_ties1$concordance - c_index_ties2$concordance) / sqrt(c_index_ties1$std.err^2 + c_index_ties2$std.err^2 - 2 * r * c_index_ties1$std.err * c_index_ties2$std.err) diff.ci.p &lt;- pt(q=t.stat, df=n - 1, lower.tail=FALSE) &#125; else &#123; diff.ci.p &lt;- 1 &#125; return(list("p.value"=diff.ci.p)) &#125;cindex.p.ties(c_index_ties1=c_index_ties1, c_index_ties2=c_index_ties2, c_index_no_ties1=c_index_no_ties1, c_index_no_ties2=c_index_no_ties2)]]></content>
  </entry>
  <entry>
    <title><![CDATA[R统计分析-假设检验]]></title>
    <url>%2Fhexo%2F2017%2F10%2F13%2F2017-10-13-statistics%2F</url>
    <content type="text"><![CDATA[图片来源于 R语言与数据分析实战 [韩] 徐珉久 著 人民邮电出版社 2017 概率分布 样本抽样1. 简单随机抽样12sample(x,size,replace=FALSE, prob #数据被抽取的权重值) 2. 按权重的样本抽样1sample(1:10,5,replace=TRUE,prob=1:10) 3. 分层随机抽样例如：男性占20%，女性占80%，如果通过抽样来统计组群的平均身高，性别不同会对统计结果有直接影响，因此可以根据男女性别比例采用分层抽样。 分层抽样的好处是可以根据每层分别抽样不同数量的样本 12strata(data, stratanames=NULL, size, method=c("srswor","srswr","poisson","systematic"), pik,description=FALSE) 123library("sampling")x= strata(data=iris,c("Species"),size=c(3,3,5),method="srswor")getdata(iris,x) 4. 系统抽样例如：要对从全天经过某一路口的车辆号牌进行抽样调查，如果采用简单随机抽样，早、晚高峰的经过的车辆会被过多抽取。 系统抽样：将样本总体从1~N编号，然后随机确定抽样起点，以固定间隔k=N/n进行等间隔抽样。因此如果整体数据呈现有序排列形式，系统抽样会获得更好的结果，但是周期数据将会导致偏向性。 12library("doBy")sampleBy(~1,frac=.3,data=x,systematic=TRUE) 列联表列联表：以表格形式记录分类变量的频数。卡方独立性检验考察变量之间是否存在依存关系。 — 预测-垃圾邮件 预测-非垃圾邮件 实际-垃圾邮件 a b 实际-非垃圾邮件 c d 创建列联表12345678910111213d=data.frame(x=c("1","2","2","1")), y=c("A","B","A","B"), num=c(3,5,8,7)xt=xtabs(num ~ x + y,data=d)##边际量margin.table(xt) margin.table(xt,1)#行margin.table(xt,2)#列##边际百分比prop.table(xt)prop.table(xt,1)#行prop.table(xt,2)#列 独立性检验 — A-TRUE A-FALSE B-TURE a b B-FALSE c d 假设A 与 B 独立，则P(AB)=P(A)*P(B). 1. 卡方检验 对两个变量的独立性进行检验，统计量如下： \sum\_{i=1}^{r}\sum\_{j=1}^{c}\frac{(O\_{ij}-E\_{ij})^2}{E\_{ij}} \backsim {\chi}^2(r-1)(c-1)其中，\(O_{ij}\)为列联表中（i,j）的记录值，\(E_{ij}=N P(i) P(j)\)为两变量相互独立时单元的期望值。 123456789101112library("MASS")data(survey)xtabs(~Sex + Exer,data=survey)##性别和运动的列联表## Exer##Sex Freq None Some##Female 49 11 58##Male 65 13 40chisq.test(xtabs(~Sex + Exer,data=survey))## Pearson's Chi-squared test##data: xtabs(~Sex + Exer, data = survey)##X-squared = 5.7184, df = 2, p-value = 0.05731 结果：p值不具有显著性，因此不能推翻原假设H0（性别与运动独立），接受H0(原假设不是小概率事件). 2. Fisher检验 针对列联表中样本数量较少，或者样本分布过分倾向某个单元，卡方检验的结果可能不准确。chisq.test()进行检验会显示警告信息。 1234567891011121314151617181920xtabs(~W.Hnd + Clap,data=survey)## Clap##W.Hnd Left Neither Right##Left 9 5 4##Right 29 45 143chisq.test(xtabs(~W.Hnd + Clap,data=survey))## Pearson's Chi-squared test##data: xtabs(~W.Hnd + Clap, data = survey)##X-squared = 19.252, df = 2, p-value = 6.598e-05##Warning message:##In chisq.test(xtabs(~W.Hnd + Clap, data = survey)) :## Chi-squared近似算法有可能不准fisher.test(xtabs(~W.Hnd + Clap,data=survey))## Fisher's Exact Test for Count Data##data: xtabs(~W.Hnd + Clap, data = survey)##p-value = 0.0001413##alternative hypothesis: two.sided 结果：显示P值具有显著性，推翻原假设【用于写字的手与鼓掌时放在上面的手独立】（原假设为小概率事件），接受两者之间有关系。 3. McNemar检验 McNemar检验检验事件发生前后被调查者的反应变化，比如推行罚款政策后系安全带人数的变化。在事件之前进行问卷调查Test1,事件发生之后问卷调查Test2. — Test 2-TRUE Test 2-FALSE Test 1 a b Test 1 c d Test 2-TRUE Test 2-FALSE Test 1 a b Test 1 c d 假设事件发生前后参与调查的人数未发生变化，a+b=a+c。只要检验b=c是否成立，即可知道事件发生前后人们心理趋向是否发生变化。若b=c成立，则b服从二项分布。 b \backsim B(b+c,\frac{1}{2})二项分布B(n,p)中当n (列联表中b+c)较大时，可以近视视作正态分布。 b \backsim N(\frac{b+c}{2},\frac{b+c}{4})将b标准化，使之服从N(0,1),并用连续校正，得到 \frac{(\|b-c\|-1)^2}{b+c} \backsim \chi^2(1)123456789performance=matrix(c(794,86,150,570),nrow=2,dimnames=list( "1st survey"=c("Approve","Disapprove"), "2st survey"=c("Approve","Disapprove"))) mcnemar.test(performance)## McNemar's Chi-squared test with continuity correction##data: performance##McNemar's chi-squared = 16.818, df = 1, p-value = 4.115e-05 结果：P值结果具有显著性，原假设为小概率事件，推翻原假设，事件发生前后，”Approve”和”Disapprove”比率发生变化。 4.Wilcoxon-Mann-Whitney秩和检验（rank sum test（或test U）） 目的：比较不满足总体为Gaussian正态分布的两个独立样本群组的平均值 例如：你想要看看两个足球队在一年进球数均值是否一样。以下为每个队在一年6场比赛中的进球数： Team A: 6, 8, 2, 4, 4, 5Team B: 7, 10, 4, 3, 5, 6 R语言实现 123456789101112131415161718a = c(6, 8, 2, 4, 4, 5)b = c(7, 10, 4, 3, 5, 6)wilcox.test(a,b, correct=FALSE) Wilcoxon rank sum testdata: a and bW = 14, p-value = 0.5174alternative hypothesis: true location shift is not equal to 0##或者a = c(6, 8, 2, 4, 4, 5)b = c(7, 10, 4, 3, 5, 6)wilcox.test(b,a, correct=FALSE) Wilcoxon rank sum testdata: b and aW = 22, p-value = 0.5174alternative hypothesis: true location shift is not equal to 0 p-value大于0.05，因此我们可接受null hypothesis H0，即两个群组的均值统计相等。 值W的计算如下 123456789sum.rank.a = sum(rank(c(a,b))[1:6]) #sum of ranks assigned to the group a W = sum.rank.a – (length(a)*(length(a)+1)) / 2 W[1] 14 sum.rank.b = sum(rank(c(a,b))[7:12]) #sum of ranks assigned to the group b W = sum.rank.b – (length(b)*(length(b)+1)) / 2 W[1] 22 拟合优度检验统计分析中，常常假设数据服从某种分布，特别是数据量超过一定数值时，一般假设数据服从正态分布。 1. 卡方检验 为了验证数据是否服从某种特定分布(需要检验者自己给出)，通常先要创建列联表。 例如：分析用左手写字的人数与用右手写字的人数比率是否为30%：70%。 123456789library("MASS")data(survey)table(survey$W.Hnd)chisq.test(table(survey$W.Hnd),p=c(0.3,0.7))#Chi-squared test for given probabilities#data: table(survey$W.Hnd)#X-squared = 56.252, df = 1, p-value = 6.376e-14#结果：p值显著，原假设为小概率事件，拒绝原假设，左手写字的人数与用右手写字的人数比率为30%：70%不成立。 2. Shapiro-Wilk（夏皮罗-威尔克）检验 检验样本是否来源于正态分布的总体（样本是否从正态分布的数据中抽取的） 12345 shapiro.test(rnorm(1000))## Shapiro-Wilk normality test##data: rnorm(1000)##W = 0.99883, p-value = 0.7749#结果：p值不显著，原假设不为小概率事件，接受原假设，肯定样本来源于正态分布整体。 3. Kolmogorov-smirnov test（柯尔莫哥-斯米诺夫 K-S test）检验 检验两个经验分布是否不同或一个经验分布与另一个理想分布是否不同。将数据的累积分布函数与要比较的分布的累积分布函数之间的最大距离作为统计量。 例如： 检验两组正态分布的随机数据之间是否拥有相同的分布。 1234567891011121314ks.test(rnorm(100),rnorm(100))# Two-sample Kolmogorov-Smirnov test#data: rnorm(100) and rnorm(100)#D = 0.1, p-value = 0.6994#alternative hypothesis: two-sided##p值不显著，原假设不为小概率事件，接受原假设，两组样本具有相同分布。ks.test(rnorm(100),runif(100))# Two-sample Kolmogorov-Smirnov test#data: rnorm(100) and runif(100)#D = 0.54, p-value = 4.335e-13#alternative hypothesis: two-sided##p值显著，原假设为小概率事件，拒绝原假设，两组样本不具有相同分布。 4. Q-Q（Quantile-Quantile）图 Q-Q图是一种检验数据是否服从某种特定分布的可视化方法。 例如：要检验X是否服从正态分布。若\(X \backsim N(\mu,\delta^2)\),则 Z=\frac{X-\mu}{\delta} \backsim N(0,1)X=\mu + \delta Z若将(X,Z)绘制到坐标平面，则为一条直线。X是待检验的已知数据，Z为标准正态分布数据，因此找到与X对应的Z即可，可以通过分位数完成。 123x=rnorm(1000,mean=10,sd=1)qqnorm(x)qqline(x,lty=2) 123x=runif(1000)qqnorm(x)qqline(x,lty=2) 假设检验提纲1.参数检验1. 单正态总体参数的检验 均值µ的假设检验 方差σ 2 的检验: χ 2 检验.2.两正态总体参数的检验 均值的比较: t检验 方差的比较: F检验 3.成对数据的t检验4.单样本比率的检验 比率p的精确检验 比率p的近似检验 5.两样本比率的检验2. 非参数检验1.单总体位置参数的检验 7.1.1 中位数的符号检验 7.1.2 Wilcoxon符号秩检验 2.分布的一致性检验：χ 23. 两总体的比较与检验 7.3.1 χ 2 独立性检验 7.3.2 Fisher精确检验 7.3.3 Wilcoxon秩和检验法和Mann-Whitney U检验 7.3.4 Mood检验 4 多总体的比较与检验 7.4.1 位置参数的Kruskal-Wallis秩和检验 7.4.2 尺度参数的Ansari-Bradley检验 7.4.3 尺度参数的Fligner-Killeen检验 相关分析1.Pearson（皮尔逊）相关系数 当两个变量的标准差都不为零时，相关系数才有定义，皮尔逊相关系数适用于： 两个变量之间是线性关系，都是连续数据。 两个变量的总体是正态分布，或接近正态的单峰分布。 两个变量的观测值是成对的，每对观测值之间相互独立。 2.Spearman Rank(斯皮尔曼等级)相关系数 斯皮尔曼等级相关系数用来估计两个变量X、Y之间的相关性，其中变量间的相关性可以使用单调函数来描述。如果两个变量取值的两个集合中均不存在相同的两个元素，那么，当其中一个变量可以表示为另一个变量的很好的单调函数时（即两个变量的变化趋势相同），两个变量之间的ρ可以达到+1或-1。 假设两个随机变量分别为X、Y（也可以看做两个集合），它们的元素个数均为N，两个随机变量取的第i（1&lt;=i&lt;=N）个值分别用Xi、Yi表示。对X、Y进行排序（同时为升序或降序），得到两个元素排行集合x、y，其中元素xi、yi分别为Xi在X中的排行以及Yi在Y中的排行。将集合x、y中的元素对应相减得到一个排行差分集合d，其中di=xi-yi，1&lt;=i&lt;=N。随机变量X、Y之间的斯皮尔曼等级相关系数可以由x、y或者d计算得到，其计算方式如下所示： 3.Kendall Rank（肯德尔等级）相关系数 xxx 4.相关系数检验 *原假设：相关系数为0 *备选假设： 相关系数不为0. 1234567891011cor(iris$Sepal.Width,iris$Sepal.Length)cor(iris[,1:4])library(corrgram)#可视化显示相关系数corrgram(iris,upper.panel=panel.conf)m=matrix(c(1:10),(1:10)^2),ncol=2))cor(m,method="spearman")cor.test(c(1,2,3,4,5),c(1,0,3,4,5),method="pearson")cor.test(c(1,2,3,4,5),c(1,0,3,4,5),method="spearman")cor.test(c(1,2,3,4,5),c(1,0,3,4,5),method="kendall") 估计与检验1.单样本均值 \frac{\overline{X}-\mu}{S/\sqrt{n}} \backsim t(n-1)显著性水平为a时，关于整体均值的95%置信水平的置信区间为： (\overline{X} - t(n-1;\alpha/2) * S/\sqrt{n},\overline{X} + t(n-1;\alpha/2) * S/\sqrt{n} 零假设：整体均值为mu. 12345678910111213141516x=rnorm(30)t.test(x)t.test(x,mu=0)## One Sample t-test##data: x##t = 0.052695, df = 29, p-value = 0.9583##alternative hypothesis: true mean is not equal to 0##95 percent confidence interval:## -0.3502195 0.3687436##sample estimates:## mean of x ##0.009262058 y=rnorm(30,mean=10)t.test(y)t.test(y,mu=10) 结果： 推断整体均值为0.009262058，总体均值95%置信区间为（-0.3502195, 0.3687436）。p值不显著，不能拒绝0假设，总体均值可以被视为0，0也在95%置信区间内。 2.两独立样本均值 从两个总体分别抽取样本，根据样本推断两个整体均值是否一致。 12345678910111213141516171819202122232425sleep2=sleep[,-3]tapply(sleep2$extra,sleep2$group,mean)var.test(extra ~ group,data = sleep2)##首先检验两组样本方差是否一致##F test to compare two variances##data: extra by group##F = 0.79834, num df = 9, denom df = 9, p-value = 0.7427##alternative hypothesis: true ratio of variances is not equal to 1##95 percent confidence interval:## 0.198297 3.214123##sample estimates:## ratio of variances ##0.7983426 ## 原假设：两者方差无差别。因此不能拒绝原假设。t.test(extra ~ group,data = sleep2,paired=FALSE,var.equal=TRUE)##Two Sample t-test##data: extra by group##t = -1.8608, df = 18, p-value = 0.07919##alternative hypothesis: true difference in means is not equal to 0##95 percent confidence interval:## -3.363874 0.203874##sample estimates:## mean in group 1 mean in group 2 ##0.75 2.33 t.test中paired=FALSE表示两独立样本检验,TRUE表示配对样本检验，var.equal=TRUE表示两样本方差一致。 结果：P值不显著，不能推翻原假设，因此可以认为两组总体均值无差异。 3.两配对样本均值 1t.test(extra ~ group,data = sleep2,paired=TRUE,var.equal=TRUE) 结果：P值显著，推翻原假设，服用两种安眠药后睡眠时间增加程度不同。 4.两样本方差 评估两独立总体的方差是否一致。两样本方差检验一般不独立使用，常配合其他检验使用，如在两独立样本检验中，如果两检验两总体方差一致，则将t.test()的var.equal设置为TRUE. 5.单样本比率 5.两 样本比率]]></content>
  </entry>
  <entry>
    <title><![CDATA[Logistic回归]]></title>
    <url>%2Fhexo%2F2017%2F10%2F12%2F2017-10-12-Logistic%2F</url>
    <content type="text"><![CDATA[原文 http://blog.csdn.net/ariessurfer/article/details/41310525 Logistic回归为概率型非线性回归模型，是研究二分类观察结果 Y 与一些影响因素(x1,x2,..,xn)之间关系的一种多变量分析方法。通常的问题是，研究某些因素条件下某个结果是否发生，比如医学中根据病人的一些症状来判断它是否患有某种病。 LR分类器: Logistic Regression Classifier在分类情形下，经过学习后的LR分类器是一组权值\(W=(w_0,w_1,…,w_n)\)，当测试样本的数据输入时，这组权值与测试数据按照线性加和得到: x=w\_0+ w\_1x\_1+...+w\_nx\_n之后按照Logistic(sigmoid函数)的形式求出 f(x)=\frac{1}{1+e^x}由于Logistic函数的定义域为(-inf,inf)，值域为(0,1)，因此最基本的LR分类器适合对两类目标进行分类。所以Logistic回归最关键的问题就是研究如何求得权值\(W=(w_0,w_1,…,w_n)\)。用极大似然估计。 Logistic回归模型Logistic回归模型可以表示为 考虑具有个独立变量的向量\(X=(x_0,x_1,…,x_n)\)， (1)在变量X条件下某事件发生的概率p为： P(y=1|x)=\pi(x)=\frac{1}{1+e^{-g(x)}}其中\(g(x)=w_0+ w_1x_1+…+w_nx_n\) (2)在变量X条件下不发生的概率1-p为: P(y=0|x)=1-P(y=1|x)= 1-\frac{1}{1+e^{-g(x)}} =\frac{1}{1+e^{g(x)}}(3)事件的发生比（the odds of experiencing an event）:事件发生与不发生的概率之比odds \frac{P(y=1|x)}{P(y=0|x)}=\frac{p}{1-p} =e^{g(x)}ln(\frac{p}{1-p})=g(x)=w\_0+ w\_1x\_1+...+w\_nx\_nLogistic回归极大似然估计假设有m个观测样本\(X=(x_0,x_1,…,x_m)\)，观测值分别为\(Y=(y_0,y_1,…,y_m)\)，设给定条件下时间发生的概率\(p_i=P(y_i=1|x_i)\)，时间不发生的概率为\(P(y_i=0|x_i)=1-p_i\)。所以得到一个观测值的概率为 P(y_i)=p\_i^{y\_i}(1-p\_i)^{1-y\_i}因为各个观测样本之间相互独立，整体事件发生的概率为各边缘分布的乘积，得到似然函数为 L(w)=\prod_{i=1}^{m}=\\{\pi(x\_i)\\}^{y\_i}\\{1-\pi(x\_i)\\}^{1-y\_i}然后用极大似然估计求得权值\(W=(w_0,w_1,…,w_n)\)的估计。]]></content>
  </entry>
  <entry>
    <title><![CDATA[R语言-线性回归-lm()]]></title>
    <url>%2Fhexo%2F2017%2F09%2F22%2F2017-09-22-R_function_regression%2F</url>
    <content type="text"><![CDATA[From http://blog.sina.com.cn/s/blog_6fbfcfb50102va2k.html 1. 回归的多面性12345678910111213回归类型 用途---简单线性 个量化的解释变量来预测一个量化的响应变量（一个因变量、一个自变量）多项式 一个量化的解释变量预测一个量化的响应变量，模型的关系是n阶多项式（一个预测变量，但同时包含变量的幂）多元线性 用两个或多个量化的解释变量预测一个量化的响应变量（不止一个预测变量）多变量 用一个或多个解释变量预测多个响应变量Logistic 用一个或多个解释变量预测一个类别型变量泊松 用一个或多个解释变量预测一个代表频数的响应变量Cox 用一个或多个解释变量预测一个事件（死亡、失败或旧病复发）发生的时间时间序列 对误差项相关的时间序列数据建模非线性 用一个或多个量化的解释变量预测一个量化的响应变量，不过模型是非线性的非参数 用一个或多个量化的解释变量预测一个量化的响应变量，模型的形式源自数据形式，不事先设定稳健 用一个或多个量化的解释变量预测一个量化的响应变量，能抵御强影响点的干扰 2. 用lm()拟合回归模型myfit&lt;-lm(formula,data) formula指要拟合的模型形式，data是一个数据框，包含了用于拟合模型的数据。 formula形式如下：Y~X1+X2+……+Xk （~左边为响应变量，右边为各个预测变量，预测变量之间用+符号分隔）。 R表达式中常用的符号123456789101112符号 用途---~ | 分隔符号，左边为响应变量，右边为解释变量，eg：要通过x、z和w预测y，代码为y~x+z+w+ | 分隔预测变量： | 表示预测变量的交互项 eg：要通过x、z及x与z的交互项预测y，代码为y~x+z+x:z* | 表示所有可能交互项的简洁方式，代码y~x*z*w可展开为y~x+z+w+x:z+x:w+z:w+x:z:w^ | 表示交互项达到某个次数，代码y~(x+z+w)^2可展开为y~x+z+w+x:z+x:w+z:w. | 表示包含除因变量外的所有变量，eg：若一个数据框包含变量x、y、z和w，代码y~.可展开为y~x+z+w- | 减号，表示从等式中移除某个变量，eg：y~(x+z+w)^2-x:w可展开为y~x+z+w+x:z+z:w-1 | 删除截距项，eg：表示y~x-1拟合y在x上的回归，并强制直线通过原点I（） | 从算术的角度来解释括号中的元素。Eg：y~x+(z+w)^2将展开为y~x+z+w+z:w。相反，代码y~x+I((z+w)^2)将展开为y~x+h，h是一个由z和w的平方和创建的新变量function | 可以在表达式中用的数学函数，例如log(y)~x+z+w表示通过x、z和w来预测log(y) 交互项是指你的几个变量一块生成了一个新的影响，比如不同性别的不同专业可能会对成绩有不同的影响，性别影响成绩，专业影响成绩，但是性别和专业和在一起又产生新影响。这时候就需要交互项。具体用不用看你的方程,一般不用。 对拟合线性模型非常有用的其他函数12345678910111213函数 用途---Summary（）展示拟合的详细结果Coefficients（）列出拟合模型的模型参数（截距项和斜率）confint（） 提供模型参数的置信区间（默认95%）fitted（）列出拟合模型的预测值residuals（）列出拟合模型的残差值anova（）生成一个拟合模型的方差分析，或者比较两个或更多拟合模型的方差分析表deviance() 计算残差平和和vcov（）列出模型参数的协方差矩阵AIC（）输出赤池信息统计量plot（）生成评价拟合模型的诊断图predict（）用拟合模型对新的数据集预测响应变量值 3. R 语言示例lm()12345678910111213141516171819202122232425fit&lt;-lm(weight~height,data=women) # summary(fit)# Call:# lm(formula = weight ~ height, data = women)# Residuals:# Min 1Q Median 3Q Max # -1.7333 -1.1333 -0.3833 0.7417 3.1167 # Coefficients:# Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -87.51667 5.93694 -14.74 1.71e-09 ***# height 3.45000 0.09114 37.85 1.09e-14 ***# ---# Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1# Residual standard error: 1.525 on 13 degrees of freedom# Multiple R-squared: 0.991, Adjusted R-squared: 0.9903 # F-statistic: 1433 on 1 and 13 DF, p-value: 1.091e-14coef(fit)#(Intercept) height # -87.51667 3.45000 fitted(fit)#拟合模型的预测值 residuals(fit)#拟合模型的残差值 plot(women$height,women$weight, xlab="Height （in inches）", ylab="Weight（in pounds）") abline(fit) 1.自变量评估 在Pr(&gt;|t|)栏，可以看到回归系数（3.45）或者截距项显著性&lt;0.05，拒绝原假设，表明身高每增加1英寸，体重将预期地增加3.45磅. *原假设H0：系数（或截距）为0。 *备选假设H1：系数（或截距）不为0。 2.判定系数和F统计量 残差的标准误(Residual standard error) 1.53 lbs则可认为模型用身高预测体重的平均误差. 判定系数R^2 R^2= \frac{SSR}{SST}=\frac{\sum(\hat{Y\_i}-\overline{Y})^2}{\sum(Y_i-\overline{Y})^2}R^2的取值范围为[0,1],越接近1表示回归模型对数据的解释能力越强。 F统计量使用F分布检验MSR/MSE的比率，原假设：β1=0，备选假设：β1≠0. 检验简化模型\(dist=β0+\varepsilon\)与完整模型\(dist=β0+β1*x +\varepsilon\)之间的残差平方和差异的显著程度。 3.方差分析 线性回归中，方差分析用于评估模型或者进行模型间比较。 123456789101112131415161718192021222324252627282930313233343536 m=lm(dist ~ speed, data=cars) summary(m)# Call:# lm(formula = dist ~ speed, data = cars)# Residuals:# Min 1Q Median 3Q Max # -29.069 -9.525 -2.272 9.215 43.201 # Coefficients:# Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -17.5791 6.7584 -2.601 0.0123 * # speed 3.9324 0.4155 9.464 1.49e-12 ***# ---# Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1# Residual standard error: 15.38 on 48 degrees of freedom# Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 # F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12anova(m)# Analysis of Variance Table# Response: dist# Df Sum Sq Mean Sq F value Pr(&gt;F) # speed 1 21186 21185.5 89.567 1.49e-12 ***# Residuals 48 11354 236.5 # ---# Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1reduced=lm(dist ~ 1, data=cars) #没有自变量，仅有截距项的简化模型anova(reduced,m)# Analysis of Variance Table# Model 1: dist ~ 1# Model 2: dist ~ speed# Res.Df RSS Df Sum of Sq F Pr(&gt;F) # 1 49 32539 # 2 48 11354 1 21186 89.567 1.49e-12 ***# ---# Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 以上3组比较F统计量中p值都为1.49e-12。表面简化模型与完整模型之间存在显著差异，换言之，speed是有意义的解释变量。 4.模型诊断图形 1plot(m,which=c(1:3,5)) 第一个图为预测值Y（X轴）与残差（Y轴），在线性回归中，由于假设误差服从均值为0，方差固定的正态分布，所以认为残差分布与预测的Y值无关，残差均值必须为0。斜率为0的直线是理想情形。 第二个图为正态QQ图，查看标准化的残差是否服从正态分布。 第三个图为预测值Y（X轴）与标准化残差（Y轴），斜率为0的直线是理想情形。若在特定点观察到距离0较远的值，说明该点不能很好的拟合原始值，这些点肯能成为异常点。 第四个图为杠杆值（X轴）与标准化残差（Y轴）。杠杆值表示解释变量向极端的偏斜程度。 分类变量 123456789101112131415161718192021m=lm(Sepal.Length ~ .,data=iris) summary(m)# Call:# lm(formula = Sepal.Length ~ ., data = iris)# Residuals:# Min 1Q Median 3Q Max # -0.79424 -0.21874 0.00899 0.20255 0.73103 # Coefficients:# Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.17127 0.27979 7.760 1.43e-12 ***# Sepal.Width 0.49589 0.08607 5.761 4.87e-08 ***# Petal.Length 0.82924 0.06853 12.101 &lt; 2e-16 ***# Petal.Width -0.31516 0.15120 -2.084 0.03889 * # Speciesversicolor -0.72356 0.24017 -3.013 0.00306 ** # Speciesvirginica -1.02350 0.33373 -3.067 0.00258 ** # ---# Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1# Residual standard error: 0.3068 on 144 degrees of freedom# Multiple R-squared: 0.8673, Adjusted R-squared: 0.8627 # F-statistic: 188.3 on 5 and 144 DF, p-value: &lt; 2.2e-16 其中Species为分类变量，有三种类别：setosa,versicolor,virginica. 4.多项式回归123456789101112131415161718fit2&lt;-lm(weight~height+I(height^2),data=women) summary(fit2)plot(women$height,women$weight, xlab="Height（in inches）", ylab="Weight（in lbs）") lines(women$height,fitted(fit2)) #一般来说，n次多项式生成一个n-1个弯曲的曲线#car包中的scatterplot（）函数，可以很容易、方便地绘制二元关系图library(car)scatterplot(weight~height, data=women, spread=FALSE, lty.smooth=2, pch=19, main="Women Age 30-39", xlab="Height (inches)", ylab="Weight(lbs.)") 6.多元线性回归1234567states&lt;-as.data.frame(state.x77[,c("Murder","Population","Illiteracy","Income","Frost")])cor(states) scatterplotMatrix(states,spread=FALSE,lty.smooth=2,main="Scatter Plot Matrix")#scatterplotMatrix（）函数默认在非对角线区域绘制变量间的散点图，并添加平滑（loess）和线性拟合曲线#多元线性回归fit&lt;-lm(Murder~Population+Illiteracy+Income+Frost,data=states) summary(fit) 对于F统计量，原假设：所有系数β全为0。 7.有交互项的多元线性回归123456789101112fit&lt;-lm(mpg~hp+wt+hp:wt,data=mtcars) summary(fit) # 通过effects包中的effect（）函数，可以用图形展示交互项的结果install.packages("effects") library(effects) plot(effect("hp:wt",fit, list(wt=c(2.2,3.2,4.2))),multiline=TRUE) # 二次拟合诊断图fit2&lt;-lm(weight~height+I(height^2),data=women) par(mfrow=c(2,2)) plot(fit2) I()防止对象解析或者转换，例如I(X^2)表示以X^2作为一个独立变量参与到回归，否者则解析为x+x+x:x交互作用。 异常值学生化残差： 残差与残差标准差的比值。 外部学生化残差：计算第i个学生化残差时，先去掉i再计算标准差。一般用外部学生化残差评估异常点。 R中rstudent()计算外部学生化残差。外部学生化残差服从t分布，可以使用t-test来寻找rstudent()值过大或者过小的点。 12345678m=lm(circumference~ age+I(age^2),data=Orange)rstudent(m)car::outlierTest(m)# No Studentized residuals with Bonferonni p &lt; 0.05# Largest |rstudent|:# rstudent unadjusted p-value Bonferonni p# 27 2.050328 0.04887 NA]]></content>
  </entry>
  <entry>
    <title><![CDATA[R语言 逐步回归分析]]></title>
    <url>%2Fhexo%2F2017%2F09%2F22%2F2017-09-22-step_regression%2F</url>
    <content type="text"><![CDATA[From http://www.cnblogs.com/liuzezhuang/p/3724497.html 逐步回归分析是以AIC信息统计量为准则，通过选择最小的AIC信息统计量，来达到删除或增加变量的目的。 R语言中用于逐步回归分析的函数 step() drop1() add1() 1.载入数据1234567891011#首先对数据进行多元线性回归分析tdata&lt;-data.frame( x1=c( 7, 1,11,11, 7,11, 3, 1, 2,21, 1,11,10), x2=c(26,29,56,31,52,55,71,31,54,47,40,66,68), x3=c( 6,15, 8, 8, 6, 9,17,22,18, 4,23, 9, 8), x4=c(60,52,20,47,33,22, 6,44,22,26,34,12,12), Y =c(78.5,74.3,104.3,87.6,95.9,109.2,102.7,72.5, 93.1,115.9,83.8,113.3,109.4))tlm&lt;-lm(Y~x1+x2+x3+x4,data=tdata)summary(tlm) 通过观察，回归方程的系数都没有通过显著性检验 2.逐步回归分析12tstep&lt;-step(tlm)summary(tstep) 结果分析： 当用x1 x2 x3 x4作为回归方程的系数时，AIC的值为26.94 去掉x3 回归方程的AIC值为24.974；去掉x4 回归方程的AIC值为25.011…… 由于去x3可以使得AIC达到最小值，因此R会自动去掉x3; *去掉x3之后 AIC的值都增加 逐步回归分析终止, 得到当前最优的回归方程 回归系数的显著性水平有所提高 但是x2 x4的显著性水平仍然不理想 3.逐步回归分析的优化1drop1(tstep) 4.进一步进行多元回归分析12tlm&lt;-lm(Y~x1+x2,data=tdata)summary(tlm) 所有的检验均为显著 因此所得回归方程为1y=52.57735+ 1.46831x1+ 0.66225x2.]]></content>
  </entry>
  <entry>
    <title><![CDATA[模型选择准则之AIC和BIC]]></title>
    <url>%2Fhexo%2F2017%2F09%2F22%2F2017-09-22-AIC_BIC%2F</url>
    <content type="text"><![CDATA[转自：http://blog.csdn.net/jteng/article/details/40823675 很多参数估计问题均采用似然函数作为目标函数，当训练数据足够多时，可以不断提高模型精度，但是以提高模型复杂度为代价的，同时带来一个机器学习中非常普遍的问题——过拟合。所以，模型选择问题在模型复杂度与模型对数据集描述能力（即似然函数）之间寻求最佳平衡。人们提出许多信息准则，通过加入模型复杂度的惩罚项来避免过拟合问题，此处我们介绍一下常用的两个模型选择方法. 赤池信息准则（Akaike Information Criterion，AIC）AIC是衡量统计模型拟合优良性的一种标准，由日本统计学家赤池弘次在1974年提出，它建立在熵的概念上，提供了权衡估计模型复杂度和拟合数据优良性的标准。通常情况下，AIC定义为： 1AIC=2k-2ln(L) 其中k是模型参数个数，L是似然函数。从一组可供选择的模型中选择最佳模型时，通常选择AIC最小的模型。当两个模型之间存在较大差异时，差异主要体现在似然函数项，当似然函数差异不显著时，上式第一项，即模型复杂度则起作用，从而参数个数少的模型是较好的选择。一般而言，当模型复杂度提高（k增大）时，似然函数L也会增大，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度（极大似然），而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。 贝叶斯信息准则（Bayesian Information Criterion，BIC）BIC（Bayesian InformationCriterion）贝叶斯信息准则与AIC相似，用于模型选择,BIC越小，模型越优，1978年由Schwarz提出。训练模型时，增加参数数量，也就是增加模型复杂度，会增大似然函数，但是也会导致过拟合现象，针对该问题，AIC和BIC均引入了与模型参数个数相关的惩罚项，BIC的惩罚项比AIC的大，考虑了样本数量，样本数量过多时，可有效防止模型精度过高造成的模型复杂度过高。 1BIC=kln(n)-2ln(L) 其中，k为模型参数个数，n为样本数量，L为似然函数。kln(n)惩罚项在维数过大且训练样本数据相对较少的情况下，可以有效避免出现维度灾难现象。]]></content>
  </entry>
  <entry>
    <title><![CDATA[常见的机器学习&数据挖掘知识点]]></title>
    <url>%2Fhexo%2F2017%2F09%2F22%2F2017-09-22-datamining_concepts%2F</url>
    <content type="text"><![CDATA[Basis(基础)：1234567891011121314151617181920212223242526272829SSE(Sum of Squared Error, 平方误差和)SAE(Sum of Absolute Error, 绝对误差和)SRE(Sum of Relative Error, 相对误差和)MSE(Mean Squared Error, 均方误差)RMSE(Root Mean Squared Error, 均方根误差)RRSE(Root Relative Squared Error, 相对平方根误差)MAE(Mean Absolute Error, 平均绝对误差)RAE(Root Absolute Error, 平均绝对误差平方根)MRSE(Mean Relative Square Error, 相对平均误差)RRSE(Root Relative Squared Error, 相对平方根误差)Expectation(期望)&amp;Variance(方差)Standard Deviation(标准差，也称Root Mean Squared Error, 均方根误差)CP(Conditional Probability, 条件概率)JP(Joint Probability, 联合概率)MP(Marginal Probability, 边缘概率)Bayesian Formula(贝叶斯公式)CC(Correlation Coefficient, 相关系数)Quantile (分位数)Covariance(协方差矩阵)GD(Gradient Descent, 梯度下降)SGD(Stochastic Gradient Descent, 随机梯度下降)LMS(Least Mean Squared, 最小均方)LSM(Least Square Methods, 最小二乘法)NE(Normal Equation, 正规方程)MLE(Maximum Likelihood Estimation, 极大似然估计)QP(Quadratic Programming, 二次规划)L1 /L2 Regularization(L1/L2正则, 以及更多的, 现在比较火的L2.5正则等)Eigenvalue(特征值)Eigenvector(特征向量) Common Distribution(常见分布)：123456789101112131415161718192021222324Discrete Distribution(离散型分布)：Bernoulli Distribution/Binomial Distribution(贝努利分布/二项分布)Negative Binomial Distribution(负二项分布)Multinomial Distribution(多项分布)Geometric Distribution(几何分布)Hypergeometric Distribution(超几何分布)Poisson Distribution (泊松分布)Continuous Distribution (连续型分布)：Uniform Distribution(均匀分布)Normal Distribution/Gaussian Distribution(正态分布/高斯分布)Exponential Distribution(指数分布)Lognormal Distribution(对数正态分布)Gamma Distribution(Gamma分布)Beta Distribution(Beta分布)Dirichlet Distribution(狄利克雷分布)Rayleigh Distribution(瑞利分布)Cauchy Distribution(柯西分布)Weibull Distribution (韦伯分布)Three Sampling Distribution(三大抽样分布)：Chi-square Distribution(卡方分布)t-distribution(t-分布)F-distribution(F-分布) Data Pre-processing(数据预处理)：1234Missing Value Imputation(缺失值填充)Discretization(离散化)Mapping(映射)Normalization(归一化/标准化) Sampling(采样)：1234567Simple Random Sampling(简单随机采样)Offline Sampling(离线等可能K采样)Online Sampling(在线等可能K采样)Ratio-based Sampling(等比例随机采样)Acceptance-rejection Sampling(接受-拒绝采样)Importance Sampling(重要性采样)MCMC(Markov Chain MonteCarlo 马尔科夫蒙特卡罗采样算法：Metropolis-Hasting&amp; Gibbs) Clustering(聚类)：12345678910111213K-MeansK-Mediods二分K-MeansFK-MeansCanopySpectral-KMeans(谱聚类)GMM-EM(混合高斯模型-期望最大化算法解决)K-PototypesCLARANS(基于划分)BIRCH(基于层次)CURE(基于层次)STING(基于网格)CLIQUE(基于密度和基于网格)2014年Science上的密度聚类算法等 Clustering Effectiveness Evaluation(聚类效果评估)：12345Purity(纯度)RI(Rand Index, 芮氏指标)ARI(Adjusted Rand Index, 调整的芮氏指标)NMI(Normalized Mutual Information, 规范化互信息)F-meaure(F测量) Classification&amp;Regression(分类&amp;回归)：1234567891011121314151617181920212223242526LR(Linear Regression, 线性回归)LR(Logistic Regression, 逻辑回归)SR(Softmax Regression, 多分类逻辑回归)GLM(Generalized Linear Model, 广义线性模型)RR(Ridge Regression, 岭回归/L2正则最小二乘回归)，LASSO(Least Absolute Shrinkage and Selectionator Operator , L1正则最小二乘回归)DT(Decision Tree决策树)RF(Random Forest, 随机森林)GBDT(Gradient Boosting Decision Tree, 梯度下降决策树)CART(Classification And Regression Tree 分类回归树)KNN(K-Nearest Neighbor, K近邻)SVM(Support Vector Machine, 支持向量机, 包括SVC(分类)&amp;SVR(回归))CBA(Classification based on Association Rule, 基于关联规则的分类)KF(Kernel Function, 核函数) Polynomial Kernel Function(多项式核函数)Guassian Kernel Function(高斯核函数)Radial Basis Function(RBF径向基函数)String Kernel Function 字符串核函数NB(Naive Bayesian,朴素贝叶斯)BN(Bayesian Network/Bayesian Belief Network/Belief Network 贝叶斯网络/贝叶斯信度网络/信念网络)LDA(Linear Discriminant Analysis/Fisher Linear Discriminant 线性判别分析/Fisher线性判别)EL(Ensemble Learning, 集成学习) BoostingBaggingStackingAdaBoost(Adaptive Boosting 自适应增强)MEM(Maximum Entropy Model, 最大熵模型) Classification EffectivenessEvaluation(分类效果评估)：123456789Confusion Matrix(混淆矩阵)Precision(精确度)Recall(召回率)Accuracy(准确率)F-score(F得分)ROC Curve(ROC曲线)AUC(AUC面积)Lift Curve(Lift曲线)KS Curve(KS曲线) PGM(Probabilistic Graphical Models, 概率图模型)：1234567891011121314151617181920BN(BayesianNetwork/Bayesian Belief Network/ Belief Network , 贝叶斯网络/贝叶斯信度网络/信念网络)MC(Markov Chain, 马尔科夫链)MEM(Maximum Entropy Model, 最大熵模型)HMM(Hidden Markov Model, 马尔科夫模型)MEMM(Maximum Entropy Markov Model, 最大熵马尔科夫模型)CRF(Conditional Random Field,条件随机场)MRF(Markov Random Field, 马尔科夫随机场)Viterbi(维特比算法)NN(Neural Network, 神经网络)ANN(Artificial Neural Network, 人工神经网络)SNN(Static Neural Network, 静态神经网络)BP(Error Back Propagation, 误差反向传播)HN(Hopfield Network)DNN(Dynamic Neural Network, 动态神经网络)RNN(Recurrent Neural Network, 循环神经网络)SRN(Simple Recurrent Network, 简单的循环神经网络)ESN(Echo State Network, 回声状态网络)LSTM(Long Short Term Memory, 长短记忆神经网络)CW-RNN(Clockwork-Recurrent Neural Network, 时钟驱动循环神经网络, 2014ICML）等. Deep Learning(深度学习)：123456789Auto-encoder(自动编码器)SAE(Stacked Auto-encoders堆叠自动编码器) Sparse Auto-encoders(稀疏自动编码器)Denoising Auto-encoders(去噪自动编码器)Contractive Auto-encoders(收缩自动编码器)RBM(Restricted Boltzmann Machine, 受限玻尔兹曼机)DBN(Deep Belief Network, 深度信念网络)CNN(Convolutional Neural Network, 卷积神经网络)Word2Vec(词向量学习模型) Dimensionality Reduction(降维)：12345678910111213141516171819202122232425262728LDA(Linear Discriminant Analysis/Fisher Linear Discriminant, 线性判别分析/Fish线性判别)PCA(Principal Component Analysis, 主成分分析)ICA(Independent Component Analysis, 独立成分分析)SVD(Singular Value Decomposition 奇异值分解)FA(Factor Analysis 因子分析法)Text Mining(文本挖掘)：VSM(Vector Space Model, 向量空间模型)Word2Vec(词向量学习模型)TF(Term Frequency, 词频)TF-IDF(TermFrequency-Inverse Document Frequency, 词频-逆向文档频率)MI(Mutual Information, 互信息)ECE(Expected Cross Entropy, 期望交叉熵)QEMI(二次信息熵)IG(Information Gain, 信息增益)IGR(Information Gain Ratio, 信息增益率)Gini(基尼系数)x2 Statistic(x2统计量)TEW(Text Evidence Weight, 文本证据权)OR(Odds Ratio, 优势率)N-Gram ModelLSA(Latent Semantic Analysis, 潜在语义分析)PLSA(Probabilistic Latent Semantic Analysis, 基于概率的潜在语义分析)LDA(Latent Dirichlet Allocation, 潜在狄利克雷模型)SLM(Statistical Language Model, 统计语言模型)NPLM(Neural Probabilistic Language Model, 神经概率语言模型)CBOW(Continuous Bag of Words Model, 连续词袋模型)Skip-gram(Skip-gram Model) Association Mining(关联挖掘)：12345678910111213141516171819202122232425Apriori算法FP-growth(Frequency Pattern Tree Growth, 频繁模式树生长算法)MSApriori(Multi Support-based Apriori, 基于多支持度的Apriori算法)GSpan(Graph-based Substructure Pattern Mining, 频繁子图挖掘)Sequential Patterns Analysis(序列模式分析)AprioriAllSpadeGSP(Generalized Sequential Patterns, 广义序列模式)PrefixSpanForecast(预测)LR(Linear Regression, 线性回归)SVR(Support Vector Regression, 支持向量机回归)ARIMA(Autoregressive Integrated Moving Average Model, 自回归积分滑动平均模型)GM(Gray Model, 灰色模型)BPNN(BP Neural Network, 反向传播神经网络)SRN(Simple Recurrent Network, 简单循环神经网络)LSTM(Long Short Term Memory, 长短记忆神经网络)CW-RNN(Clockwork Recurrent Neural Network, 时钟驱动循环神经网络)……Linked Analysis(链接分析)HITS(Hyperlink-Induced Topic Search, 基于超链接的主题检索算法)PageRank(网页排名) Recommendation Engine(推荐引擎)：1234567SVDSlope OneDBR(Demographic-based Recommendation, 基于人口统计学的推荐)CBR(Context-based Recommendation, 基于内容的推荐)CF(Collaborative Filtering, 协同过滤)UCF(User-based Collaborative Filtering Recommendation, 基于用户的协同过滤推荐)ICF(Item-based Collaborative Filtering Recommendation, 基于项目的协同过滤推荐) Similarity Measure&amp;Distance Measure(相似性与距离度量)：1234567891011EuclideanDistance(欧式距离)Chebyshev Distance(切比雪夫距离)Minkowski Distance(闵可夫斯基距离)Standardized EuclideanDistance(标准化欧氏距离)Mahalanobis Distance(马氏距离)Cos(Cosine, 余弦)Hamming Distance/Edit Distance(汉明距离/编辑距离)Jaccard Distance(杰卡德距离)Correlation Coefficient Distance(相关系数距离)Information Entropy(信息熵)KL(Kullback-Leibler Divergence, KL散度/Relative Entropy, 相对熵) Optimization(最优化)：Non-constrained Optimization(无约束优化)：12345Cyclic Variable Methods(变量轮换法)Variable Simplex Methods(可变单纯形法)Newton Methods(牛顿法)Quasi-Newton Methods(拟牛顿法)Conjugate Gradient Methods(共轭梯度法)。 Constrained Optimization(有约束优化)：1234567Approximation Programming Methods(近似规划法)Penalty Function Methods(罚函数法)Multiplier Methods(乘子法)。Heuristic Algorithm(启发式算法)SA(Simulated Annealing, 模拟退火算法)GA(Genetic Algorithm, 遗传算法)ACO(Ant Colony Optimization, 蚁群算法) Feature Selection(特征选择)：12345Mutual Information(互信息)Document Frequence(文档频率)Information Gain(信息增益)Chi-squared Test(卡方检验)Gini(基尼系数) Outlier Detection(异常点检测)：123Statistic-based(基于统计)Density-based(基于密度)Clustering-based(基于聚类)。 Learning to Rank(基于学习的排序)：1234567891011Pointwise McRankPairwise RankingSVMRankNetFrankRankBoost；Listwise AdaRankSoftRankLamdaMART Tool(工具)：12345678910MPIHadoop生态圈SparkIGraphBSPWekaMahoutScikit-learnPyBrainTheano]]></content>
  </entry>
  <entry>
    <title><![CDATA[SSE, MSE, RMSE, R-square]]></title>
    <url>%2Fhexo%2F2017%2F09%2F22%2F2017-09-22-SSE_MSE_RMSE_R-square%2F</url>
    <content type="text"><![CDATA[12345SSE(和方差、误差平方和)：The sum of squares due to errorMSE(均方误、方差)：Mean squared errorRMSE(均方根、标准差)：Root mean squared errorR-square(确定系数)：Coefficient of determinationAdjusted R-square：Degree-of-freedom adjusted coefficient of determination SSE(和方差、误差平方和)拟合数据和原始数据对应点的误差的平方和 SSE越接近于0，说明模型选择和拟合更好，数据预测也越成功。接下来的MSE和RMSE因为和SSE是同出一宗，所以效果一样。 MSE(均方误)预测数据和原始数据对应点误差的平方和的均值，也就是SSE/n，和SSE没有太大的区别，最常用！ RMSE(均方根)回归系统的拟合标准差，是MSE的平方根，就算公式如下 在这之前，我们所有的误差参数都是基于预测值(y_hat)和原始值(y)之间的误差(即点对点)。从下面开始是所有的误差都是相对原始数据平均值(y_ba)而展开的(即点对全) R-square(确定系数)(1)SSR：Sum of squares of the regression，即预测数据与原始数据均值之差的平方和 (2)SST：Total sum of squares，即原始数据和均值之差的平方和 SST=SSE+SSR “确定系数”是定义为SSR和SST的比值 R-square是通过数据的变化来表征一个拟合的好坏。由上面的表达式可以知道“确定系数”的正常取值范围为[0 1]，越接近1，表明方程的变量对y的解释能力越强，这个模型对数据拟合的也较好。]]></content>
  </entry>
  <entry>
    <title><![CDATA[线性回归-岭回归-Lasso-弹性网-多重共线性]]></title>
    <url>%2Fhexo%2F2017%2F09%2F22%2F2017-09-22-Ridge_Lasso_glmnet%2F</url>
    <content type="text"><![CDATA[原文 1. 回归问题的数学描述1.n个样本，p个变量，X，y已知。对数据中心化、标准化处理后，可以去掉截距项。 2.矩阵形式的多元线性模型为: 求解β，使得误差项ε能达到较低. 3.残差平方和RSS为 4.多元线性回归问题变为求解β，从而使残差平方和极小值问题（关于系数向量β的二次函数极值问题） 5.几何意义 残差向量的几何意义：响应y向量到由p个x向量组成的超平面的距离向量。残差平方和几何意义：残差向量长度的平方。 2.最小二乘回归使用最小二乘法拟合的普通线性回归是数据建模的基本方法。其建模要点在于误差项一般要求独立同分布（常假定为正态）零均值。t检验用来检验拟合的模型系数的显著性，F检验用来检验模型的显著性（方差分析）。如果正态性不成立，t检验和F检验就没有意义。 β的最小二乘估计为： 在统计学上，可证明β的最小二乘解为无偏估计，即多次得到的采样值X而计算出来的多个系数估计值向量的平均值将无限接近于真实值向量β。 如果存在较强的共线性，即X中各列向量之间存在较强的相关性，会导致|X^T X|≈0, 从而引起对角线上的值很大(X^T X的逆矩阵不不存在) 问题： X矩阵不存在广义逆（即奇异性）的情况。 X本身存在线性相关关系（即多重共线性），即非满秩矩阵。当采样值误差造成本身线性相关的样本矩阵仍然可以求出逆阵时，此时的逆阵非常不稳定，所求的解也没有什么意义。 当变量比样本多，即p&gt;n时.回归系数会变得很大，无法求解。 对较复杂的数据建模（比如文本分类，图像去噪或者基因组研究）的时候，普通线性回归会有一些问题：（1）预测精度的问题 如果响应变量和预测变量之间有比较明显的线性关系，最小二乘回归会有很小的偏倚，特别是如果观测数量n远大于预测变量p时，最小二乘回归也会有较小的方差。但是如果n和p比较接近，则容易产生过拟合；如果n&lt;p，最小二乘回归得不到有意义的结果。 （2）模型解释能力的问题 包括在一个多元线性回归模型里的很多变量可能是和响应变量无关的；也有可能产生多重共线性的现象：即多个预测变量之间明显相关。这些情况都会增加模型的复杂程度，削弱模型的解释能力。这时候需要进行变量选择（特征选择）。 针对OLS (ordinary least squares)的问题，在变量选择方面有三种扩展的方法： （1）子集选择 这是传统的方法，包括逐步回归和最优子集法等，对可能的部分子集拟合线性模型，利用判别准则 （如AIC,BIC,Cp,调整R2 等）决定最优的模型。 （2）收缩方法（shrinkage method） 收缩方法又称为正则化（regularization）。主要是岭回归（ridge regression）和lasso回归。通过对最小二乘估计加入罚约束，使某些系数的估计为0。 (3)维数缩减 主成分回归（PCR）和偏最小二乘回归（PLS）的方法。把p个预测变量投影到m维空间（m&lt;p），利用投影得到的不相关的组合建立线性模型。 3.岭回归（Ridge Regression，RR, 1962）思路：在原先的β的最小二乘估计中加一个小扰动λI，是原先无法求广义逆的情况变成可以求出其广义逆，使得问题稳定并得以求解。 极值问题： 对上式用偏导数求极值，结果就是 其中为惩罚函数，它保证了β值不会变的很大。岭参数λ不同，岭回归系数也会不同。 岭回归是回归参数β的有偏估计。它的结果是使得残差平和变大，但是会使系数检验变好，即R语言summary结果中变量后的*变多。 岭回归缺陷: 1.主要靠目测选择岭参数 2.计算岭参数时，各种方法结果差异较大 所以一般认为，岭迹图只能看多重共线性，却很难做变量筛选 4.几何解释以两个变量为例，系数β1和β2已经经过标准化。残差平方和RSS可以表示为β1和β2的一个二次函数，数学上可以用一个抛物面表示。 最小二乘法 2.岭回归 约束项为 β1^2+β2^2≤t 对应着投影为β1和β2平面上的一个圆，即下图中的圆柱. 该圆柱与抛物面的交点对应的β1、β2值，即为满足约束项条件下的能取得的最小的β1和β2. 从β1,β2平面理解，即为抛物面等高线在水平面的投影和圆的交点，如下图所示,可见岭回归解与原先的最小二乘解是有一定距离的。 3.岭回归性质 4.岭迹图 岭迹图作用： 1）观察λ较佳取值； 2）观察变量是否有多重共线性； 是λ的函数，岭迹图的横坐标为λ，纵坐标为β(λ)。而β(λ)是一个向量，由β1(λ)、β2(λ)、…等很多分量组成，每一个分量都是λ的函数，将每一个分量分别用一条线。当不存在奇异性时，岭迹应是稳定地逐渐趋向于0。 岭迹图比较 通过岭迹的形状来判断我们是否要剔除掉该参数（例如：岭迹波动很大，说明该变量参数有共线性） 可见，在λ很小时，通常各β系数取值较大；而如果λ=0，则跟普通意义的多元线性回归的最小二乘解完全一样；当λ略有增大，则各β系数取值迅速减小，即从不稳定趋于稳定。上图类似喇叭形状的岭迹图，一般都存在多重共线性。 λ的选择：一般通过观察，选取喇叭口附近的值，此时各β值已趋于稳定，但总的RSS又不是很大。 选择变量：删除那些β取值一直趋于0的变量。 注意：用岭迹图筛选变量并非十分靠谱。 岭回归选择变量的原则（不靠谱，仅供参考） 1）在岭回归中设计矩阵X已经中心化和标准化了，这样可以直接比较标准化岭回归系数癿大小。可以剔除掉标准化岭回归系数比较稳定且值很小癿自变量。 2）随着λ的增加，回归系数不稳定，震动趋于零的自变量也可以剔除。 3）如果依照上述去掉变量的原则，有若干个回归系数不稳定，究竟去掉几个，去掉哪几个，这幵无一般原则可循，这需根据去掉某个变量后重新进行岭回归分析的效果来确定。 5.岭回归R语言分析 123456789101112131415library(MASS)#岭回归在MASS包中。longley #内置数据集，有关国民经济情况的数据，以多重共线性较强著称summary(fm1&lt;-lm(Employed~.,data=longley)) #最小二乘估计的多元线性回归#结果可见，R^2很高，但是系数检验不是非常理想names(longley)[1]&lt;-&quot;y&quot; lm.ridge(y~.,longley) #此时，仍为线性回归plot(lm.ridge(y~.,longley,lambda=seq(0,0.1,0.001))) #加了参数lambda的描述后才画出响应的岭迹图#由于lambda趋于0时，出现了不稳定的情况，所以可以断定变量中存在多重共线性select(lm.ridge(y~.,longley,lambda=seq(0,0.1,0.001))) #用select函数可算lambda值，结果给出了3种方法算的的lambda的估计值## modified HKB estimator is 0.006836982 ## modified L-W estimator is 0.05267247 ## smallest value of GCV at 0.006 #以上结果通常取GCV估计，或者观察大多数方法趋近哪个值。 5. LASSOTibshirani(1996)提出了Lasso(The Least Absolute Shrinkage and Selectionatoroperator)算法，这里 Absolute 指绝对值。Shrinkage收缩的含义：即系数收缩在一定区域内（比如圆内）。 主要思想：通过构造一个一阶惩罚函数获得一个精炼的模型；通过最终确定一些指标（变量）癿系数为零（岭回归估计系数等于0癿机会微乎其微，造成筛选变量困难），解释力很强。擅长处理具有多重共线性癿数据，筛选变量，与岭回归一样是有偏估计。 几何解释 由于方框的顶点更容易交于抛物面，也就是lasso更易求解，而该顶点对应的很多系数为0，也就是起到了筛选变量的目的。 Lasso plot 12345678910111213141516set.seed(101)x=matrix(rnorm(1000),100,10)y=rnorm(100)fit=glmnet(x,y)par(mfrow=c(1,3))#par(mar=c(4.5,4.5,1,4))##plot1plot(fit)vnat=coef(fit)vnat=vnat[-1,ncol(vnat)] # remove the intercept, and get the coefficients at the end of the pathaxis(4, at=vnat,line=-.5,label=paste("feature",1:10),las=1,tick=FALSE, cex.axis=0.5)#plot2plot(fit, xvar = "lambda")# plot3plot(fit, xvar = "dev") 6.LASSO vs 岭回归岭回归一方面可以将其变成一个最小二乘问题。另一方面可以将它解释成一个带约束项的系数优化问题。λ增大的过程就是t减小的过程，该图也说明了岭回归系数估计值为什么通常不为0，因为随着抛物面的扩展，它与约束圆的交点可能在圆周上的任意位置，除非交点恰好位于某个坐标轴或坐标平面上，否则大多数情况交点对应的系数值都不为零。再加上λ的选择应使椭球面和圆周的交点恰好在一个坐标平面上，更增加了求解λ的难度。 左图为岭回归，右图为lasso回归。横轴越往左，自由度越小（即圆或方框在收缩的过程），λ越大，系数（即纵轴）会越趋于0。但是岭回归没有系数真正为0，但lasso的不断有系数变为0. 7.一般化的模型 不同q对应的约束域形状 8.弹性网模型Zouand Hastie (2005)提出elasticnet，介于岭回归和lasso回归之间，现在被认为是处理多重共线性和变量筛选较好的收缩方法，而且损失的精度不会太多。 9.最小角回归(Least Angel Regression)是lasso regression癿一种高效解法。Lasso回归中表达式用偏导求极值时，存在部分点不可导的情况（如方框的尖点），如何解决？ Efron于2004年提出癿一种变量选择癿方法，类似于向前逐步回归(Forward Stepwise)的形式，最初用于解决传统的线性回归问题，有清晰的几何意义。 与向前逐步回归(Forward Stepwise)不同点在于，Forward Step wise 每次都是根据选择的变量子集，完全拟合出线性模型，计算出RSS，再设计统计量（如AIC）对较高癿模型复杂度作出惩罚，而LAR是每次先找出和因变量相关度较高的那个变量, 再沿着LSE的方向一点点调整这个predictor的系数，在这个过程中，这个变量和残差的相关系数会逐渐减小，等到这个相关性没那么显著的时候，就要选进新的相关性较高的变量，然后重新沿着LSE的方向进行变动。而到最后，所有变量都被选中，就和LSE相同了。 左图为LAR逐步加上变量的过程（从左往右看），右图为LASSO变量逐渐淘汰的收缩过程（从右往左看）。对比两幅图，非常类似。所以可以用LAR方法来计算LASSO，该方法完全是线性解决方法，没有迭代的过程。 10. 相关系数的几何意义设变量y=[y1,y2,…yn]; 变量x=[x1,x2,…,xn]. 其相关系数为 其中cov—协方差、var—-方差。 如果对x和y进行中心化、标准化，则var(y)=var(x)=1,相关系数变为x1y1+x2y2+….+xnyn，即为向量x和y的内积=||x||||y||cos θ，其中θ为x和y的夹角。而对于标准化和中心化后的x和y，则有||x||=||y||=1，所以此时x和y的内积就是它们夹角的余弦。 如果x和y向量很像，几乎重合，则夹角θ=0，也就是相关系数=内积=1，此时称为高度相关. 如果x和y相关程度很低，则表现出来的x和y向量相互垂直，相关系数=0. 如果相关系数=-1，标明x和y呈180°，即负相关。 11. LAR算法及几何意义参考书The Elements of Statistical Learning .pdf的74页。LAR和Lasso的区别以及LAR解Lasso的修正参考书The Elements of Statistical Learning .pdf的76页。 假设有6个变量，最先加入与残差向量相关系数较大的浅蓝色v2，在v2变化过程中，相关系数越变越小，直到等于深蓝色的v6，于是加入v6，沿着v2与v6的最小角方向（即向量角分线方向）前进，此后v2和v6与残差向量的相关系数是共同变化的，即两者合并变化，使得相关系数越来越小，直到加入黑色v4为止，三个变量一起变化，…，一直打到最小二乘解为止，此时残差向量与所有变量的相关系数都为0，即与他们都垂直。 横坐标L1 Length表示：从原点开始走了多长距离，就是值距离，L1范数。 12. R语言中对LAR的实现123456789101112131415161718192021install.packages(&quot;lars&quot;) #lars包longley #用longley数据集，它是一个著名的多重共线性例子w=as.matrix(longley) #将数据集转换为一个矩阵laa=lars(w[,2:7],w[,1]) #w的2:7列为自变量，第1列为因变量laa #显示LAR回归过程##Call:##lars(x = w[, 2:7], y = w[, 1])##R-squared: 0.993 ##Sequence of LASSO moves:## GNP Year Armed.Forces Unemployed Employed Population Year Employed Employed Year Employed Employed##Var 1 5 3 2 6 4 -5 -6 6 5 -6 6 ##Step 1 2 3 4 5 6 7 8 9 10 11 12 plot(laa) #画lasso回归过程图summary(laa)#以上结果显示了每一步的残差平方和RSS和多重共线性指标Cp（Mallows&apos;s Cp http://en.wikipedia.org/wiki/Mallows%27_Cp）#Cp越小，多重共线性越小，因此结果以第八步为准，即只剩下第1、2、3、4个变量 13.glmnet包From https://site.douban.com/182577/widget/notes/10567212/note/289294468/ glmnet包是关于Lasso and elastic-net regularized generalized linear models。 作者是Friedman, J., Hastie, T. and Tibshirani, R这三位。 这个包采用的算法是循环坐标下降法（cyclical coordinate descent），处理的模型包括 linear regression,logistic and multinomial regression models, poisson regression 和 the Cox model，用到的正则化方法就是l1范数（lasso）、l2范数（岭回归）和它们的混合 （elastic net）。 坐标下降法是关于lasso的一种快速计算方法（是目前关于lasso最快的计算方法），其基本要点为： 对每一个参数在保持其它参数固定的情况下进行优化，循环，直到系数稳定为止。这个计算是在lambda的格点值上进行的。 关于这个算法见[5]。 关于glmnet包的细节可参考[4]，这篇文献同时也是关于lasso的一个不错的文献导读。 [1]Tibshirani, R.: Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society: Series B, Vol. 58 (1996), No 1, 267–288 [2]Efron, B., Johnstone, I., Hastie, T., and Tibshirani, R.: Least angle regression. Annals of Statistics, Vol. 32 (2004), No 2, 407–499. [3]Hastie, T., Tibshirani, R., and Friedman, J.: The Elements of Statistical Learning: Data Mining, Inference and Prediction. Second edition. New York: Springer, 2009. [4]Friedman,J.,Hastie,T.,Tibshirani.R.:Regularization Paths for Generalized Linear Models via Coordinate Descent.Journal of Statistical Software,Volume 33(2010), Issue 1. [5]J. Friedman, T. Hastie, H. Hoe ing, and R. Tibshirani.:Pathwise coordinate optimization. Annals of Applied Statistics, 2(1):302-332, 2007. http://www.stanford.edu/~hastie/Papers/pathwise.pdf [6]Trevor Hastie,Sparse Linear Models:with demonstrations using glmnet.2013. [7] Zou, Hui &amp; Trevor Hastie (2005): Regularization and variable selection via the Elastic Net, JRSS (B)67(2):301-320) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273library(glmnet)prostate=read.csv(url("https://taoshengxu.github.io/DocumentGit/data/prostate.csv"))prostate=prostate[,c(1,3,4,6,7,9)]head(prostate)x &lt;- as.matrix(prostate[, 2:6])y &lt;- prostate[, 1]set.seed(1)train &lt;- sample(1:nrow(x), nrow(x) * 2/3)test &lt;- (-train)## 1. Ridge Regressionr1 &lt;- glmnet(x = x[train, ], y = y[train], family = "gaussian", alpha = 0)plot(r1, xvar = "lambda")r1.cv &lt;- cv.glmnet(x = x, y = y, family = "gaussian", alpha = 0, nfold = 10)plot(r1.cv)mte &lt;- predict(r1, x[test, ])mte &lt;- apply((mte - y[test])^2, 2, mean)points(log(r1$lambda), mte, col = "blue", pch = 19)legend("topleft", legend = c("10 - fold CV", "Test"), col = c("red", "blue"))r1.min &lt;- glmnet(x = x, y = y, family = "gaussian", alpha = 0, lambda = r1.cv$lambda.min)coef(r1.min)##2. Lassor2 &lt;- glmnet(x = x[train, ], y = y[train], family = "gaussian", alpha = 1)plot(r2)plot(r2, xvar = "lambda")r2.cv &lt;- cv.glmnet(x = x, y = y, family = "gaussian", alpha = 1, nfold = 10)plot(r2.cv)mte &lt;- predict(r2, x[test, ])mte &lt;- apply((mte - y[test])^2, 2, mean)points(log(r2$lambda), mte, col = "blue", pch = 19)legend("topleft", legend = c("10 - fold CV", "Test"), col = c("red", "blue"))# cv.min vs cv.1se,用全部数据再次拟合模型r2.cv$lambda.min## [1] 0.002954r2.cv$lambda.1se## [1] 0.1771r2.1se &lt;- glmnet(x = x, y = y, family = "gaussian", alpha = 1, lambda = r2.cv$lambda.1se)coef(r2.1se)## 6 x 1 sparse Matrix of class "dgCMatrix"## s0## (Intercept) 0.3234## age . ## lbph . ## lcp 0.2462## gleason . ## lpsa 0.4320r2.min &lt;- glmnet(x = x, y = y, family = "gaussian", alpha = 1, lambda = r2.cv$lambda.min)coef(r2.min)## 6 x 1 sparse Matrix of class "dgCMatrix"## s0## (Intercept) -1.44505## age 0.01851## lbph -0.08585## lcp 0.29688## gleason 0.05081## lpsa 0.53741# 岭回归和lasso的比较lasso.pred &lt;- predict(r2, s = r2.cv$lambda.1se, newx = x[test, ])ridge.pred &lt;- predict(r1, s = r1.cv$lambda.1se, newx = x[test, ])mean((lasso.pred - y[test])^2)## [1] 0.3946mean((ridge.pred - y[test])^2)## [1] 0.4239 关于glmnet包的使用 (1)glment（）和cv.glmnet() 第一次用这个包的时候，我有个很蠢的问题，为什么有了cv.glmnet()还需要保留glmnet（）呢？ cv.glmnet()可以通过交叉验证得到（关于lambda的）最优的方程，但是就glment包来说仍然不是一个完美的结果，关于alpha的交叉验证依然需要使用者自己来完成（包的文档中给了点提示）。glmnet（）仍然需要保留，因为可以得到正则化的路径，因为算法的原因，coordinate descent 在选取极值上有随机性，路径在变量的选择中还是很重要的。 (2)cv.glmnet() 中的lambda.min和lambda.1se lambda.min: value of lambda that gives minimum cvm. lambda.1se: largest value of lambda such that error is within 1 standard error of the minimum. 关于这两个输出值的使用，似乎有点混乱。看了很多网上的讨论推荐使用lambda.1se的比较多，这样可以得到更简洁的模型。 涉及到所谓的1-SE rule。 “one standard error” rule to select the best model, i.e. selecting the most parsimonious model from the subset of models whose score is within one standard error of the best score.但是还有这样的说法：1se rule在低noise的时候才好用高noise的时候，有一两个fold的error很大，cv curve就会增长很快，导致选的lambda太大。 14.glmnet Vignettes 非常易读，有益理解https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf https://cran.r-project.org/web/packages/glmnet/vignettes/Coxnet.pdf]]></content>
  </entry>
  <entry>
    <title><![CDATA[Cox 分层原理]]></title>
    <url>%2Fhexo%2F2017%2F09%2F13%2F2017-09-13-CoxModle_with_stratified%2F</url>
    <content type="text"><![CDATA[PPT 首先理解为什么COX模型会对协变量分层处理？需要分层的变量不满足PH假设，需要分层处理。 如何确定 协变量不满足PH假设？首先对需要研究的协变量进行多协变量COX回归，挑出不满足PH假设的协变量 Cross-validated partial likelihood (CVPL) for the Cox modelcvpl {in Package survcomp} function …如果得到一个Subtypes 信息，对subtype分层进行 协变量为 age的COX 回归。目的是研究在不同亚型内，age 是否为影响生存预后的重要因素。]]></content>
  </entry>
  <entry>
    <title><![CDATA[混淆矩阵(Confusion matrix)-ROC曲线-AUC(Area under Curve)]]></title>
    <url>%2Fhexo%2F2017%2F09%2F10%2F2017-09-10-ConfusionMatrix-ROC-AUC%2F</url>
    <content type="text"><![CDATA[混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习， _在无监督学习一般叫做匹配矩阵_ 。在图像精度评价中，主要用于比较分类结果和实际测得值，可以把分类结果的精度显示在一个混淆矩阵里面。混淆矩阵是通过将每个实测像元的位置和分类与分类图像中的相应位置和分类像比较计算的。 混淆矩阵的每一列代表了预测类别[1] ，每一列的总数表示预测为该类别的数据的数目；每一行代表了数据的真实归属类别[1] ，每一行的数据总数表示该类别的数据实例的数目。每一列中的数值表示真实数据被预测为该类的数目：如下图，第一行第一列中的43表示有43个实际归属第一类的实例被预测为第一类，同理，第二行第一列的2表示有2个实际归属为第二类的实例被错误预测为第一类。 12345678如有150个样本数据，这些数据分成3类，每类50个。分类结束后得到的混淆矩阵为： 预测 类1 类2 类3 类1 43 5 2实际 类2 2 45 3 类3 0 1 49每一行之和为50，表示50个样本，第一行说明类1的50个样本有43个分类正确，5个错分为类2，2个错分为类3 另外一个例子 From:http://blog.csdn.net/vesper305/article/details/44927047 假设有一个用来对猫（cats）、狗（dogs）、兔子（rabbits）进行分类的系统，混淆矩阵就是为了进一步分析性能而对该算法测试结果做出的总结。假设总共有 27 只动物：8只猫， 6条狗， 13只兔子。结果的混淆矩阵如下图： 在这个混淆矩阵中，实际有 8只猫，但是系统将其中3只预测成了狗；对于 6条狗，其中有 1条被预测成了兔子，2条被预测成了猫。从混淆矩阵中我们可以看出系统对于区分猫和狗存在一些问题，但是区分兔子和其他动物的效果还是不错的。所有正确的预测结果都在对角线上，所以从混淆矩阵中可以很方便直观的看出哪里有错误，因为他们呈现在对角线外面。 在预测分析中，混淆表格（有时候也称为混淆矩阵），是由false positives，false negatives，true positives和true negatives组成的两行两列的表格。它允许我们做出更多的分析，而不仅仅是局限在正确率。准确率对于分类器的性能分析来说，并不是一个很好地衡量指标，因为如果数据集不平衡（每一类的数据样本数量相差太大），很可能会出现误导性的结果。例如，如果在一个数据集中有95只猫，但是只有5条狗，那么某些分类器很可能偏向于将所有的样本预测成猫。整体准确率为95%，但是实际上该分类器对猫的识别率是100%，而对狗的识别率是0%。 下面内容总结了假设检验的重要内容，清晰全面。 ROC 曲线为什么使用Roc和Auc评价分类器 ROC曲线和AUC常被用来评价一个二值分类器（binary classifier）的优劣。既然已经这么多标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。 ROC曲线：接收者操作特征(receiveroperating characteristic),roc曲线上每个点反映着对同一信号刺激的感受性。 横轴：负正类率(false postive rate FPR)，划分实例中所有负例占所有负例的比例；(1-Specificity) 纵轴：真正类率(true postive rate TPR)，Sensitivity(灵敏度) 对于一个二分类问题，假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR),在平面中得到对应坐标点。随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着真正的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点为(0,0),阈值最小时，对应坐标点(1,1)。通过调节不同的阀值，从而得到一条曲线。 横轴FPR:1-TNR,1-Specificity，FPR越大，预测正类中实际负类越多。 纵轴TPR：Sensitivity(正类覆盖率),TPR越大，预测正类中实际正类越多。 理想目标：TPR=1，FPR=0,即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。 AUC(Area under Curve)Roc曲线下的面积，介于0.5和1之间。Auc作为数值可以直观的评价分类器的好坏，值越大越好。 R12345678910111213141516library("pROC")data(aSAH) # Build a ROC object and compute the AUC, draw ROC, print AUC and the best THRESHOLDS roc(aSAH$outcome, aSAH$s100b, plot=TRUE, print.thres=TRUE, print.auc=TRUE) roc1 &lt;- plot.roc(aSAH$outcome, aSAH$s100, main="Statistical comparison", percent=TRUE, col="1")roc2 &lt;- lines.roc(aSAH$outcome, aSAH$ndka, percent=TRUE, col="2")testobj&lt;- roc.test(roc1,roc2)text(50, 50, labels=paste("p-value =", format.pval(testobj$p.value)), adj=c(0, .5))legend("bottomright", legend=c("S100B", "NDKA"), col=c("1", "2"), lwd=2)r1=roc(vs~wt,mtcars)plot.roc(r1)r2=roc(vs~mpg,mtcars)lines.roc(r2,col='2')roc.test(r1,r2) 在学习genefu包时候遇到Confusion Matrix，有必要有个系统的学习和总结。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Fold Change 与火山图]]></title>
    <url>%2Fhexo%2F2017%2F09%2F09%2F2017-09-09-FoldChange%2F</url>
    <content type="text"><![CDATA[Fold Change计算公式：样本平均值log2还原的比值 火山图（Volcano Plot）火山图只存在于两分组样本比较中，并且有生物学重复(经过相同方式处理的相同样品) 标准的火山图常用于展示显著差异表达的基因，这里有两个关键词：显著是指P&lt;&gt;差异表达一般我们按照Fold Change(倍数变化)&gt;=2.0作为标准。 当我们拿到基因表达的P值和倍数后，为了用火山图展示结果，一般需要把倍数进行Log2的转化，比如某基因在实验组表达水平是对照组的4倍，log2（4）=2，同样的如果是1/4，也就是0.25，转换后的结果就是-2。 同样的道理，对P值进行-log10的转化，-log10（0.05）约等于1.30103，由于P值越小表示越显著，所以我们进行-log10（P value）转化后，转化值越大表示差异约显著，比如-log10（0.001）=3 &gt; -log10(0.01)=2 &gt; -log10(0.05)=1.30。 在上面这个图中，横轴是log2（FC），纵轴是-log10（P value），每个点代表一个基因，平行于Y轴的两条线分别是X=1和X=-1，在X=-1左侧的点是下调2倍以上的基因，在X=1右侧的点是上调2倍以上的基因。同时，平行于X轴有一条虚线Y=1.30，即-log10(0.05），在虚线以上的点表示显著性. 这样，我们就把虚线Y=1.30以上，X=1右侧和X=-1左侧的基因标记为表达显著差异的基因，一般我们把大于2倍（X=1右侧）的点标记为红色，把小于-2（X=-1左侧）的点标记为绿色，一些我们特别关注的基因需要把基因名标记出来。]]></content>
  </entry>
  <entry>
    <title><![CDATA[GCBI 学院]]></title>
    <url>%2Fhexo%2F2017%2F09%2F09%2F2017-09-09-GCBI%2F</url>
    <content type="text"><![CDATA[GCBI学院里一些视频还是值得看看的,一直想看看一直忘记，记录在这里。 Here]]></content>
  </entry>
  <entry>
    <title><![CDATA[ExpressionSet_SummarizedExperiment_GEOquery_biomaRt_S4]]></title>
    <url>%2Fhexo%2F2017%2F09%2F08%2F2017-09-08-ExpressionSet_SummarizedExperiment%2F</url>
    <content type="text"><![CDATA[ExpressionSet下面代码列举了ExpressionSet对象大部分常用函数的用法，ExpressionSet对象可以直接取pData()样本分组信息的列名的数据如 ALL$sex exprs() 表达谱数据 pData() 对象分组信息 sampleNames() featureNames() phenoData() featureData() experimentData() 12345678910111213141516## 非常全面的示例代码library(ALL)data(ALL)experimentData(ALL)exprs(ALL)[1:4,1:4]head(sampleNames(ALL))head(featureNames(ALL))head(pData(ALL))ALL$sexALL[1:10,1:5]featureData(ALL)#包含关于基因的信息，但是经常没有ids &lt;- featureNames(ALL)[1:5]library(hgu95av2.db)#之前数据集里面写了芯片平台hgu95av2.dbas.list(hgu95av2ENTREZID[ids])phenoData(ALL)#不如使用pData(ALL),phenoData(ALL)内容更多些names(pData(ALL))#有时相当于varLabels(ALL),有时varLabels(ALL)更详细 SummarizedExperiment123456789101112131415library(airway)data("airway")airwaycolData(airway)#pData(ALL)airway$cellcolnames(airway)head(rownames(airway))assayNames(airway)#要想获得表达矩阵,使用assay accessor,用assayNames()获得全部表达矩阵名assay(airway,"counts")[1:4,1:4]length(rowRanges(airway))rowRanges(airway)#SummarizedExperiment特别之处在于每行每列都有关联的GRangesstart(airway)gr = GRanges("1",ranges = IRanges(start = 1,end = 10^7))subsetByOverlaps(airway,gr)#'正是有着关联GRanges,可以取出染色体"某一区域内"基因的表达值 GEOquery12345678library(GEOquery)eList &lt;- getGEO("GSE11675")length(eList)eData = eList[[1]]eDatanames(pData(eData))eList2 = getGEOSuppFiles("GSE11675")#下载原始tar包eList2 biomaRt123456789101112131415161718library(biomaRt)head(listMarts())mart &lt;- useMart("ensembl")marthead(listDatasets(mart))ensemble &lt;- useDataset("hsapiens_gene_ensembl",mart)values &lt;- c("202763_at","209310_s_at","207500_at")getBM(attributes = c("ensembl_gene_id","affy_hg_u133_plus_2"), filters = "affy_hg_u133_plus_2",values = values,mart = ensemble)attributes &lt;- listAttributes(ensemble)#可以查询到的条目nrow(attributes)#可以把一个物种的基因转换到另一个物种的同源基因head(attributes)filters &lt;- listFilters(ensemble)#可以查询到的条目nrow(filters)#可以把一个物种的基因转换到另一个物种的同源基因head(filters)attributePages(ensemble)#attributes存储在一个一个page中,可以用这个减小搜索范围attributes &lt;- listAttributes(ensemble,page = "feature_page")nrow(attributes) R S4 Classes12345678910111213141516171819library(ALL)library(GenomicRanges)#'S3对象就是像一个list,list中每个对象都有各自的name#'而S4对象定义了每个class应该是有些什么东西data("ALL")ALLclass(ALL)isS4(ALL)class?ExpressionSet#查看一个class的简介?"ExpressionSet-class"#查看一个class的简介#'list的规则:首字母大写#'构造方法ExpressionSet()getClass("ExpressionSet")#Slots插槽,就是这个class由哪儿些小class构成ALL@annotationannotation(ALL)#class升级了,定义改变了,用updateObjectOLD_OBJECT = updateObject(OLD_OBJECT)validObject(ALL)#检测对象是否正确,是否符合class的定义 R S4 Methods1234567891011121314151617library(GenomicRanges)GenomicRanges::as.data.frame#S4方法base::as.data.frame#S3方法showMethods("as.data.frame")#可以看见,X类型不同,后续选用的程序代码也不同#查看传入某一特定类型，对应的相关程序代码getMethod("as.data.frame","GenomicRanges")getMethod("as.data.frame",signature(x="GenomicRanges"))#查看传入某一特定类型，对应的帮助文档method?"as.data.frame,DataFrame"method?"as.data.frame,GenomicRanges"?"as.data.frame,DataFrame-method"?"as.data.frame,GenomicRanges-method"showMethods("findOverlaps")getMethod("findOverlaps",signature(query = "Ranges",subject = "Ranges"))?"findOverlaps,Ranges,Ranges-method"#'S4缺点:难以找到help文档,难以直接看源代码,难以debug#'但是最好S4写一个package,方便管理 作者：wangyunpeng_bio来源：CSDN原文：https://blog.csdn.net/qq_29300341/article/details/77009100版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
  </entry>
  <entry>
    <title><![CDATA[ExpressionSet简单讲解]]></title>
    <url>%2Fhexo%2F2017%2F09%2F08%2F2017-09-08-ExpressionSet%2F</url>
    <content type="text"><![CDATA[From 生信菜鸟团 http://www.bio-info-trainee.com/1510.html ExpressionSet对象其实是对表达矩阵加上样本分组信息的一个封装，由Biobase这个包引入。它是eSet这个对象的继承。属性基本和eSet保持一致。 重点就是 exprs 函数提取表达矩阵，pData 函数看看该对象的样本分组信息。在Biobase基础包中，ExpressionSet是非常重要的类，因为Bioconductor设计之初是为了对基因芯片数据进行分析，而ExpressionSet正是Bioconductor为基因表达数据格式所定制的标准。它是所有涉及基因表达量相关数据在Bioconductor中进行操作的基础数据类型，比如affyPLM, affy, oligo, limma, arrayMagic等等。所以当我们学习Bioconductor时，第一个任务就是了解并掌握ExpressionSet的一切。 ExpressionSet的组成： assayData: 一个matrix类型或者environment类型数据。用于保存表达数据值。当它是一个matrix时，它的行表示不同的探针组（probe sets）（也是features，总之是一个无重复的索引值）的值，它的列表示不同的样品。如果有行号或者列号的话，那么行号必须与featureData及phenoData中的行号一致，列号就是样品名。当我们使用exprs()方法时，就是调取的这个assayData的matrix。当它是一个enviroment时，它必须有两个变量，一个就是与上一段描述一致的matrix，另一个就是epxrs，而这个exprs会响应exprs()方法，返回表达值。 头文件：用于描述实验平台相关的数据，其中包括phenoData, featureData，protocolData以及annotation等等。其中phenoData是一个存放样品信息的data.frame或者AnnotatedDataFrame类型的数据。如果有行号的话，其行号必须与assayData的列号一致（也就是样品名）。如果没有行号，则其行数必须与assayData的列数一致。featureData是一个存放features的data.frame或者AnnotatedDataFrame类型的数据。它的行数必须与assayData的行数一致。如果有行号的话，那么它的行号必须和assayData的行号一致。annotation是用于存放芯片类型的字符串，比如hgu95av2之类。protocolData用于存放设备相当的数据。它是AnnotatedDataFrame类型。它的维度必须与assayData的维度一致。 experimentData: 一个MIAME类型的数据，它用于保存和实验设计相关的资料，比如实验室名，发表的文章，等等。那么什么是MIAME类呢？MIAME是Minimum Information About a Microarray Experiment的首字母缩写，它包括以下一些属性（slots）： name: 字符串，实验名称 lab: 字符串，实验室名称 contact: 字符串，联系方式 title: 字符串，一句话描述实验的内容 abstract: 字符串，实验摘要 url: 字符串，实验相关的网址 samples: list类，样品的信息 hybridizations: list类，杂交的信息 normControls: list类，对照信息，比如一些持家基因（house keeping genes） preprocessing: list类，原始数据的预处理过程 pubMedIds: 字符串，pubMed索引号 others: list类，其它相关的信息 有了这些，所有实验相关的信息基本全备。 那么，对于一个ExpressionSet，哪些属性是必须的？哪些有可能缺失呢？很显然，assayData是必须的，其它的可能会缺失，但是不能都缺失，因为那样的话就无法完成数据分析的工作。 一个现成例子 下面是一个具体的例子，来源于CLL这个包，是用hgu95av2芯片测了22个样本 12345678910111213141516171819202122232425library(CLL)data(sCLLex)sCLLexExpressionSet (storageMode: lockedEnvironment)assayData: 12625 features, 22 samples ##表达矩阵 element names: exprs protocolData: nonephenoData sampleNames: CLL11.CEL CLL12.CEL ... CLL9.CEL (22 total) varLabels: SampleID Disease ## 样本分组信息 varMetadata: labelDescriptionfeatureData: noneexperimentData: use 'experimentData(object)'Annotation: hgu95av2&gt; exprMatrix=exprs(sCLLex)&gt; dim(exprMatrix)[1] 12625 22&gt; meta=pData(sCLLex)&gt; table(meta$Disease)progres. stable 14 8 &gt; 根据上面的信息可以看出该芯片共12625个探针，这22个样本根据疾病状态分成两组，14vs8 这个数据对象就可以打包做很多包的分析输入数据。 limma等包使用该对象作为输入数据 下面这个例子充分说明了 ExpressionSet 对象的重要性 123456789101112131415161718&gt; library(limma)&gt; design=model.matrix(~factor(sCLLex$Disease))&gt; fit=lmFit(sCLLex,design)&gt; fit=eBayes(fit)&gt; options(digits = 4)&gt; topTable(fit,coef=2,adjust='BH') logFC AveExpr t P.Value adj.P.Val B39400_at 1.0285 5.621 5.836 8.341e-06 0.03344 3.23436131_at -0.9888 9.954 -5.772 9.668e-06 0.03344 3.11733791_at -1.8302 6.951 -5.736 1.049e-05 0.03344 3.0521303_at 1.3836 4.463 5.732 1.060e-05 0.03344 3.04436122_at -0.7801 7.260 -5.141 4.206e-05 0.10619 1.93536939_at -2.5472 6.915 -5.038 5.362e-05 0.11283 1.73741398_at 0.5187 7.602 4.879 7.824e-05 0.11520 1.42832599_at 0.8544 5.746 4.859 8.207e-05 0.11520 1.38936129_at 0.9161 8.209 4.859 8.212e-05 0.11520 1.38937636_at -1.6868 5.697 -4.804 9.355e-05 0.11811 1.282&gt; 还有非常多的其它包会使用 ExpressionSet 对象，我就不一一介绍了。 自己构造 ExpressionSet 对象 根据上面的讲解，我们知道了在这个对象其实很简单，就是对表达矩阵加上样本分组信息的一个封装。所以我们就用上面得到的exprMatrix和meta来构建一个ExpressionSet对象，biobase包里面提供了详细的说明,建议大家仔细看官方手册 12345678910111213141516171819metadata &lt;- data.frame(labelDescription=c('SampleID', 'Disease'), row.names=c('SampleID', 'Disease'))phenoData &lt;- new("AnnotatedDataFrame",data=meta,varMetadata=metadata)myExpressionSet &lt;- ExpressionSet(assayData=exprMatrix, phenoData=phenoData, annotation="hgu95av2")&gt; myExpressionSetExpressionSet (storageMode: lockedEnvironment)assayData: 12625 features, 22 samples element names: exprs protocolData: nonephenoData sampleNames: CLL11.CEL CLL12.CEL ... CLL9.CEL (22 total) varLabels: SampleID Disease varMetadata: labelDescriptionfeatureData: noneexperimentData: use 'experimentData(object)'Annotation: hgu95av2 &gt; 从上面的构造过程可以看出，重点就是表达矩阵加上样本分组信息 其它例子ALL包的数据自带 ExpressionSet 对象12345678910111213141516library(ALL)data(ALL)ALLExpressionSet (storageMode: lockedEnvironment)assayData: 12625 features, 128 samples element names: exprsprotocolData: nonephenoData sampleNames: 01005 01010 … LAL4 (128 total) varLabels: cod diagnosis … date last seen (21 total) varMetadata: labelDescriptionfeatureData: noneexperimentData: use ‘experimentData(object)’pubMedIds: 14684422 16243790 Annotation: hgu95av2 这个数据非常出名，很多其它算法包都会拿这个数据来举例子，只有真正理解了ExpressionSet对象才能学会bioconductor系列包 用GEOquery包来下载得到 ExpressionSet 对象12gse1009=GEOquery::getGEO("GSE1009")gse1009[[1]] ## 这就是ExpressionSet对象]]></content>
  </entry>
  <entry>
    <title><![CDATA[生存曲线美化]]></title>
    <url>%2Fhexo%2F2017%2F09%2F07%2F2017-09-07-BeautySurvCurves%2F</url>
    <content type="text"><![CDATA[好久以前在微信里看到这个文章。今天整理用rmarkdonw整理在这里，以后更新的我的包里。 如何优雅地绘制生存曲线 12345678910111213141516library(survival)library(ggplot2)library(survminer)# 载入数据#使用Surv（）函数建立基本生存对象fit&lt;- survfit(Surv(time, status) ~ sex, data = lung)summary(fit) #查看结果#使用survminer程序包ggsurvplot（）函数绘制生存曲线#简单绘图ggsurvplot(fit)#分生存曲线下面给出number.at riskggsurvplot(fit,risk.table=TRUE)#添加log-rank检验p-valueggsurvplot(fit,risk.table=TRUE,pval=TRUE)#添加置信区间带ggsurvplot(fit,risk.table=TRUE,conf.int=TRUE,pval=TRUE)]]></content>
  </entry>
  <entry>
    <title><![CDATA[基因本体论(Gene Ontology) 与通路分析(Pathway Analysis)]]></title>
    <url>%2Fhexo%2F2017%2F09%2F07%2F2017-09-07-Gene%20Ontology%2F</url>
    <content type="text"><![CDATA[基因本体论针对于单个基因特征 本体论这个词一看就逼格很高的样子，源于哲学，本体论用于描述事物的本质，所以基因本体论就是为了描述基因的本质。GO从三个方面对基因的本质进行描述， 1）细胞组分（Cellular Component, CC）：一般用来描述基因作用的位置，比如说高尔基体，内质网这样的； 2）分子功能（Molecular Function, MF）：可以描述为分子水平的活性，如催化或结合活性； 3）生物学过程(BP)：比如说蛋白质磷酸化，细胞粘附都是生物学过程。 简单地说，GO就像是给基因贴标签进行注释，比如说给X湿兄贴标签，出没地点——小张聊科研(CC)，文风诙谐幽默(MF)，能够让大家轻松愉悦地学到东西(BP)。 GO的术语是分层的，呈现出树状结构，上文提到的CC、MF和BP即为GO术语的最顶层，比如说下图是BP的分析结果树状图，最顶端即为BP. 通路分析针对于群基因（protein）特征 一个生物学过程的实现会涉及到许多蛋白质，这些蛋白质合在一起就是一个通路。通路分析能够帮助我们更好地了解某个或某一些蛋白质在一个生物学过程中所扮演的角色。通路分析和GO都是对基因进行注释，那么为什么要对基因进行注释呢？因为基因说穿了其实是一串RNA，那么它的功能和结构虽然都是客观存在的，但是要如何描述这些客观的东西是基因注释所要解决的问题。最常用的通路分析数据库是京都基因与基因组百科全书 (Kyoto Encyclopedia of Genes and Genomes, KEGG)。1995年，KEGG数据库项目由京都大学化学研究所教授Minoru Kanehisa领头启动。KEGG数据库是手工绘制的KEGG途径图的集合，每个途径图包含分子相互作用和反应的网络，将基因组中的基因与通路中的基因产物（主要是蛋白质）连接。KEGG pathway analysis即为将目的基因定位到KEGG途径图中的过程。下图为small cell lung cancer的KEGG途径图。 GO分析 与 Pathyway 分析 总结 GO数据库分别从功能、参与的生物途径及细胞中的定位对基因产物进行了标准化描述，即对基因产物进行简单注释，通过GO富集分析可以粗略了解差异基因富集在哪些生物学功能、途径或者细胞定位。GO分析好比是将基因分门别类放入一个个功能类群的篮子，而pathway则是将基因一个个具体放到代谢网络中的指定位置。 Pathway指代谢通路，对差异基因进行pathway分析，可以了解实验条件下显著改变的代谢通路，在机制研究中显得尤为重要。 基因功能分析(Gene Ontology)和代谢通路（pathway）分析方法（核心） 进行样本组和对照组基因表达差异分析 对获取的差异表达基因进行功能（GO）和信号通路（Pathway）分析 在得到功能（GO）和信号通路（Pathway）分析的结果中找出和疾病/研究目标相关的GO 和 Pathway 对这些相关的基因功能和信号通路的基因取交集，缩小候选基因的范围。 对取交集得到的基因，如果基因数目还比较多（目标基因：1-2个），就将这些基因和差异表达基因再取交集，根据Fold Change 选择差异表达倍数最大的基因作为我们研究的候选基因。 date: ‘2016-12-15 17:07:42’ 干Bioinformatics差不多2年半了，却一直到对GO和pathway的区别搞不清楚，现在明白又觉好笑，记几个字在这里。 一般对一组Gene Set 做GO是想看哪些生物功能；比如一个功能，有很多基因都和这个功能相关，把这个功能相关的所有基因找出来。 而对一组GeneSet 做Kegg主要是看通路，在通路上的联系。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Git 参考手册]]></title>
    <url>%2Fhexo%2F2017%2F09%2F07%2F2017-09-07-GitCommand%2F</url>
    <content type="text"><![CDATA[Git 参考手册中文1 Git 参考手册中文2 Git 参考手册ENG git branch 命令1234567891011121314git branch 列出所有分支git branch -v 查看每一个分支的最后一次提交git branch -d hotfix 表示删除hotfix分支 【git 删除本地分支】git branch -D br【git 删除远程分支】git push origin :br (origin 后面有空格) 或者$ git push origin --delete serverfix--merged 与 --no-merged 这两个有用的选项可以过滤这个列表中已经合并或尚未合并到当前分支的分支如果要查看哪些分支已经合并到当前分支，可以运行 git branch --merged：git branch --merged在这个列表中分支名字前没有 * 号的分支通常可以使用 git branch -d 删除掉 git checkout 命令12345git checkout -b 创建新分支，并立即切换到它。与以下等效：git branch newbranch git checkout newbranch git merge命令1234567git merge [branch] 将[branch] 分枝合并到当前分支中例如,首先切换到master分支，然后将hotfix分支合并到master分支git checkout mastergit merge hotfixgit merge origin/serverfix 将远程git服务器origin的serverfix合并到当前分支 git add 命令git add xx命令可以将xx文件添加到暂存区， 1234git add . 提交被修改的和新建的文件，添加全部修改或新增的文件但不包括被删除的文件 git add -u --update update tracked files 更新所有改变的文件，即提交所有变化的文件,表示添加编辑或者删除的文件，不包括新添加的文件git add -A --all add changes from all tracked and untracked files 提交已被修改和已被删除文件，但是不包括新的文件git add -A . 如果有很多改动，可以一次添加所有改变的文件（注意 `-A` 选项后面还有一个句点）。 git commit命令1git commit -a -m "commit discription" "-a" 表示git add，"-m"表示commit的注释 git push 命令1git push [alias] [branch]，就会将你的[branch]分支推送成为[alias]远端上的同名[branch] 分支 git fetch 命令12从服务器上抓取本地没有的数据时，它并不会修改工作目录中的内容。它只会获取数据然后让你自己合并。 git pull 命令1在大多数情况下它的含义是一个 git fetch 紧接着一个git merge 命令 删除commit1234567891011121314git reset --hard HEAD~3 删除最顶部的3个commitgit reset --hard commit-id 回滚到commit-id,将commit-id之后提交的commit都去除【远程代码库回滚】：应用场景：自动部署系统发布后发现问题，需要回滚到某一个commit，再重新发布原理：先将本地分支退回到某个commit，删除远程分支，再重新push本地分支操作步骤：1、git checkout the_branch2、git pull3、git branch the_branch_backup //备份一下这个分支当前的情况4、git reset --hard the_commit_id //把the_branch本地回滚到the_commit_id5、git push origin :the_branch //删除远程 the_branch6、git push origin the_branch //用回滚后的本地分支重新建立远程分支7、git push origin :the_branch_backup //如果前面都成功了，删除这个备份分支 Git 学习1.初始化Git初始化后，在当前目录下会出现一个名为 .git 的目录，所有 Git 需要的数据和资源都存放在这个目录中12cd D:/Testgit init 如果当前目录下有几个文件想要纳入版本控制，需要先用 git add 命令告诉 Git 开始对这些文件进行跟踪，然后提交：123git add *.cgit add READMEgit commit -m 'initial project version' 从现有仓库克隆1git clone git://github.com/schacon/grit.git +目录(可选，指定存的目录) 检查当前文件状态123git statusOn branch masternothing to commit, working directory clean 跟踪新文件git add 将 修改文件放到暂存区，然后再看看 git status 的输出12git add READMEgit status 提交更新1git commit -m "commit message" git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤： 移除文件123456rm grit.gemspec #问磁盘中删除文件git rm grit.gemspec#从git 跟踪中删除#如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f（译注：即 force 的首字母），以防误删除文件后丢失修改的内容#只想把文件从 Git 仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。换句话说，仅是从跟踪清单中删除。比如一些大型日志文件或者一堆 .a 编译文件，不小心纳入仓库后，要移除跟踪但不删除文件，以便稍后在 .gitignore 文件中补上，用 --cached 选项即可git rm --cached readme.txt 移动文件123456git mv file_from file_to#其实，运行 git mv 就相当于运行了下面三条命令：$ mv README.txt README$ git rm README.txt$ git add README 忽略某些文件创建一个名为 .gitignore 的文件，列出要忽略的文件模式。123$ cat .gitignore*.[oa]*~ 第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件。第二行告诉 Git 忽略所有以波浪符（~）结尾的文件。 文件 .gitignore 的格式规范如下： 所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式最后跟反斜杠（/）说明要忽略的是目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。星号（*）匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 我们再看一个 .gitignore 文件的例子12345678910111213# 此为注释 – 将被 Git 忽略# 忽略所有 .a 结尾的文件*.a# 但 lib.a 除外!lib.a# 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO/TODO# 忽略 build/ 目录下的所有文件build/# 会忽略 doc/notes.txt 但不包括 doc/server/arch.txtdoc/*.txt# 忽略 doc/ 目录下所有扩展名为 txt 的文件doc/**/*.txt 远程仓库可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL1234567891011121314151617git remote -v添加远程仓库git remote add &lt;shortname&gt; &lt;url&gt;添加一个新的远程 Git 仓库，同时指定一个你可以轻松引用的简从远程仓库中抓取与拉取git fetch [remote-name]git fetch 命令会将数据拉取到你的本地仓库,并不会自动合并或修改你当前的工作。当准备好时你必须手动将其合并入你的工作。git pull 命令来自动的抓取然后合并远程分支到当前分支。推送到远程仓库git push [remote-name] [branchname]当你想要将 master 分支推送到 origin 服务器时（再次说明，克隆时通常会自动帮你设置好那两个名字），那么运行这个命令就可以将你所做的备份到服务器：git push origin master]]></content>
  </entry>
  <entry>
    <title><![CDATA[层叠样式表 (Cascading Style Sheets)小结]]></title>
    <url>%2Fhexo%2F2017%2F09%2F06%2F2017-09-05-StudyCSS%2F</url>
    <content type="text"><![CDATA[CSS是一个我一直认为web前段技术，我知道我肯定能学会，但是我却不肯学，可是可是我总是对网页有一种莫名的向往，现在有了R,markdown,shiny,可以回避JSP,PHP等总不愿意学会的工具了，可是HTML,CSS,JS却无法回避了,这是开始shiny之后又不得不继续深入的一个topic. Shiny 的表现太土了. 一些概念：样式表定义如何显示 HTML 元素 CSS 在线教程 参考手册CSS 语法 CSS 规则由两个主要的部分构成：选择器，以及一条或多条声明:如： 12345p&#123; text-align:center; /*这是另一个注释*/ color:black; font-family:arial; &#125; id 选择器: id 选择器以 “#” 来定义 12345#para1&#123;text-align:center;color:red;&#125; class 选择器：类选择器以一个点”.”号显示 123.cen &#123;text-align:center;&#125; /*所以拥有cen类的HTML元素都居中*/ 另外，可以指定所有 p 元素使用 class=&quot;center&quot; 让该元素的文本居中 1234567891011121314151617 &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;style&gt;p.center&#123; text-align:center;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1 class=&quot;center&quot;&gt;这个标题不受影响&lt;/h1&gt;&lt;p class=&quot;center&quot;&gt;这个段落居中对齐。&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 样式表种类 外部样式表 内部样式表 内联样式 123456789101112 &lt;head&gt; &lt;!-- 外部样式 style.css --&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;style.css&quot;/&gt; &lt;!-- 设置：h3&#123;color:blue;&#125; --&gt; &lt;style type=&quot;text/css&quot;&gt; /* 内部样式 */ h3&#123;color:green;&#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;h3&gt;测试！&lt;/h3&gt;&lt;/body&gt; CSS 背景 1234567背景颜色body &#123;background-color:#b0c4de;&#125; #&quot;#ff0000&quot;,&quot;rgb(255,0,0)&quot;,&quot;red&quot;三种表示都可以h1 &#123;background-color:#6495ed;&#125;p &#123;background-color:#e0ffff;&#125;div &#123;background-color:#b0c4de;&#125;背景图像body &#123;background-image:url(&apos;paper.gif&apos;);&#125; CSS 文本格式 123456789101112131415161718##颜色body &#123;color:red;&#125;h1 &#123;color:#00ff00;&#125;h2 &#123;color:rgb(255,0,0);&#125;##对齐h1 &#123;text-align:center;&#125;p.date &#123;text-align:right;&#125;p.main &#123;text-align:justify;&#125;##文本修饰h1 &#123;text-decoration:overline;&#125;h2 &#123;text-decoration:line-through;&#125;h3 &#123;text-decoration:underline;&#125;## 文本转换p.uppercase &#123;text-transform:uppercase;&#125;p.lowercase &#123;text-transform:lowercase;&#125;p.capitalize &#123;text-transform:capitalize;&#125;##文本缩进p &#123;text-indent:50px;&#125; CSS 字体 12345678910111213141516171819##字体样式&lt;style&gt;p.normal &#123;font-style:normal;&#125;p.italic &#123;font-style:italic;&#125;p.oblique &#123;font-style:oblique;&#125;&lt;/style&gt;&lt;body&gt;&lt;p class=&quot;normal&quot;&gt;这是一个段落,正常。&lt;/p&gt;&lt;p class=&quot;italic&quot;&gt;这是一个段落,斜体。&lt;/p&gt;&lt;p class=&quot;oblique&quot;&gt;这是一个段落,斜体。&lt;/p&gt;&lt;/body&gt;##字体大小h1 &#123;font-size:40px;&#125;h2 &#123;font-size:30px;&#125;p &#123;font-size:14px;&#125;h1 &#123;font-size:2.5em;&#125; /* 40px/16=2.5em */h2 &#123;font-size:1.875em;&#125; /* 30px/16=1.875em */p &#123;font-size:0.875em;&#125; /* 14px/16=0.875em */ 学到这里也就了解CSS的一个基本语法结构了，不需要深入了]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 安装R, RStudio, Rstudio Server, JAVA]]></title>
    <url>%2Fhexo%2F2017%2F09%2F05%2F2017-09-04-UbuntuInstall_R%2F</url>
    <content type="text"><![CDATA[R安装最新的R 版本 https://mirrors.ustc.edu.cn/CRAN/ Ubuntu14.04 12345678sudo gedit /etc/apt/sources.list#sudo vim /etc/apt/sources.list# 手动加入最新新镜像源：# deb http://cran.rstudio.com/bin/linux/ubuntu trusty/ Ubuntu14.04sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 51716619E084DAB9# 然后更新一下sudo apt-get updatesudo apt-get install r-base r-base-dev Ubuntu16.04 12345sudo echo "deb http://cran.rstudio.com/bin/linux/ubuntu xenial/" | sudo tee -a /etc/apt/sources.list gpg --keyserver keyserver.ubuntu.com --recv-key 51716619E084DAB9gpg -a --export 51716619E084DAB9 | sudo apt-key add -sudo apt-get updatesudo apt-get install r-base r-base-dev Rstudiohttps://www.rstudio.com/products/rstudio/download/ 1234sudo apt-get install gdebi-coresudo apt-get install libapparmor1sudo gdebi -n rstudio-1.0.44-amd64.debrm rstudio-1.0.44-amd64.deb Rstudio serverhttps://www.rstudio.com/products/rstudio/download-server/ Ubuntu RStudio server 错误解决方法在/home目录下删除.rstudio文件夹 可解决 12345678sudo apt-get install gdebi-coresudo apt-get install libapparmor1wget http://download2.rstudio.org/rstudio-server-0.97.551-amd64.debsudo gdebi rstudio-server-0.97.551-amd64.deb完成安装后，RStudio Server会自动启动运行ps -aux|grep rstudio8787端口被打开访问地址：192.168.1.107:8787 1234567891011121314151617181920212223242526272829303132333435363738root用户无法登陆，新建一个用户进行登陆 useradd -d /home/R -m R，创建用户的同时指定主目录 passwd R，设置密码#系统设置 主要有两个配置文件，默认文件不存在 /etc/rstudio/rserver.conf /etc/rstudio/rsession.conf#设置端口和ip控制:vi /etc/rstudio/rserver.confwww-port=8080#监听端口www-address=127.0.0.0#允许访问的IP地址，默认0.0.0.0#重启服务器，生效rstudio-server restart会话配置管理vi /etc/rstudio/rsession.confsession-timeout-minutes=30#会话超时时间r-cran-repos=http://ftp.ctex.org/mirrors/CRAN#CRAN资源库rstudio-server start #启动rstudio-server stop #停止rstudio-server restart #重启#查看运行中R进程rstudio-server active-sessions#指定PID，停止运行中的R进程rstudio-server suspend-session &lt;pid&gt;#停止所有运行中的R进程rstudio-server suspend-all#强制停止运行中的R进程，优先级最高，立刻执行rstudio-server force-suspend-session &lt;pid&gt;rstudio-server force-suspend-all#RStudio Server临时下线，不允许web访问，并给用户友好提示rstudio-server offline# RStudio Server临时上线rstudio-server online JAVA12345678910111213141516171819202122232425##下载 解压wget -c http://download.oracle.com/otn-pub/java/jdk/8u11-b12/jdk-8u11-linux-i586.tar.gzmkdir -p /usr/lib/jvmsudo mv jdk-8u11-linux-i586.tar.gz /usr/lib/jvmcd /usr/lib/jvmsudo tar xzvf jdk-8u11-linux-i586.tar.gzsudo ln -s jdk1.8.0_11 java8##添加环境变量vi ~/.bashrcxport JAVA_HOME=/usr/lib/jvm/java8export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATHsource ~/.bashrc##配置（似乎不必要）##有的系统中会预装OpenJDK，系统默认使用的是这个，而不是刚才装的。##所以这一步是通知系统使用Oracle的JDK，非OpenJDK。sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/java8/bin/java 300sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java8/bin/javac 300sudo update-alternatives --config java## 测试验证java -version]]></content>
  </entry>
  <entry>
    <title><![CDATA[HTML基本语法]]></title>
    <url>%2Fhexo%2F2017%2F09%2F05%2F2017-09-06-HTML_Basic%2F</url>
    <content type="text"><![CDATA[wait for…]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.01 安装拼音输入法]]></title>
    <url>%2Fhexo%2F2017%2F09%2F05%2F2017-09-04-UbuntuInstall_pinying%2F</url>
    <content type="text"><![CDATA[请注意命令中不应该的空格可能导致命令不合法！ 一、检查 fctix 框架 ​ 首先，要安装中文输入法，必须要保证系统上有 fctix。fctix 是一个以 GPL 方式发布的输入法框架，安装 fctix 后可以为操作系统的桌面环境提供一个灵活的输入方案，解决在 GNU/Linux 环境下安装中文输入法的问题。​ win + a 打开所有应用程序，找到 Language Support 并打开，在 kygboard input method system 中查看是否有 fcitx 。 1234&gt; fcitx&gt; sudo apt-get install fcitx-bin&gt; sudo apt-get install fcitx-table&gt; sudo apt-get install -f #修复依赖关系 ​ 之后重启电脑，重新设置 kygboard input method system 为 fctix 。 二、安装输入法（二选一）注：搜狗输入法比 Google 输入法好用 (一)、搜狗输入法1、进入搜狗输入法的官网，找到linux环境下的 .deb 的安装包并下载（注意 32 位 和 64 位）。下载的时候有一个可选项，是下载安装或者是保存，建议大家保存，下载安装的那个选项安装完后好像没效果（可能是自身操作的问题）。搜狗输入法 Linux 版官方链接：https://pinyin.sogou.com/linux/?r=pinyin 。12sudo dpkg -i sogoupinyin_2.2.0.0108_amd64.debsudo apt-get install -f 4、重启系统。 (二)、Google 输入法 注：googlepinyin 有基于 fctix 框架的，也有基于 ibus 框架的。 1、直接安装。(如果安装失败提示 E: Unable to locate package fcitx-googlepinyin 时，先执行： sudo apt install fctix 再执行如下命令)12sudo apt install fcitx-googlepinyin sudo apt-get install -f 3、重启系统。 三、设置输入法：1、win + a 打开所有应用，搜索 fcitx configuration 并打开。2、取消勾选3、点击 “+” 新增输入法，在搜索框直接搜索即可4、OK 即可5、将你喜欢使用的输入法置顶，因为输入法在启动的时候是按顺序启动的。注：如果设置完后输入法还是无法启动，重启系统即可。 四、卸载1、卸载 googlepinyin①、sudo apt-get purge fcitx-googlepinyin②、百度的方法，没试过（基于ibus框架下的）。sudo apt-get remove ibussudo apt-get remove ibus-goog 2、卸载搜狗输入法（卸载之前将系统的输入法设置为 ibus ，否则系统重启后没有输入法可用）卸载 fctix： sudo apt-get purge fcitx卸载 fctix 及相关配置：sudo apt-get autoremove]]></content>
  </entry>
  <entry>
    <title><![CDATA[Shiny and Shiny server学习]]></title>
    <url>%2Fhexo%2F2017%2F09%2F05%2F2017-09-05-StudyShiny%2F</url>
    <content type="text"><![CDATA[Rblogdonw+Hugo用来实现静态网页，Shiny用于实现动态网页，几乎把R这个工具发挥到了极致，使其无所不能，R可以解决一切简单的需要了。 1. Shiny 中文教程. Among them, the Articles are very useful. 英文教程 Examples shiny-cheatsheet然而shiny太素颜了，需要一些扩展使得其表现美妙起来。 2. Shiny with HTML123456789101112131415161718192021222324252627282930313233343536373839404142names(shiny::tages) ## 100多种HTML标签tags$div(class = &quot;header&quot;, checked = NA, tags$p(&quot;Ready to take the Shiny tutorial? If so&quot;), tags$a(href = &quot;shiny.rstudio.com/tutorial&quot;, &quot;Click Here!&quot;))## &lt;div class=&quot;header&quot; checked&gt;## &lt;p&gt;Ready to take the Shiny tutorial? If so&lt;/p&gt;## &lt;a href=&quot;shiny.rstudio.com/tutorial&quot;&gt;Click Here!&lt;/a&gt;## &lt;/div&gt; withTags(&#123; div(class=&quot;header&quot;, checked=NA, p(&quot;Ready to take the Shiny tutorial? If so&quot;), a(href=&quot;shiny.rstudio.com/tutorial&quot;, &quot;Click Here!&quot;) )&#125;)## &lt;div class=&quot;header&quot; checked&gt;## &lt;p&gt;Ready to take the Shiny tutorial? If so&lt;/p&gt;## &lt;a href=&quot;shiny.rstudio.com/tutorial&quot;&gt;Click Here!&lt;/a&gt;## &lt;/div&gt; ##liststags$div(class=&quot;header&quot;, checked=NA, list( tags$p(&quot;Ready to take the Shiny tutorial? If so&quot;), tags$a(href=&quot;shiny.rstudio.com/tutorial&quot;, &quot;Click Here!&quot;), &quot;Thank you&quot; ))## &lt;div class=&quot;header&quot; checked&gt;## &lt;p&gt;Ready to take the Shiny tutorial? If so&lt;/p&gt;## &lt;a href=&quot;shiny.rstudio.com/tutorial&quot;&gt;Click Here!&lt;/a&gt;## Thank you## &lt;/div&gt; # Raw HTML 加 HTML()tags$div( HTML(&quot;&lt;strong&gt;Raw HTML!&lt;/strong&gt;&quot;))## &lt;div&gt;&lt;strong&gt;Raw HTML!&lt;/strong&gt;&lt;/div&gt; 1234567891011121314151617181920tags$head( tags$style(type='text/css', "select, textarea, input[type='text'] &#123;margin-bottom: 0px;&#125;" , "#submit &#123; color: rgb(255, 255, 255); text-shadow: 0px -1px 0px rgba(0, 0, 0, 0.25); background-color: rgb(189,54,47); background-image: -moz-linear-gradient(center top , rgb(238,95,91), rgb(189,54,47)); background-repeat: repeat-x; border-color: rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.25); &#125;" ), tags$script(HTML(' Shiny.addCustomMessageHandler("jsCode", function(message) &#123; eval(message.value); &#125; );' )) ) HTML 前端+ Shiny服务器端数据响应机制 HTML form elements (in this case a select list and a number input) are bound to input slots using their name attribute. Output is rendered into HTML elements based on matching their id attribute to an output slot and by specifying the requisite css class for the element (in this case either shiny-text-output, shiny-plot-output, or shiny-html-output) 12library(shiny)runExample("08_html") 3. shiny server学习入口最新安装文件 1234567891011$ sudo su - \-c &quot;R -e \&quot;install.packages(&apos;shiny&apos;, repos=&apos;https://cran.rstudio.com/&apos;)\&quot;&quot;$ sudo apt-get install gdebi-core$ wget https://download3.rstudio.org/ubuntu-12.04/x86_64/shiny-server-1.5.4.869-amd64.deb$ sudo gdebi shiny-server-1.5.4.869-amd64.debstart shiny-server # 启动stop shiny-server # 停止restart shiny-server # 重启status shiny-server #查看状态 reload shiny-server #不中断服务的前提下 更新加载配置项 web 查看默认端口时3838 1localhost:3838 可以在配置文件(/etc/shiny-server/shiny-server.conf)中修改 端口 run_as默认为shiny,可改为username 开发的apps拷贝到目录(/srv/shiny-server/)下 1sudo cp ~/app file /srv/shiny-server/ 将文件拷入 日志文件日志文件在(/var/log/shiny-server/)目录下查看 一个生信技能树的安装参考教程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172######前几天刚好在亚马逊云上注册了一个1年免费的Amazon Web Services (AWS) ，正好以此来尝试学习下shiny的相关东西。主要参考了：http://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/http://www.bio-info-trainee.com/1683.html操作系统：ubuntu1. 安装 R sudo apt install r-base2. 安装Rstudio-server sudo apt-get install gdebi-core sudo apt-get install libapparmor1 wget https://download2.rstudio.org/rstudio-server-1.0.143-amd64.deb sudo gdebi rstudio-server-1.0.143-amd64.deb因为Rstudio-server不能以root用户登录，所以我们需要创建一个用户 sudo adduser xxxxx ......然后在网页上输入ip:8787进入Rstudio-server界面，输入用户和密码，即可登录3. 安装Shiny sudo su - -c &quot;R -e \&quot;install.packages(&apos;shiny&apos;, repos=&apos;http://cran.rstudio.com/&apos;)\&quot;&quot;(我注：**如果将配置文件run_as 改为username就不用考虑这个问题，所以安装完shiny server之后就修改run_as为当前的用户，不然后面开发中许多包都找不到**)不能直接进入R，然后install.packages(&quot;shiny&quot;)，因为如果这样安装，是将shiny包安装下当前登录用户的个人library中，使得最终shiny-server无法运行。 apt-get install gdebi-core wget https://download3.rstudio.org/ub ... 1.5.3.838-amd64.deb sudo gdebi shiny-server-1.5.3.838-amd64.deb做完以上几步后，shiny-server算是初步安装好了，然后可以在网页上ip:3838进入shiny-server界面(ip是你服务器的ip地址)。一般我们能看到左边一列的文字和右边的两个框。当然还需要再安装个rmarkdown，不然还是会有error的 sudo su - -c &quot;R -e \&quot;install.packages(&apos;rmarkdown&apos;, repos=&apos;http://cran.rstudio.com/&apos;)\&quot;&quot;sudo su - -c &quot;R -e \&quot;devtools::install_github(&apos;name/package&apos;)\&quot;&quot;4. 配置shiny server * Shiny Server log is at /var/log/shiny-server.log * The default Shiny Server homepage you’re seeing is located at /srv/shiny-server/index.html- you can edit it or remove it. * Any Shiny app directory that you place under /srv/shiny-server/ will be served as a Shiny app. For example, there is a default app at /srv/shiny-server/sample-apps/hello/, which means you can run the app by going to http://123.456.1.2:3838/sample-apps/hello/ * The config file for Shiny Server is at /etc/shiny-server/shiny-server.conf * To reload the server after editing the config, use sudo reload shiny-server * When hosting an Rmarkdown file, name the file index.rmd and add runtime: shiny to the document’s frontmatter5. 赋予shiny权限假设当你登录是以自己用户登录，你在shiny server创建的文件只有该用户（除了root）才有权限读写，但是shiny server是以shiny用户来运行shiny的app，所以要给予shiny用户在一些目录的权限。例如： sudo groupadd shiny-apps sudo usermod -aG shiny-apps dean sudo usermod -aG shiny-apps shiny sudo chown -R dean:shiny-apps /srv/shiny-server sudo chmod g+w /srv/shiny-server sudo chmod g+s /srv/shiny-server ####在该目录下创建的文件都属于该目录所属的组 6. 下载shiny官网的例子git clone https://github.com/rstudio/shiny-examples.git文件是要下载到/srv/shiny-server中7. 运行例子程序http://ip:3838/shiny-examples/010-download/ （ip是你服务器的ip地址）想要运行哪个shiny app，只要在ip:3838/后面添加/srv/shiny-server中的文件的相对路径即可### 4. Shinydashboard这是一个扩展R包，使其有一些面板功能。入门讲解在这里 5. Shinyjs从这里学习，这个放在以后再深入吧。 6. Shinythemes 最近用shiny做了一个页面，也是一个艰难的开始。Mark in(“Mon Sep 04 19:03:13 2017”)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下载指令]]></title>
    <url>%2Fhexo%2F2017%2F08%2F28%2F2017-08-28-UbuntuDonwload%2F</url>
    <content type="text"><![CDATA[1. wgetwget -P 目录 网址 wget是ubuntu原生集成的命令 支持 断点续传，后台下载。 12wget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP075/SRP075747/SRR3589957/SRR3589957.srawget -c -b ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/mm10.zip 2. axelaxel是一个多线程下载工具，下载速度会比wget快很多，据说是支持断点续传的 1nohup axel ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP075/SRP075747/SRR3589957/SRR3589957.sra &amp; (后台执行) 3. uget+aria2插件12345678910111213141.uget的安装：sudo add-apt-repository ppa:plushuang-tw/uget-stable sudo apt-get update sudo apt-get install uget2.aria2的安装：sudo add-apt-repository ppa:t-tujikawa/ppa sudo apt-get update sudo apt-get install aria2#查看版本和支持的特性。aria2 -v在桌面中打开 uget程序打开菜单中“编辑”-&gt;“设置（S）……”，在弹出的界面中选择“插件”选择插件匹配顺序“aria2”，参数设置：--enable-rpc=true -D --disable-ipv6 --check-certificate=false 4.wget命令下载FTP整个目录进行文件备份https://www.cnblogs.com/crxis/p/7072813.html 使用wget下载整个FTP目录，可以用于服务器间文件传输，进行远程备份。通过限制网速，可以解决带宽限制问题。 1234567891011121314wget ftp://IP:PORT/* --ftp-user=xxx --ftp-password=xxx -r -c-r参数就是用来目录下载的-c是使用断点续传，服务器要支持或wget -r -nH -P/webapp/ ftp://172.31.1.1:21/* --ftp-user=ftpuser --ftp-password=ftpuser星号*必须有-r参数就是用来目录下载的，递归下载-nH不包含主机文件夹-P 下载到指定目录wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/* -r -cwget ftp://ftp.broadinstitute.org/bundle/hg19/* --ftp-user=gsapubftp-anonymous -r -cwget -r -c -nH ftp://ftp.broadinstitute.org:21/bundle/hg19/* --ftp-user=gsapubftp-anonymous 可以写到shell脚本中，后台执行。 1#!/bin/shwget ftp://IP:PORT/* --ftp-user=xxx --ftp-password=xxx -r -c 保存为wget.sh，并赋予可执行权限 1# chmod +x wget.sh 后台执行这个脚本，在命令后加“&amp;”即可 1./wget.sh &amp; 如果希望把执行结果输出到文件的话，前面加 nohup，默认输出文件名为 nohup.out 1nohup ./wget.sh &amp; 如果要限速的话，加上—limit-rate=100k，代表限速100k，-c是断点续传的意思。]]></content>
  </entry>
  <entry>
    <title><![CDATA[杂项]]></title>
    <url>%2Fhexo%2F2017%2F08%2F27%2F2017-08-27_tricks%2F</url>
    <content type="text"><![CDATA[有道词典不能联网internet选项-连接-局域网设置-去掉复选框-确定 outlook 2000 配置阿里云邮箱教程 1234567891011文件-添加账号-电子邮件账号-手动配置服务器...-Internet电子邮件用户信息您的姓名：输入您的姓名，作为外发邮件"发件人："字段中显示的内容。电子邮件地址：输入您的完整邮箱的地址 (如 name@aliyun.com )服务器信息接收邮件服务器 (POP3)：pop3.aliyun.com邮件发送服务器 (SMTP)：smtp.aliyun.com登录信息用户名：输入您完整的邮箱地址密码：输入您在阿里云web邮箱设置的客户端密码。下一步-[发送服务器]勾选 使用与接收邮件服务器相同的设置；[高级]勾选 在服务器上保留邮件副本-测试账户是否生效 系统office路径 C:\Program Files (x86)\Microsoft Office\root\Office16\POWERPNT.EXE 使用Redirector插件解决googleapis公共库加载的问题Redirector目前支持Firefox、Chrome、Opera三款浏览器，可以前往对应浏览器的应用市场下载安装相应版本 熟悉Github的用户也可以直接前往项目主页上找下载链接:einaregilsson/Redirector 直接浏览“使用Import来导入设置”一节来使用导入来进行设置 123456789101112131415161718192021&#123; "createdBy": "Redirector v3.1.0", "createdAt": "2016-09-10T13:29:02.323Z", "redirects": [ &#123; "description": "Ajax", "exampleUrl": "http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js", "exampleResult": "http://ajax.lug.ustc.edu.cn/ajax/libs/jquery/1.7.1/jquery.min.js", "error": null, "includePattern": "*ajax.googleapis.com*", "excludePattern": "", "redirectUrl": "$1ajax.lug.ustc.edu.cn$2", "patternType": "W", "processMatches": "noProcessing", "disabled": false, "appliesTo": [ "script" ] &#125; ]&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Markdown博客技巧汇总]]></title>
    <url>%2Fhexo%2F2017%2F08%2F27%2F2017-08-27_markdown_tricks%2F</url>
    <content type="text"><![CDATA[1. 用”&gt; “实现以下功能1234&gt; 纤云弄巧，飞星传恨，银汉迢迢暗度。 金风玉露一相逢，便胜却、人间无数。 柔情似水，佳期如梦，忍顾鹊桥归路。 两情若是久长时，又岂在、朝朝暮暮。 效果： 纤云弄巧，飞星传恨，银汉迢迢暗度。 金风玉露一相逢，便胜却、人间无数。 柔情似水，佳期如梦，忍顾鹊桥归路。 两情若是久长时，又岂在、朝朝暮暮。 2. 用三个以上空格即可实现背景宽框1234纤云弄巧，飞星传恨，银汉迢迢暗度。金风玉露一相逢，便胜却、人间无数。柔情似水，佳期如梦，忍顾鹊桥归路。两情若是久长时，又岂在、朝朝暮暮。 效果： 纤云弄巧，飞星传恨，银汉迢迢暗度。 金风玉露一相逢，便胜却、人间无数。 柔情似水，佳期如梦，忍顾鹊桥归路。 两情若是久长时，又岂在、朝朝暮暮。 3. Markdonw 文字颜色1&lt;font color=red&gt;内容&lt;/font&gt; 内容 4. HTML注释标签1HTML &lt;!-- XXXX --&gt; 5. 插入公式1234add &lt;script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt;行间公式： $$\lambda=n \ast p, p=\frac&#123;l&#125;&#123;s&#125;$$行内公式： \\(\lambda\\) 6. 插入图片1https://xxx.github.io/DocumentGit/img/xx.jpg]]></content>
  </entry>
  <entry>
    <title><![CDATA[上线测试]]></title>
    <url>%2Fhexo%2F2017%2F08%2F27%2F2017-08-27-Online%20test%2F</url>
    <content type="text"><![CDATA[今天终于把blog搭建好了，mark and test 于2017年8月27日 周日]]></content>
      <tags>
        <tag>杂项</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序言]]></title>
    <url>%2Fhexo%2F2017%2F07%2F16%2F2017-07-16-preface%2F</url>
    <content type="text"><![CDATA[无一事马虎 无一日懈怠好句，若真如此，生命无光，况凡人不可为之。可作凡人的目标，努力逼近它吧。 最近几年我总是在寻找一个网络服务系统，能使我平生所得所获的有一个的去处，方便以后某个时间点来寻他们。在IT的世界里，我一直认为重要的是一种知识概念和印象，以前学过做过，过一段时间还是得从头再来，再做一遍与第一遍的时间相差无几， 假如你没有保存的话….. Hogo的出现使最熟练R的人感到喜悦。看到那么多Hugo Themes，于是我开始挑啊，总告诉我有适合我的，却总是找不到它。我想找的它是一个目录列表+时间，目录列表的字体不要太大，这样能方便我快速的定位和找回过去的记忆，也省去频繁的翻页。 Yihui 是我在打开Bioinformatics世界之初就偶然关注到的一个[学者] OR [IT工程师] OR [老师]，偶然和必然等意，Bioinformatics and Statistics的天下谁人不识君。他的页面和他的博客样式是我梦寐以求，就是它了。我没办法自己修改主题，我也不屑再去学，睡觉的时间宝贵，不要总是学，什么都学，学来学去魂消了。 Yihui 吟的几句词也还不失雅趣，不舍删留此处，可算小读怡情。以后的拾遗也填在这里吧…. 深情似海，问相逢初度，是何年纪？依约而今还记取，不是前生夙世。放学花前，题诗石上，春水园亭里。逢君一笑，人间无此欢喜。无奈苍狗看云，红羊数劫，惘惘休提起。客气渐多真气少，汩没心灵何已。千古声名，百年担负，事事违初意。心头阁住，儿时那种情味。 END 于 2017年7月16日]]></content>
  </entry>
</search>
